{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff6baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append('/home/vdhanraj/llama/llama3/llama')\n",
    "sys.path.append('/home/vdhanraj/llama/llama3')\n",
    "\n",
    "import openai\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, TypedDict\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from fairscale.nn.model_parallel.initialize import (\n",
    "    get_model_parallel_rank,\n",
    "    initialize_model_parallel,\n",
    "    model_parallel_is_initialized,\n",
    ")\n",
    "from fairscale.nn.model_parallel.layers import (\n",
    "    ColumnParallelLinear,\n",
    "    RowParallelLinear,\n",
    "    VocabParallelEmbedding,\n",
    ")\n",
    "\n",
    "from llama.model import ModelArgs, Transformer, RMSNorm\n",
    "from llama.tokenizer import ChatFormat, Dialog, Message, Tokenizer\n",
    "from llama.generation import sample_top_p\n",
    "from llama.EncoderNetworks import Encoder, Decoder, Encoder_Deep, Decoder_Deep, LastTokenTransformer\n",
    "\n",
    "from typing import List, Optional\n",
    "import fire\n",
    "\n",
    "from llama import Dialog, Llama\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from llama.SP_engine import SemanticEngine\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "\n",
    "from nengo.dists import UniformHypersphere\n",
    "\n",
    "from graphviz import Digraph\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchviz import make_dot\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e332fad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "#ckpt_dir = \"Meta-Llama-3-8B-Instruct\"\n",
    "ckpt_dir = \"/home/vdhanraj/.llama/checkpoints/Llama3.1-8B-Instruct\"\n",
    "#tokenizer_path = \"Meta-Llama-3-8B-Instruct/tokenizer.model\"\n",
    "tokenizer_path = \"/home/vdhanraj/.llama/checkpoints/Llama3.1-8B-Instruct/tokenizer.model\"\n",
    "max_seq_len = 10000\n",
    "max_batch_size = 2 # set to 1 if doing CoT to not overload the GPU, otherwise can handle up to 4\n",
    "model_parallel_size = 1\n",
    "\n",
    "top_p = 0.9\n",
    "temperature = 0\n",
    "max_gen_len = None\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb48371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdhanraj/anaconda3/envs/torch/lib/python3.8/site-packages/torch/__init__.py:690: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 15.22 seconds\n"
     ]
    }
   ],
   "source": [
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['WORLD_SIZE'] = \"1\"\n",
    "os.environ['MASTER_ADDR'] = \"127.0.0.2\"\n",
    "os.environ['MASTER_PORT'] = \"29502\"\n",
    "os.environ['LOCAL_RANK']  = \"0\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "\n",
    "generator = Llama.build(\n",
    "    ckpt_dir=ckpt_dir,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    max_seq_len=max_seq_len,\n",
    "    max_batch_size=max_batch_size,\n",
    ")\n",
    "self = generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dbe4dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import num2words as n2w\n",
    "from word2number import w2n\n",
    "\n",
    "def generate_dialog(complexity=8, samples=1, problem_type=\"addition\", cot=False, string_nums=False):\n",
    "    #x = np.random.randint(low=10**(complexity), high=10**(complexity+1), size=samples)\n",
    "    x = np.random.randint(low=1, high=10**(complexity+1), size=samples)\n",
    "    #y = np.random.randint(low=10**(complexity), high=10**(complexity+1), size=samples)\n",
    "    y = np.random.randint(low=1, high=10**(complexity+1), size=samples)\n",
    "    \n",
    "    temp_x = []\n",
    "    temp_y = []\n",
    "    for n in range(samples):\n",
    "        x[n], y[n] = max(x[n], y[n]), min(x[n], y[n])\n",
    "        if string_nums:\n",
    "            temp_x += [n2w.num2words(x[n])]\n",
    "            temp_y += [n2w.num2words(y[n])]\n",
    "    if string_nums:\n",
    "        x, y, temp_x, temp_y = np.array(temp_x), np.array(temp_y), x, y\n",
    "\n",
    "    #example_x1, example_y1 = np.random.randint(low=10**(complexity), high=10**(complexity+1)), np.random.randint(low=10**(complexity), high=10**(complexity+1))\n",
    "    example_x1, example_y1 = np.random.randint(low=1, high=10**(complexity+1)), np.random.randint(low=1, high=10**(complexity+1))\n",
    "    #example_x2, example_y2 = np.random.randint(low=10**(complexity), high=10**(complexity+1)), np.random.randint(low=10**(complexity), high=10**(complexity+1))\n",
    "    example_x2, example_y2 = np.random.randint(low=1, high=10**(complexity+1)), np.random.randint(low=1, high=10**(complexity+1))\n",
    "    example_x1, example_y1 = max(example_x1, example_y1), min(example_x1, example_y1)\n",
    "    example_x2, example_y2 = max(example_x2, example_y2), min(example_x2, example_y2)\n",
    "    \n",
    "    if string_nums:\n",
    "        example_x1, example_y1 = n2w.num2words(example_x1), n2w.num2words(example_y1)\n",
    "        example_x2, example_y2 = n2w.num2words(example_x2), n2w.num2words(example_y2)\n",
    "\n",
    "    if string_nums:\n",
    "        conv = lambda x: w2n.word_to_num(str(x))\n",
    "        conv_inv = lambda x: n2w.num2words(int(x))\n",
    "\n",
    "    else:\n",
    "        conv = lambda x: x\n",
    "        conv_inv = lambda x: x\n",
    "\n",
    "\n",
    "    dialog: List[Dialog] = []\n",
    "\n",
    "    if type(problem_type) == type([]):\n",
    "        problem_type = random.choice(problem_type)\n",
    "    \n",
    "    if problem_type == \"random\":\n",
    "        problem_type = random.choice([\"addition\", \"multiplication\", \"division\", \"modulo\", \"gcd\", \"lcm\", \"square_mod\", \"bitwise_and\", \"bitwise_xor\", \"bitwise_or\"])\n",
    "\n",
    "    for n in range(samples):\n",
    "        if cot:\n",
    "            dialog += [\n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": \n",
    "                     \"You are a math-solving assistant. Always explain your reasoning step by step. \"\n",
    "                     \"Regardless of the steps taken, ensure the final answer is clearly marked with 'Final Answer: x'.\"\n",
    "                    },\n",
    "                ]\n",
    "            ]\n",
    "        else:\n",
    "            dialog += [\n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": \n",
    "                     \"You are a math solving helper. Don't use any commas in your output, \"\n",
    "                     \"and always answer problems according to the format of previous answers.\"\n",
    "                    },\n",
    "                ]\n",
    "            ]\n",
    "\n",
    "        if problem_type == \"addition\":\n",
    "            if cot:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"Solve the following problem step by step: \" \n",
    "                     f\"What is {x[n]} plus {y[n]}?\"},\n",
    "                ]\n",
    "            else:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x1} plus {example_y1}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(conv(example_x1) + conv(example_y1))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x2} plus {example_y2}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(conv(example_x2) + conv(example_y2))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {x[n]} plus {y[n]}?\"},\n",
    "                ]\n",
    "\n",
    "        elif problem_type == \"multiplication\":\n",
    "            if cot:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"Solve the following problem step by step: \" \n",
    "                     f\"What is {x[n]} times {y[n]} mod {10**(complexity+1)}?\"},\n",
    "                ]\n",
    "            else:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x1} times {example_y1} mod {10**(complexity+1)}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv((conv(example_x1) * conv(example_y1)) % 10**(complexity+1))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x2} times {example_y2} mod {10**(complexity+1)}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv((conv(example_x2) * conv(example_y2)) % 10**(complexity+1))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {x[n]} times {y[n]} mod {10**(complexity+1)}?\"},\n",
    "                ]\n",
    "\n",
    "        elif problem_type == \"division\":\n",
    "            if cot:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"Solve the following problem step by step: \" \n",
    "                     f\"What is {x[n]} // {y[n]}?\"},\n",
    "                ]\n",
    "            else:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x1} // {example_y1}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(conv(example_x1)//conv(example_y1))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x2} // {example_y2}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(conv(example_x2)//conv(example_y2))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {x[n]} // {y[n]}?\"},\n",
    "                ]\n",
    "\n",
    "        elif problem_type == \"modulo\":\n",
    "            if cot:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"Solve the following problem step by step: \" \n",
    "                     f\"What is {x[n]} mod {y[n]}?\"},\n",
    "                ]\n",
    "            else:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x1} mod {example_y1}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(conv(example_x1) % conv(example_y1))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x2} mod {example_y2}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(conv(example_x2) % conv(example_y2))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {x[n]} mod {y[n]}?\"},\n",
    "                ]\n",
    "\n",
    "        elif problem_type == \"gcd\":\n",
    "            if cot:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"Solve the following problem step by step: \" \n",
    "                     f\"What is the GCD of {x[n]} and {y[n]}?\"},\n",
    "                ]\n",
    "            else:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"What is the GCD of {example_x1} and {example_y1}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(np.gcd(conv(example_x1), conv(example_y1)))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is the GCD of {example_x2} and {example_y2}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(np.gcd(conv(example_x2), conv(example_y2)))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is the GCD of {x[n]} and {y[n]}?\"},\n",
    "                ]\n",
    "\n",
    "        elif problem_type == \"lcm\":\n",
    "            if cot:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"Solve the following problem step by step: \" \n",
    "                     f\"What is the LCM of {x[n]} and {y[n]} mod {10**(complexity+1)}?\"},\n",
    "                ]\n",
    "            else:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"What is the LCM of {example_x1} and {example_y1} mod {10**(complexity+1)}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(np.lcm(conv(example_x1), conv(example_y1)) % 10**(complexity+1))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is the LCM of {example_x2} and {example_y2} mod {10**(complexity+1)}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(np.lcm(conv(example_x2), conv(example_y2)) % 10**(complexity+1))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is the LCM of {x[n]} and {y[n]} mod {10**(complexity+1)}?\"},\n",
    "                ]\n",
    "\n",
    "        elif problem_type == \"square_mod\":\n",
    "            if cot:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"Solve the following problem step by step: \" \n",
    "                     f\"What is {x[n]}^2 mod {y[n]}?\"},\n",
    "                ]\n",
    "            else:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x1}^2 mod {example_y1}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv((conv(example_x1))**2 % conv(example_y1))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x2}^2 mod {example_y2}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv((conv(example_x2))**2 % conv(example_y2))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {x[n]}^2 mod {y[n]}?\"},\n",
    "                ]\n",
    "\n",
    "        elif problem_type == \"bitwise_and\":\n",
    "            if cot:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"Solve the following problem step by step: \" \n",
    "                     f\"What is {x[n]} AND {y[n]}? Remember to convert your final answer back to decimal\"},\n",
    "                ]\n",
    "            else:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x1} AND {example_y1}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(conv(example_x1) & conv(example_y1))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x2} AND {example_y2}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(conv(example_x2) & conv(example_y2))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {x[n]} AND {y[n]}?\"},\n",
    "                ]\n",
    "\n",
    "        elif problem_type == \"bitwise_xor\":\n",
    "            if cot:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"Solve the following problem step by step: \" \n",
    "                     f\"What is {x[n]} XOR {y[n]}? Remember to convert your final answer back to decimal\"},\n",
    "                ]\n",
    "            else:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x1} XOR {example_y1}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(conv(example_x1) ^ conv(example_y1))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x2} XOR {example_y2}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(conv(example_x2) ^ conv(example_y2))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {x[n]} XOR {y[n]}?\"},\n",
    "                ]\n",
    "        elif problem_type == \"bitwise_or\":\n",
    "            if cot:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"Solve the following problem step by step: \" \n",
    "                     f\"What is {x[n]} OR {y[n]}? Remember to convert your final answer back to decimal\"},\n",
    "                ]\n",
    "            else:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x1} OR {example_y1}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(conv(example_x1) | conv(example_y1))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x2} OR {example_y2}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(conv(example_x2) | conv(example_y2))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {x[n]} OR {y[n]}?\"},\n",
    "                ]\n",
    "        elif problem_type == \"bitwise_nor\":\n",
    "            if cot:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"Solve the following problem step by step: \" \n",
    "                     f\"What is {x[n]} NOR {y[n]}? Remember to convert your final answer back to decimal\"},\n",
    "                ]\n",
    "            else:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x1} NOR {example_y1}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(~(conv(example_x1) | conv(example_y1)))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x2} NOR {example_y2}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(~(conv(example_x2) | conv(example_y2)))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {x[n]} NOR {y[n]}?\"},\n",
    "                ]\n",
    "        elif problem_type == \"bitwise_nand\":\n",
    "            if cot:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"Solve the following problem step by step: \" \n",
    "                     f\"What is {x[n]} NAND {y[n]}? Remember to convert your final answer back to decimal\"},\n",
    "                ]\n",
    "            else:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x1} NAND {example_y1}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(~(conv(example_x1) & conv(example_y1)))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x2} NAND {example_y2}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(~(conv(example_x2) & conv(example_y2)))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {x[n]} NAND {y[n]}?\"},\n",
    "                ]\n",
    "        elif problem_type == \"bitwise_nxor\":\n",
    "            if cot:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"Solve the following problem step by step: \" \n",
    "                     f\"What is {x[n]} NXOR {y[n]}? Remember to convert your final answer back to decimal\"},\n",
    "                ]\n",
    "            else:\n",
    "                dialog[n] += [\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x1} NXOR {example_y1}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(~(conv(example_x1) ^ conv(example_y1)))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {example_x2} NXOR {example_y2}?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{conv_inv(~(conv(example_x2) ^ conv(example_y2)))}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"What is {x[n]} NXOR {y[n]}?\"},\n",
    "                ]\n",
    "\n",
    "\n",
    "    return dialog, x, y, problem_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ee4239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_non_math_dialog(samples=1, topic=\"philosophy\", cot=False):\n",
    "    example_questions = {\n",
    "        \"philosophy\": [\n",
    "            (\"Is the Ship of Theseus still the same ship after all parts are replaced?\", \"This is a classic thought experiment questioning the nature of identity.\"),\n",
    "            (\"What is the meaning of life according to existentialism?\", \"Existentialists argue that individuals create their own meaning through choices and actions.\"),\n",
    "            (\"Can free will exist in a deterministic universe?\", \"This question explores the compatibility of determinism with the concept of free will.\"),\n",
    "            (\"What distinguishes knowledge from belief?\", \"Knowledge typically requires justified true belief, while belief does not necessarily require justification.\"),\n",
    "            (\"Does morality exist independently of humans?\", \"This question examines moral realism versus moral anti-realism.\")\n",
    "        ],\n",
    "        \"ethics\": [\n",
    "            (\"Is it morally acceptable to lie to protect someone's feelings?\", \"This involves balancing honesty with the value of kindness.\"),\n",
    "            (\"Should animals have the same rights as humans?\", \"This raises questions about sentience, suffering, and ethical consideration.\"),\n",
    "            (\"Is it ethical to use artificial intelligence in decision-making?\", \"This question considers fairness, accountability, and potential biases in AI systems.\"),\n",
    "            (\"Does the end justify the means?\", \"This touches on consequentialist versus deontological ethical theories.\"),\n",
    "            (\"Is capital punishment morally justifiable?\", \"This question explores justice, deterrence, and the value of human life.\")\n",
    "        ],\n",
    "        \"history\": [\n",
    "            (\"What if the Roman Empire never fell?\", \"Speculative history suggests it could have led to advanced technology earlier.\"),\n",
    "            (\"How did the Industrial Revolution change society?\", \"It shifted economies from agrarian to industrial, reshaping labor and urbanization.\"),\n",
    "            (\"What were the causes and consequences of the French Revolution?\", \"It led to the rise of democracy and the decline of monarchies in Europe.\"),\n",
    "            (\"How did the Cold War influence global politics?\", \"It created a bipolar world order, leading to numerous proxy wars and political tensions.\"),\n",
    "            (\"What if World War II had a different outcome?\", \"This explores alternate history scenarios with potential geopolitical shifts.\")\n",
    "        ],\n",
    "        \"psychology\": [\n",
    "            (\"What does the Stanford Prison Experiment reveal about human behavior?\", \"It highlights the power of situational influences over personal traits.\"),\n",
    "            (\"How do cognitive biases affect decision-making?\", \"Biases like confirmation bias can distort our perception and judgments.\"),\n",
    "            (\"What is the impact of social media on mental health?\", \"It can influence self-esteem, anxiety levels, and social connections both positively and negatively.\"),\n",
    "            (\"How does memory work in the human brain?\", \"Memory involves encoding, storage, and retrieval processes within neural networks.\"),\n",
    "            (\"What role does nature versus nurture play in personality development?\", \"This explores the influence of genetics and environment on behavior.\")\n",
    "        ],\n",
    "        \"science_fiction\": [\n",
    "            (\"What are the ethical implications of artificial intelligence surpassing human intelligence?\", \"This involves concerns about autonomy, control, and societal impact.\"),\n",
    "            (\"Could time travel ever be possible according to current physics?\", \"While speculative, theories like wormholes explore this possibility.\"),\n",
    "            (\"How might colonizing Mars change human society?\", \"It could lead to new cultural developments, governance systems, and ethical dilemmas.\"),\n",
    "            (\"What are the potential risks of genetic engineering?\", \"Concerns include unintended consequences, ethical issues, and impacts on biodiversity.\"),\n",
    "            (\"What would society look like in a post-scarcity economy?\", \"It would challenge traditional economic models and social structures.\")\n",
    "        ],\n",
    "        \"technology\": [\n",
    "            (\"How has the internet changed the way we communicate?\", \"It has enabled instant, global communication but also introduced challenges like misinformation.\"),\n",
    "            (\"What are the ethical concerns with facial recognition technology?\", \"Issues include privacy invasion, surveillance, and potential biases.\"),\n",
    "            (\"Will quantum computing revolutionize cybersecurity?\", \"Quantum computing poses both opportunities and risks for data encryption and security.\"),\n",
    "            (\"How does blockchain technology work?\", \"It is a decentralized, secure method for recording transactions using cryptographic techniques.\"),\n",
    "            (\"What is the future of autonomous vehicles?\", \"Advancements may lead to changes in transportation, safety, and urban planning.\")\n",
    "        ],\n",
    "        \"art_and_culture\": [\n",
    "            (\"What defines a work of art?\", \"Art can be defined by its aesthetic value, emotional impact, or cultural significance.\"),\n",
    "            (\"How has pop culture influenced societal norms?\", \"Pop culture reflects and shapes attitudes, trends, and behaviors in society.\"),\n",
    "            (\"What role does art play in social movements?\", \"Art can inspire, provoke thought, and mobilize people for causes.\"),\n",
    "            (\"How does music affect human emotions?\", \"Music influences mood, cognitive functions, and even physiological responses.\"),\n",
    "            (\"What is the significance of cultural heritage?\", \"It preserves the identity, history, and values of communities across generations.\")\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    dialog: List[Dialog] = []\n",
    "    correct_responses    = []\n",
    "    \n",
    "\n",
    "    if topic == \"random\":\n",
    "        topic = random.choice(list(example_questions.keys()))\n",
    "\n",
    "    if isinstance(topic, list):\n",
    "        topic = random.choice(topic)\n",
    "\n",
    "    for _ in range(samples):\n",
    "        example_1, response_1 = random.choice(example_questions[topic])\n",
    "        example_2, response_2 = random.choice(example_questions[topic])\n",
    "        new_question, new_response = random.choice(example_questions[topic])\n",
    "        correct_responses += [new_response]\n",
    "\n",
    "        if cot:\n",
    "            dialog.append([\n",
    "                {\"role\": \"system\", \"content\": \"You are a thoughtful assistant. Provide reasoned and reflective answers.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Consider this question carefully: {new_question}\"},\n",
    "            ])\n",
    "        else:\n",
    "            dialog.append([\n",
    "                {\"role\": \"system\", \"content\": \"You are a knowledgeable assistant providing concise answers.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{example_1}\"},\n",
    "                {\"role\": \"assistant\", \"content\": f\"{response_1}\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{example_2}\"},\n",
    "                {\"role\": \"assistant\", \"content\": f\"{response_2}\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{new_question}\"}\n",
    "            ])\n",
    "\n",
    "    return dialog, correct_responses, topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d754283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode(dialogs, self, temperature=0.0, top_p=0.9, inference_mode=None,\n",
    "            max_decoding_length=100, problem_type=\"addition\", add_noise=False, \n",
    "            symbolic_encoding_layer=17, symbolic_decoding_layers=[17], normalize_vector=False, \n",
    "            rms_layer=False, double_rep=True, complexity=2, bypass_symbolic=False, verbose=False):\n",
    "    prompt_tokens = generator.parse_chat(dialogs)\n",
    "\n",
    "    max_gen_len = self.model.params.max_seq_len - 1\n",
    "    top_p = top_p\n",
    "    echo = False\n",
    "\n",
    "    params = self.model.params\n",
    "    bsz = len(prompt_tokens)\n",
    "    assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
    "\n",
    "    min_prompt_len = min(len(t) for t in prompt_tokens)\n",
    "    max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "    assert max_prompt_len <= params.max_seq_len\n",
    "    total_len = min(params.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "    pad_id = self.tokenizer.pad_id\n",
    "    tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n",
    "    for k, t in enumerate(prompt_tokens):\n",
    "        tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n",
    "\n",
    "    prev_pos = 0\n",
    "    eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n",
    "    input_text_mask = tokens != pad_id\n",
    "\n",
    "    stop_tokens = torch.tensor(list(self.tokenizer.stop_tokens))\n",
    "\n",
    "    transitions = []\n",
    "    curr_token = 0\n",
    "    list_of_probs  = []\n",
    "    list_of_logits = []\n",
    "    h_stacks = []\n",
    "    for cur_pos in range(min_prompt_len, total_len):\n",
    "        logits, h_stack, h = inference_mode(tokens[:, prev_pos:cur_pos], prev_pos, problem_type=problem_type, add_noise=add_noise,\n",
    "                                            bypass_symbolic=bypass_symbolic, symbolic_encoding_layer=symbolic_encoding_layer,\n",
    "                                            symbolic_decoding_layers=symbolic_decoding_layers, normalize_vector=normalize_vector,\n",
    "                                            rms_layer=rms_layer, double_rep=double_rep, complexity=complexity, \n",
    "                                            curr_token=curr_token, verbose=verbose)\n",
    "        # Shape of logits are (batch_size, total_sequence_length, num_tokens)\n",
    "        h_stacks += [h_stack]\n",
    "        # probs are intentionally being calculated here, so that it contains an extra token (the stop token), to help with loss calculation\n",
    "        probs = torch.softmax(logits[:, -1,:] / 1, dim=-1)\n",
    "        list_of_probs  += [probs]\n",
    "        list_of_logits += [logits[:,-1,:]]\n",
    "        new_logits = logits.detach()\n",
    "        if temperature > 0:\n",
    "            probs = torch.softmax(new_logits[:, -1] / temperature, dim=-1)\n",
    "            #print(logits, logits.shape)\n",
    "            next_token = sample_top_p(probs, top_p)\n",
    "        else:\n",
    "            next_token = torch.argmax(new_logits[:, -1], dim=-1)\n",
    "        if curr_token > max_decoding_length:\n",
    "            next_token = stop_tokens[0]\n",
    "        next_token = next_token.reshape(-1)\n",
    "        # only replace token if prompt has already been generated\n",
    "        next_token = torch.where(\n",
    "            input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "        )\n",
    "        tokens[:, cur_pos] = next_token\n",
    "\n",
    "        eos_reached |= (~input_text_mask[:, cur_pos]) & (\n",
    "            torch.isin(next_token, stop_tokens)\n",
    "        )\n",
    "        prev_pos = cur_pos\n",
    "        if all(eos_reached):\n",
    "            break\n",
    "\n",
    "        curr_token += 1\n",
    "        \n",
    "    list_of_probs = torch.stack(list_of_probs)\n",
    "    list_of_logits = torch.stack(list_of_logits)\n",
    "\n",
    "    out_tokens = []\n",
    "    for i, toks in enumerate(tokens.tolist()):\n",
    "        # cut to max gen len\n",
    "        start = 0 if echo else len(prompt_tokens[i])\n",
    "        toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n",
    "        # cut to after eos tok if any\n",
    "        for stop_token in self.tokenizer.stop_tokens:\n",
    "            try:\n",
    "                eos_idx = toks.index(stop_token)\n",
    "                toks = toks[:eos_idx]\n",
    "            except ValueError:\n",
    "                pass\n",
    "        out_tokens.append(toks)\n",
    "\n",
    "    return h_stacks, list_of_probs, list_of_logits, out_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e91f2ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dialog_indices(dialog, calculate_end_index=False):\n",
    "    start_indices = []\n",
    "    end_indices   = []\n",
    "    for i in range(len(dialog)):\n",
    "        # Find the final occurance of user chat (which is the question being asked to the LLM)\n",
    "        start_index = len(generator.parse_chat(dialog)[i]) - generator.parse_chat(dialog)[i][::-1].index(882) + 2\n",
    "        # The final token position to save\n",
    "        if not calculate_end_index:\n",
    "            end_index   = -1 # If end_index is -1, use all tokens up till the end, otherwise calculate based on eot token\n",
    "        else:\n",
    "            end_index   = len(generator.parse_chat(dialog)[i]) - generator.parse_chat(dialog)[i][::-1].index(128009) - 1\n",
    "        start_indices += [start_index]\n",
    "        end_indices   += [end_index]\n",
    "        \n",
    "    return start_indices, end_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3783a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(n_samples, self, temperature=0, problem_type=\"addition\", add_noise=False, normalize_vector=False,\n",
    "                  inference_to_backprop_ratio=1, symbolic_encoding_layer=16, symbolic_decoding_layers=[32],\n",
    "                  optimizer=None, criterion=None, bypass_symbolic=False,\n",
    "                  complexity=2, losses_per_pt=None, scores_per_pt=None,rms_layer=False, double_rep=True, verbose=False):\n",
    "    # Set the model in training mode\n",
    "\n",
    "    all_logits = []\n",
    "    #all_x     = []\n",
    "    #all_y     = []\n",
    "    all_corr  = []\n",
    "\n",
    "    total_score = 0\n",
    "    outputs = []\n",
    "    pts     = []\n",
    "\n",
    "    for n in range(inference_to_backprop_ratio):\n",
    "        if verbose:\n",
    "            print(\"On sub-epoch iteration:\", n+1)\n",
    "        response_data = []\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        dialogs, x, y, curr_problem_type = generate_dialog(complexity=complexity, samples=n_samples, problem_type=problem_type)\n",
    "\n",
    "        if self.model.encoder_input_tokens == \"all\":\n",
    "            start_indices, end_indices    = get_dialog_indices(dialogs, calculate_end_index=self.model.calculate_end_index)\n",
    "            self.model.curr_start_indices = start_indices\n",
    "            self.model.curr_end_indices   = end_indices\n",
    "            self.model.dialogs            = dialogs\n",
    "\n",
    "        if self.model.calculate_encoding_accuracy:\n",
    "            self.model.curr_x = x\n",
    "            self.model.curr_y = y\n",
    "\n",
    "        pts += [curr_problem_type]\n",
    "        if curr_problem_type==\"addition\":\n",
    "            correct_responses = [x[i] + y[i] for i in range(len(x))]\n",
    "        if curr_problem_type==\"multiplication\":\n",
    "            correct_responses = [(x[i] * y[i]) % 10**(complexity+1) for i in range(len(x))]\n",
    "        if curr_problem_type==\"division\":\n",
    "            correct_responses = [int(x[i] // y[i]) for i in range(len(x))]\n",
    "        if curr_problem_type==\"modulo\":\n",
    "            correct_responses = [x[i] % y[i] for i in range(len(x))]\n",
    "        if curr_problem_type==\"gcd\":\n",
    "            correct_responses = [np.gcd(x[i], y[i]) for i in range(len(x))]\n",
    "        if curr_problem_type==\"lcm\":\n",
    "            correct_responses = [np.lcm(x[i], y[i]) % 10**(complexity+1) for i in range(len(x))]\n",
    "        if curr_problem_type==\"square_mod\":\n",
    "            correct_responses = [x[i]**2 % y[i] for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_and\":\n",
    "            correct_responses = [x[i] & y[i] for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_xor\":\n",
    "            correct_responses = [x[i] ^ y[i] for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_or\":\n",
    "            correct_responses = [x[i] | y[i] for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_nand\":\n",
    "            correct_responses = [~(x[i] & y[i]) for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_nxor\":\n",
    "            correct_responses = [~(x[i] ^ y[i]) for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_nor\":\n",
    "            correct_responses = [~(x[i] | y[i]) for i in range(len(x))]\n",
    "\n",
    "        # Shape of list_of_probs and list_of_logits is (sequence_output_length, batch_size, num_tokens)\n",
    "        h_stack, list_of_probs, list_of_logits, out_tokens = episode(dialogs=dialogs, self=self, temperature=temperature,\n",
    "                                                                     inference_mode=self.model.forward_symbolic_funnel, \n",
    "                                                                     max_decoding_length=complexity+5, bypass_symbolic=bypass_symbolic,\n",
    "                                                                     problem_type=curr_problem_type, add_noise=add_noise, \n",
    "                                                                     symbolic_encoding_layer=symbolic_encoding_layer, \n",
    "                                                                     symbolic_decoding_layers=symbolic_decoding_layers, \n",
    "                                                                     normalize_vector=normalize_vector, complexity=complexity,\n",
    "                                                                     rms_layer=rms_layer, double_rep=double_rep, verbose=verbose)\n",
    "\n",
    "        #print(list_of_logits.shape, list_of_probs.shape)\n",
    "\n",
    "        all_logits = all_logits + [list_of_logits]\n",
    "        #all_x      = all_x      + [torch.tensor(x)]\n",
    "        #all_y      = all_y      + [torch.tensor(y)]\n",
    "        all_corr   = all_corr   + [torch.tensor(correct_responses)]\n",
    "\n",
    "        for i in range(len(out_tokens)): # Iterate over n_samples\n",
    "            try:\n",
    "                output = int(self.tokenizer.decode(out_tokens[i]))\n",
    "                score  = int(output == correct_responses[i])\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(\"!! Can't convert to int !!:\", e)\n",
    "                output = self.tokenizer.decode(out_tokens[i])\n",
    "                score = 0\n",
    "            total_score += score\n",
    "            if type(problem_type) == list:\n",
    "                scores_per_pt[curr_problem_type] += [score]\n",
    "            outputs += [output]\n",
    "            if verbose == 2 or (verbose and i == 0):\n",
    "                print(\"Actual values:             \",\n",
    "                      \"first number:\", x[i], \"second number:\", y[i], \n",
    "                      curr_problem_type + \":\", correct_responses[i],\n",
    "                      \"LLM response:\", output, \"score:\", score)\n",
    "\n",
    "        response_data += [\"Model Guesses:\", output]\n",
    "        response_data += [\"Correct Answer:\", correct_responses]\n",
    "\n",
    "    max_len = max(t.size(0) for t in all_logits)\n",
    "    padded_tensors = []\n",
    "    for t in all_logits:\n",
    "        T, B, V = t.shape\n",
    "        pad_amount = max_len - T\n",
    "        t_padded = F.pad(t, (0, 0, 0, 0, 0, pad_amount), value=0)\n",
    "        padded_tensors.append(t_padded)\n",
    "\n",
    "    all_logits = torch.cat(padded_tensors, dim=1)\n",
    "    #all_x      = torch.concat(all_x)\n",
    "    #all_y      = torch.concat(all_y)\n",
    "    all_corr   = torch.concat(all_corr)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "    pt_index = 0\n",
    "    for batch in range(len(all_corr)):\n",
    "        correct_tokens = torch.tensor(self.tokenizer.encode(str(all_corr[batch].item()), bos=False, eos=False))\n",
    "        correct_sequence_length  = len(correct_tokens)\n",
    "        response_sequence_length = all_logits[:,batch,:].shape[0]\n",
    "        sequence_length = min(correct_sequence_length, response_sequence_length)\n",
    "        batch_loss = criterion(all_logits[:sequence_length,batch,:], correct_tokens[:sequence_length])\n",
    "        loss += batch_loss\n",
    "        losses_per_pt[pts[batch//n_samples]] += [batch_loss.detach().cpu().float()]\n",
    "        wandb.log({\n",
    "            f\"epoch_{pts[batch//n_samples]}\": len(losses_per_pt[pts[batch//n_samples]])-1,\n",
    "            f\"loss_{pts[batch//n_samples]}\":  losses_per_pt[pts[batch//n_samples]][-1],\n",
    "            f\"score_{pts[batch//n_samples]}\": scores_per_pt[pts[batch//n_samples]][-1],\n",
    "        })\n",
    "\n",
    "    loss = loss / len(all_corr)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    total_score = total_score / len(all_corr)\n",
    "\n",
    "    if verbose:\n",
    "        tn = 0\n",
    "        for p in self.model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                tn += param_norm.item() ** 2\n",
    "        total_norm = (tn ** 0.5) / len(dialogs) # normalize by batch size\n",
    "        print(f\"Total gradient norm after clipping: {total_norm}\")\n",
    "\n",
    "    return total_loss, total_score, response_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db2cff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_step(n_samples, self, temperature=0, problem_type=\"addition\", add_noise=False,\n",
    "                   bypass_symbolic=False, inference_to_backprop_ratio=1,\n",
    "                   symbolic_encoding_layer=17, symbolic_decoding_layers=[17], criterion=None,\n",
    "                   cot=False, normalize_vector=False, complexity=2, rms_layer=False, double_rep=True, verbose=False):\n",
    "\n",
    "    all_logits = []\n",
    "    #all_x     = []\n",
    "    #all_y     = []\n",
    "    all_corr  = []\n",
    "\n",
    "    total_score = 0\n",
    "    outputs = []\n",
    "    pts     = []\n",
    "\n",
    "    for n in range(inference_to_backprop_ratio):\n",
    "        if verbose:\n",
    "            print(\"On sub-epoch iteration:\", n+1)\n",
    "        response_data = []\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        if self.model.test_on_unrelated_questions:\n",
    "            dialogs, correct_responses, curr_problem_type = generate_non_math_dialog(samples=n_samples, topic=\"random\", cot=cot,\n",
    "                                                                                     string_nums=self.model.test_with_non_numerical_rep)\n",
    "        else:\n",
    "            dialogs, x, y, curr_problem_type = generate_dialog(complexity=complexity, samples=n_samples, \n",
    "                                                               problem_type=problem_type, cot=cot,\n",
    "                                                               string_nums=self.model.test_with_non_numerical_rep)\n",
    "            if self.model.encoder_input_tokens == \"all\":\n",
    "                start_indices, end_indices    = get_dialog_indices(dialogs, calculate_end_index=self.model.calculate_end_index)\n",
    "                self.model.curr_start_indices = start_indices\n",
    "                self.model.curr_end_indices   = end_indices\n",
    "                self.model.dialogs            = dialogs\n",
    "\n",
    "            if self.model.calculate_encoding_accuracy:\n",
    "                self.model.curr_x = x\n",
    "                self.model.curr_y = y\n",
    "\n",
    "        if self.model.test_with_non_numerical_rep:\n",
    "            conv = lambda x: w2n.word_to_num(str(x))\n",
    "            conv_inv = lambda x: n2w.num2words(int(x))\n",
    "            conv_inv = lambda x: x\n",
    "\n",
    "        else:\n",
    "            conv = lambda x: x\n",
    "            conv_inv = lambda x: x\n",
    "\n",
    "\n",
    "\n",
    "        pts += [curr_problem_type]\n",
    "        if curr_problem_type==\"addition\":\n",
    "            correct_responses = [conv_inv(conv(x[i]) + conv(y[i])) for i in range(len(x))]\n",
    "        if curr_problem_type==\"multiplication\":\n",
    "            correct_responses = [conv_inv((conv(x[i]) * conv(y[i])) % 10**(complexity+1)) for i in range(len(x))]\n",
    "        if curr_problem_type==\"division\":\n",
    "            correct_responses = [conv_inv(int(conv(x[i]) // conv(y[i]))) for i in range(len(x))]\n",
    "        if curr_problem_type==\"modulo\":\n",
    "            correct_responses = [conv_inv(conv(x[i]) % conv(y[i])) for i in range(len(x))]\n",
    "        if curr_problem_type==\"gcd\":\n",
    "            correct_responses = [conv_inv(np.gcd(conv(x[i]), conv(y[i]))) for i in range(len(x))]\n",
    "        if curr_problem_type==\"lcm\":\n",
    "            correct_responses = [conv_inv(np.lcm(conv(x[i]), conv(y[i])) % 10**(complexity+1)) for i in range(len(x))]\n",
    "        if curr_problem_type==\"square_mod\":\n",
    "            correct_responses = [conv_inv(conv(x[i])**2 % conv(y[i])) for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_and\":\n",
    "            correct_responses = [conv_inv(conv(x[i]) & conv(y[i])) for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_xor\":\n",
    "            correct_responses = [conv_inv(conv(x[i]) ^ conv(y[i])) for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_or\":\n",
    "            correct_responses = [conv_inv(conv(x[i]) | conv(y[i])) for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_nand\":\n",
    "            correct_responses = [conv_inv(~(conv(x[i]) & conv(y[i]))) for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_nxor\":\n",
    "            correct_responses = [conv_inv(~(conv(x[i]) ^ conv(y[i]))) for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_nor\":\n",
    "            correct_responses = [conv_inv(~(conv(x[i]) | conv(y[i]))) for i in range(len(x))]\n",
    "            \n",
    "\n",
    "        # if using cot, set the max decoding length to a large value, otherwise set it to a small value\n",
    "        if cot:\n",
    "            mdl = min(1000, max_seq_len)\n",
    "        else:\n",
    "            mdl = complexity+5\n",
    "\n",
    "        #if verbose:\n",
    "        #    print(\"Max decoding length:\", mdl)\n",
    "\n",
    "        # Shape of list_of_probs and list_of_logits is (sequence_output_length, batch_size, num_tokens)\n",
    "        h_stack, list_of_probs, list_of_logits, out_tokens = episode(dialogs=dialogs, self=self, temperature=temperature,\n",
    "                                                                     inference_mode=self.model.forward_symbolic_funnel, \n",
    "                                                                     bypass_symbolic=bypass_symbolic, max_decoding_length=mdl,\n",
    "                                                                     problem_type=curr_problem_type, add_noise=add_noise, \n",
    "                                                                     symbolic_encoding_layer=symbolic_encoding_layer, \n",
    "                                                                     symbolic_decoding_layers=symbolic_decoding_layers, \n",
    "                                                                     normalize_vector=normalize_vector, complexity=complexity, \n",
    "                                                                     rms_layer=rms_layer, double_rep=double_rep, verbose=verbose)\n",
    "\n",
    "        if cot and not self.model.test_on_unrelated_questions:\n",
    "            token_for_final = 19918\n",
    "            bold_token = 334 # Sometimes llama outputs Final Answer in bold markdown (** symbol)\n",
    "            modified_list_of_logits = []\n",
    "            modified_out_tokens     = []\n",
    "            modified_list_of_probs  = []\n",
    "            for i in range(len(out_tokens)):\n",
    "                if token_for_final not in out_tokens[i]: # 19918 is the token for the word \"Final\":\n",
    "                    if verbose == 1 and i == 0:\n",
    "                        print(\"COT response does not contain the phrase 'Final Answer:' as required:\\n\", \n",
    "                              self.tokenizer.decode(out_tokens[i]))\n",
    "                    elif verbose == 2:\n",
    "                        print(\"COT response does not contain the phrase 'Final Answer:' as required:\\n\", \n",
    "                              self.tokenizer.decode(out_tokens[i]))\n",
    "                    modified_out_tokens     += [out_tokens[i]]\n",
    "                    modified_list_of_logits += [list_of_logits[:,i,:]]\n",
    "                    modified_list_of_probs  += [list_of_probs [:,i,:]]\n",
    "                else:\n",
    "                    if verbose == 1 and i == 0:\n",
    "                        print(\"COT response:\\n\", \n",
    "                              self.tokenizer.decode(out_tokens[i]))\n",
    "                    if verbose == 2:\n",
    "                        print(\"COT response:\\n\", \n",
    "                              self.tokenizer.decode(out_tokens[i]))\n",
    "                    # The phrase \"Final Answer: \" should be 4 tokens long, so we skip that many tokens to get the answer\n",
    "                    if bold_token in out_tokens[i][(out_tokens[i].index(token_for_final) + 4):   ]:\n",
    "                        fp = (out_tokens[i][(out_tokens[i].index(token_for_final) + 4):].index(bold_token) + \n",
    "                              out_tokens[i].index(token_for_final) + 4)\n",
    "                        modified_out_tokens     += [out_tokens    [  i  ][(out_tokens[i].index(token_for_final) + 4):fp   ]]\n",
    "                        modified_list_of_logits += [list_of_logits[:,i,:][(out_tokens[i].index(token_for_final) + 4):fp,:,]]\n",
    "                        modified_list_of_probs  += [list_of_probs [:,i,:][(out_tokens[i].index(token_for_final) + 4):fp,:,]]\n",
    "                    else:\n",
    "                        modified_out_tokens     += [out_tokens    [  i  ][(out_tokens[i].index(token_for_final) + 4):   ]]\n",
    "                        modified_list_of_logits += [list_of_logits[:,i,:][(out_tokens[i].index(token_for_final) + 4):,:,]]\n",
    "                        modified_list_of_probs  += [list_of_probs [:,i,:][(out_tokens[i].index(token_for_final) + 4):,:,]]\n",
    "                    #print(\"Truncated response :\\n\",\n",
    "                    #      self.tokenizer.decode(modified_out_tokens[i]))\n",
    "\n",
    "                    #print(\"Modified Shapes:\", len(modified_out_tokens[-1]), modified_list_of_logits[-1].shape)\n",
    "\n",
    "            list_of_logits = torch.stack(modified_list_of_logits, axis=1)\n",
    "            list_of_probs  = torch.stack(modified_list_of_probs,  axis=1)\n",
    "            out_tokens     = modified_out_tokens\n",
    "\n",
    "        elif self.model.test_on_unrelated_questions:\n",
    "            for i in range(len(out_tokens)):\n",
    "                output = self.tokenizer.decode(out_tokens[i])\n",
    "\n",
    "                if verbose == 1 and i == 0:\n",
    "                    print(\"COT response on non math problems:\\n\", output)\n",
    "                if verbose == 2:\n",
    "                    print(\"COT response on non math problems:\\n\", output)\n",
    "\n",
    "                response_data += [\"Model Guesses:\", output]\n",
    "                response_data += [\"Correct Answer:\", correct_responses[i]]\n",
    "                \n",
    "            return 0, 0, response_data\n",
    "\n",
    "\n",
    "\n",
    "        all_logits = all_logits + [list_of_logits]\n",
    "        #all_x      = all_x      + [torch.tensor(x)]\n",
    "        #all_y      = all_y      + [torch.tensor(y)]\n",
    "        all_corr   = all_corr   + [torch.tensor(correct_responses)]\n",
    "\n",
    "        for i in range(len(out_tokens)): # Iterate over n_samples\n",
    "            try:\n",
    "                output = int(self.tokenizer.decode(out_tokens[i]))\n",
    "                score  = int(output == correct_responses[i])\n",
    "            except Exception as e:\n",
    "                if verbose == 1 and i == 0:\n",
    "                    print(\"!! Can't convert to int !!:\", e)\n",
    "                elif verbose == 2:\n",
    "                    print(\"!! Can't convert to int !!:\", e)\n",
    "                output = self.tokenizer.decode(out_tokens[i])\n",
    "                score = 0\n",
    "            total_score += score\n",
    "            outputs += [output]\n",
    "            if verbose == 2 or (verbose and i == 0):\n",
    "                print(\"Actual values:             \",\n",
    "                      \"first number:\", x[i], \"second number:\", y[i], \n",
    "                      curr_problem_type + \":\", correct_responses[i],\n",
    "                      \"LLM response:\", output, \"score:\", score)\n",
    "\n",
    "            response_data += [\"Model Guesses:\", output]\n",
    "            response_data += [\"Correct Answer:\", correct_responses[i]]\n",
    "\n",
    "    max_len = max(t.size(0) for t in all_logits)\n",
    "    padded_tensors = []\n",
    "    for t in all_logits:\n",
    "        T, B, V = t.shape\n",
    "        pad_amount = max_len - T\n",
    "        t_padded = F.pad(t, (0, 0, 0, 0, 0, pad_amount), value=0)\n",
    "        padded_tensors.append(t_padded)\n",
    "\n",
    "    all_logits = torch.cat(padded_tensors, dim=1)\n",
    "    #all_x      = torch.concat(all_x)\n",
    "    #all_y      = torch.concat(all_y)\n",
    "    all_corr   = torch.concat(all_corr)\n",
    "\n",
    "    loss = 0\n",
    "    for batch in range(len(all_corr)):\n",
    "        correct_tokens = torch.tensor(self.tokenizer.encode(str(all_corr[batch].item()), bos=False, eos=False))\n",
    "        correct_sequence_length  = len(correct_tokens)\n",
    "        response_sequence_length = all_logits[:,batch,:].shape[0]\n",
    "        sequence_length = min(correct_sequence_length, response_sequence_length)\n",
    "        batch_loss = criterion(all_logits[:sequence_length,batch,:], correct_tokens[:sequence_length])\n",
    "        loss += batch_loss\n",
    "    loss = loss / len(all_corr)\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    total_score = total_score / len(all_corr)\n",
    "\n",
    "    return total_loss, total_score, response_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d88b7a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(testing_n_samples, testing_num_epochs, testing_temperature, problem_type, self, add_noise=False, criterion=None, \n",
    "                   bypass_symbolic=False, inference_to_backprop_ratio=1, symbolic_encoding_layer=17, symbolic_decoding_layers=[17],\n",
    "                   normalize_vector=False, complexity=2, rms_layer=False, double_rep=True, cot=False,\n",
    "                   testing_epochs_to_print=10, testing_verbose=0):\n",
    "    losses    = []\n",
    "    scores    = []\n",
    "    responses = []\n",
    "    with torch.no_grad():\n",
    "        for epoch in range(testing_num_epochs):\n",
    "            loss, score, response_data = inference_step(n_samples=testing_n_samples, self=self,\n",
    "                                                        temperature=testing_temperature, problem_type=problem_type, \n",
    "                                                        add_noise=add_noise, bypass_symbolic=bypass_symbolic, \n",
    "                                                        inference_to_backprop_ratio=inference_to_backprop_ratio, \n",
    "                                                        symbolic_encoding_layer=symbolic_encoding_layer, \n",
    "                                                        symbolic_decoding_layers=symbolic_decoding_layers, criterion=criterion,\n",
    "                                                        normalize_vector=normalize_vector, complexity=complexity, \n",
    "                                                        rms_layer=rms_layer, double_rep=double_rep, cot=cot, verbose=testing_verbose)\n",
    "            losses += [loss]\n",
    "            scores += [score]\n",
    "            responses += [response_data]\n",
    "            if testing_epochs_to_print and testing_num_epochs // testing_epochs_to_print and not epoch % (testing_num_epochs // testing_epochs_to_print):\n",
    "                #print(f\" -------------- Epoch {epoch}, Loss: {np.mean(losses)}, Score: {np.mean(scores)}  -------------- \")\n",
    "                print(f\" -------------- Epoch {epoch}, Loss: {loss}, Score: {score}  -------------- \")\n",
    "    return losses, scores, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df6eb067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(losses, scores, problem_type, bypass_symbolic):\n",
    "    losses = np.array(losses)\n",
    "    scores = np.array(scores)\n",
    "\n",
    "    if bypass_symbolic == 1:\n",
    "        output_text = f\"Mean score and loss of standard LLM on {problem_type}: \" + str(round(scores.mean()*100, 3)) + \" ± \" + str(round(scores.std()*100, 4)) + \", \" + str(round(losses.mean(), 3)) + \" ± \" + str(round(losses.std(), 4))\n",
    "    else:\n",
    "        output_text = f\"Mean score and loss of symbolic LLM on {problem_type}: \" + str(round(scores.mean()*100, 3)) + \" ± \" + str(round(scores.std()*100, 4)) + \", \" + str(round(losses.mean(), 3)) + \" ± \" + str(round(losses.std(), 4))\n",
    "    print(f\"\\n\", output_text)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f06a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_smooth_data(curve, n=3, starting_index=10):\n",
    "    data = curve[starting_index:]\n",
    "    smoothed_list = []\n",
    "    length = len(data)\n",
    "    for i in range(length):\n",
    "        # Determine the range of indices to average\n",
    "        start = max(0, i - n)\n",
    "        end = min(length, i + n + 1)\n",
    "        # Calculate the average of the surrounding elements\n",
    "        smoothed_list.append(sum(data[start:end]) / (end - start))\n",
    "    return smoothed_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57084fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(self, config):\n",
    "    encoder_path                        = config['encoder_path']\n",
    "    decoder_path                        = config['decoder_path']\n",
    "    save_model                          = config['save_model']\n",
    "    problem_type                        = config['problem_type']\n",
    "    complexity                          = config['complexity']\n",
    "    temperature                         = config['temperature']\n",
    "\n",
    "    train_model                         = config['train_model']\n",
    "    lora_baseline                       = config['lora_baseline']\n",
    "    starting_skip_strength              = config['starting_skip_strength']\n",
    "    problem_score_threshold             = config['problem_score_threshold']\n",
    "    normalize_SP_before_dot             = config['normalize_SP_before_dot']\n",
    "    rms_layer                           = config['rms_layer']\n",
    "    double_rep                          = config['double_rep']\n",
    "    use_specific_identities             = config['use_specific_identities']\n",
    "    initialize_decoders                 = config['initialize_decoders']\n",
    "    normalize_vector                    = config['normalize_vector']\n",
    "    symbolic_encoding_layer             = config['symbolic_encoding_layer']\n",
    "    symbolic_decoding_layers            = config['symbolic_decoding_layers']\n",
    "\n",
    "    num_epochs                          = config['num_epochs']\n",
    "    n_samples                           = config['n_samples']\n",
    "    inference_to_backprop_ratio         = config['inference_to_backprop_ratio']\n",
    "    trainable_skip                      = config['trainable_skip']\n",
    "    learning_rate                       = config['learning_rate']\n",
    "    learning_rate_reduction_factors     = config['learning_rate_reduction_factors']\n",
    "    epochs_to_print                     = config['epochs_to_print']\n",
    "    print_all_pts_freq                  = config['print_all_pts_freq']\n",
    "    verbose                             = config['verbose']\n",
    "\n",
    "    testing_problems                    = config['testing_problems']\n",
    "    testing_num_epochs                  = config['testing_num_epochs']\n",
    "    testing_inference_to_backprop_ratio = config['testing_inference_to_backprop_ratio']\n",
    "    testing_n_samples                   = config['testing_n_samples']\n",
    "    testing_temperature                 = config['testing_temperature']\n",
    "    testing_epochs_to_print             = config['testing_epochs_to_print']\n",
    "    testing_verbose                     = config['testing_verbose']\n",
    "    record_score_per_problem            = config['record_score_per_problem']\n",
    "    test_baseline                       = config['test_baseline']\n",
    "    cot                                 = config['cot']\n",
    "    \n",
    "    test_on_unrelated_questions         = config['test_on_unrelated_questions']\n",
    "    test_with_non_numerical_rep         = config['test_with_non_numerical_rep']\n",
    "\n",
    "    encoder_input_tokens                = config['encoder_input_tokens']\n",
    "    calculate_end_index                 = config['calculate_end_index']\n",
    "    \n",
    "    multi_token_intervention            = config[\"multi_token_intervention\"]\n",
    "    static_encoding                     = config[\"static_encoding\"]\n",
    "    calculate_encoding_accuracy         = config[\"calculate_encoding_accuracy\"]\n",
    "\n",
    "    #######################################################################################\n",
    "    ############################## Hyperparameter Definition ##############################\n",
    "    #######################################################################################\n",
    "\n",
    "    if \"post_fine_tuning\" in decoder_path:\n",
    "        initialize_decoders = False\n",
    "    \n",
    "    if test_baseline == 2:\n",
    "        train_model = False\n",
    "\n",
    "    if cot == True:\n",
    "        test_baseline = 2\n",
    "        train_model = False\n",
    "\n",
    "    if test_on_unrelated_questions == True:\n",
    "        cot = True\n",
    "        testing_problems = ['philosophy', 'ethics', 'history', 'psychology', 'science_fiction', 'technology', 'art_and_culture']\n",
    "        test_baseline = 0\n",
    "        train_model = False\n",
    "\n",
    "\n",
    "    if type(problem_type) == list:\n",
    "        losses_per_pt = {pt: [] for pt in problem_type}\n",
    "        scores_per_pt = {pt: [] for pt in problem_type}\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    save_path  = decoder_path.split(\"/\")[-1].split(\".pth\")[0] + f\"_post_fine_tuning_{wandb.run.id}_\"\n",
    "\n",
    "    resume    = False\n",
    "    \n",
    "\n",
    "    #################################################################################\n",
    "    ############################## Model preprocessing ##############################\n",
    "    #################################################################################\n",
    "    \n",
    "\n",
    "    self.model.encoders = torch.load(encoder_path)\n",
    "    self.model.decoders = torch.load(decoder_path)\n",
    "\n",
    "    if lora_baseline:\n",
    "        lora_encoders = nn.ModuleList()\n",
    "        lora_decoders = nn.ModuleList()\n",
    "        for layer_id in torch.stack([self.model.encoders[i].layer_id for i in range(len(self.model.encoders))]):\n",
    "            # Assume that linear encoder and decoder networks are used\n",
    "            lora_encoder = Encoder(layer_id, self.model.output.weight.shape[1], self.model.SE.SP_dim).to(device)\n",
    "            lora_decoder = Decoder(layer_id, self.model.SE.SP_dim, self.model.output.weight.shape[1]).to(device)\n",
    "            lora_encoders.append(lora_encoder)\n",
    "            lora_decoders.append(lora_decoder)\n",
    "\n",
    "        self.model.encoders = lora_encoders\n",
    "        self.model.decoders = lora_decoders\n",
    "\n",
    "        initialize_decoders = False\n",
    "        rms_layer = True\n",
    "\n",
    "    self.model.lora_baseline               = lora_baseline\n",
    "\n",
    "    self.model.problem_score_threshold     = problem_score_threshold\n",
    "    self.model.training_problems           = problem_type\n",
    "    self.model.record_score_per_problem    = record_score_per_problem\n",
    "    self.model.normalize_SP_before_dot     = normalize_SP_before_dot\n",
    "    self.model.use_specific_identities     = use_specific_identities\n",
    "    self.model.test_on_unrelated_questions = test_on_unrelated_questions\n",
    "    self.model.test_with_non_numerical_rep = test_with_non_numerical_rep\n",
    "    \n",
    "    self.model.encoder_input_tokens        = encoder_input_tokens\n",
    "    self.model.calculate_end_index         = calculate_end_index\n",
    "\n",
    "    self.model.multi_token_intervention    = multi_token_intervention\n",
    "    self.model.static_encoding             = static_encoding\n",
    "    self.model.calculate_encoding_accuracy = calculate_encoding_accuracy\n",
    "\n",
    "    if self.model.calculate_encoding_accuracy:\n",
    "        # During training, calculate accuracy per problem type, per digit, per input number\n",
    "        self.model.encoding_accuracy = {}\n",
    "        for pt in problem_type:\n",
    "            self.model.encoding_accuracy[pt] = {}\n",
    "            for digit in range(complexity + 1):\n",
    "                self.model.encoding_accuracy[pt][\"digit \" + str(digit)] = {}\n",
    "                self.model.encoding_accuracy[pt][\"digit \" + str(digit)][\"first_number\"]  = []\n",
    "                self.model.encoding_accuracy[pt][\"digit \" + str(digit)][\"second_number\"] = []\n",
    "    \n",
    "\n",
    "    starting_encoder_layer = 0\n",
    "    for i in range(len(self.model.encoders)):\n",
    "        if (self.model.encoders[i]) != type(None):\n",
    "            starting_encoder_layer = i\n",
    "            break\n",
    "\n",
    "    starting_decoder_layer = 0\n",
    "    for i in range(len(self.model.decoders)):\n",
    "        if (self.model.decoders[i]) != type(None):\n",
    "            starting_decoder_layer = i\n",
    "            break\n",
    "\n",
    "    self.model.starting_encoder_layer = starting_encoder_layer\n",
    "    self.model.starting_decoder_layer = starting_decoder_layer\n",
    "\n",
    "    self.model.encoders.eval()\n",
    "\n",
    "    if rms_layer:\n",
    "        self.model.rms_layers = [] \n",
    "        for sl in symbolic_decoding_layers:\n",
    "            if sl != 33:\n",
    "                self.model.rms_layers.append(RMSNorm(self.model.output.weight.shape[1], eps=1e-05)) # params.dim, eps=params.norm_eps\n",
    "            else:\n",
    "                self.model.rms_layers.append(RMSNorm(self.model.output.weight.shape[0], eps=1e-05)) # num_tokens, eps=params.norm_eps\n",
    "    else:\n",
    "        self.model.skip_weights = nn.Parameter(torch.zeros(len(symbolic_decoding_layers)) + starting_skip_strength)\n",
    "\n",
    "    if initialize_decoders:\n",
    "        pseudo_inverses = {}\n",
    "        for sl in symbolic_decoding_layers:\n",
    "            for p in self.model.encoders[sl-self.model.starting_decoder_layer].parameters():\n",
    "                pseudo_inverses[sl-self.model.starting_decoder_layer] = torch.linalg.pinv(p.float()).to(torch.bfloat16)\n",
    "\n",
    "        for sl in symbolic_decoding_layers:\n",
    "            for p in self.model.decoders[sl-self.model.starting_decoder_layer].parameters():\n",
    "                p = pseudo_inverses[sl-self.model.starting_decoder_layer]\n",
    "\n",
    "    if 33 in symbolic_decoding_layers:\n",
    "        self.model.decoders.append(ColumnParallelLinear(\n",
    "            self.model.SE.SP_dim, self.model.output.weight.shape[0], bias=False, init_method=lambda x: x\n",
    "        ))\n",
    "        print(\"Created 33rd decoder network\")\n",
    "\n",
    "    # Delete unnecessary layers to save memory\n",
    "    for i in range(len(self.model.decoders)):\n",
    "        if i not in symbolic_decoding_layers:\n",
    "            del self.model.decoders[i]  # Delete layer\n",
    "            self.model.decoders.insert(i, None)  # Insert None to maintain indexing\n",
    "\n",
    "    for i in range(len(self.model.encoders)):\n",
    "        if i != symbolic_encoding_layer:\n",
    "            del self.model.encoders[i]  # Delete layer\n",
    "            self.model.encoders.insert(i, None)  # Insert None to maintain indexing\n",
    "\n",
    "\n",
    "\n",
    "    if record_score_per_problem == 1:\n",
    "        with open(\"outputs/score_per_problem_training_and_testing.txt\", \"w\") as file:\n",
    "            file.write(\"actual_problem_type,problem_type,score\\n\")\n",
    "\n",
    "\n",
    "    if train_model:\n",
    "        #############################################################################\n",
    "        ############################## Train the model ##############################\n",
    "        #############################################################################\n",
    "\n",
    "        if not resume:\n",
    "            losses = []\n",
    "            scores = []\n",
    "            responses = []\n",
    "\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for sl in symbolic_decoding_layers:\n",
    "                for param in self.model.decoders[sl-self.model.starting_decoder_layer].parameters():\n",
    "                    param.requires_grad = True\n",
    "            if lora_baseline:\n",
    "                for param in self.model.encoders[symbolic_encoding_layer-self.model.starting_encoder_layer].parameters():\n",
    "                    param.requires_grad=True\n",
    "            if rms_layer:\n",
    "                for r_layer in self.model.rms_layers:\n",
    "                    for param in r_layer.parameters():\n",
    "                        param.requires_grad = True\n",
    "\n",
    "\n",
    "            if not rms_layer:\n",
    "                self.model.skip_weights.requires_grad = trainable_skip\n",
    "\n",
    "            #original_weights = {}\n",
    "            #for name, param in self.model.named_parameters():\n",
    "            #    for sl in symbolic_decoding_layers:\n",
    "            #        if f\"decoders.{sl-self.model.starting_decoder_layer}.decoder_layer\" in name  or name == \"layers.0.feed_forward.w1.weight\":\n",
    "            #            original_weights[name] = param.clone().detach()\n",
    "\n",
    "\n",
    "\n",
    "            # Training loop\n",
    "\n",
    "            params = list(filter(lambda p: p.requires_grad, self.model.parameters()))\n",
    "            print(\"Number of trainable parameters:\", sum(p.numel() for p in params))\n",
    "        else:\n",
    "            print(\"Resuming training\")\n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=learning_rate)\n",
    "            torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "            self.model.train()\n",
    "            if epoch in learning_rate_reduction_factors.keys():\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = param_group['lr'] * learning_rate_reduction_factors[epoch]  # Set new learning rate\n",
    "                    print(\"Learning Rate changed to:\", param_group['lr'])\n",
    "\n",
    "            loss, score, response_data = training_step(n_samples, self, temperature=temperature, problem_type=problem_type, \n",
    "                                                       inference_to_backprop_ratio=inference_to_backprop_ratio, \n",
    "                                                       symbolic_encoding_layer=symbolic_encoding_layer, \n",
    "                                                       symbolic_decoding_layers=symbolic_decoding_layers, \n",
    "                                                       optimizer=optimizer, criterion=criterion,\n",
    "                                                       normalize_vector=normalize_vector, complexity=complexity, \n",
    "                                                       losses_per_pt=losses_per_pt, scores_per_pt=scores_per_pt,\n",
    "                                                       rms_layer=rms_layer, double_rep=double_rep, verbose=verbose)\n",
    "            losses += [loss]\n",
    "            scores += [score]\n",
    "            responses += [response_data]\n",
    "\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"loss\":  loss,\n",
    "                \"score\": score\n",
    "            })\n",
    "\n",
    "            for n, sw in enumerate(self.model.skip_weights.detach().cpu().float().numpy()):\n",
    "                wandb.log({f\"skip_weights_{n}\": sw})\n",
    "\n",
    "            if epochs_to_print and num_epochs // epochs_to_print and not epoch % (num_epochs // epochs_to_print):\n",
    "                if num_epochs // epochs_to_print >= 10:\n",
    "                    if not rms_layer and trainable_skip:\n",
    "                        print(f\" -------------- Epoch {epoch}, Loss: {np.mean(losses)}, Score: {np.mean(scores)}, Skip Weight: {self.model.skip_weights.detach().cpu().float().numpy()}\"\"  -------------- \", flush=True)\n",
    "                    else:\n",
    "                        print(f\" -------------- Epoch {epoch}, Loss: {np.mean(losses)}, Score: {np.mean(scores)}  -------------- \", flush=True)\n",
    "                else:\n",
    "                    if not rms_layer and trainable_skip:\n",
    "                        print(f\" -------------- Epoch {epoch}, Loss: {loss}, Score: {score}, Skip Weight: {self.model.skip_weights.detach().cpu().float().numpy()}\"\"  -------------- \", flush=True)\n",
    "                    else:\n",
    "                        print(f\" -------------- Epoch {epoch}, Loss: {loss}, Score: {score}  -------------- \", flush=True)\n",
    "\n",
    "            if epoch and not epoch % print_all_pts_freq:\n",
    "                print(\"~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\")\n",
    "                for pt in losses_per_pt:\n",
    "                    if len(losses_per_pt[pt]):\n",
    "                        print(\"    Problem type:\", pt)\n",
    "                        print(\"    Loss:\", np.mean(losses_per_pt[pt]), \", Score:\", np.mean(scores_per_pt[pt]))\n",
    "\n",
    "\n",
    "\n",
    "        #wandb.log({\"losses\": losses})\n",
    "        #wandb.log({\"scores\": scores})\n",
    "        #wandb.log({\"losses_per_pt\": losses_per_pt})\n",
    "        #wandb.log({\"scores_per_pt\": scores_per_pt})\n",
    "\n",
    "        ###################################################################################\n",
    "        ############################## Plot Training Metrics ##############################\n",
    "        ###################################################################################\n",
    "\n",
    "        if save_model:\n",
    "            # Add rms and skip connection paramters to decoder to be saved\n",
    "            if rms_layer:\n",
    "                self.model.decoders.rms_layer   = nn.ModuleList(self.model.rms_layers)\n",
    "            if trainable_skip:\n",
    "                self.model.decoders.skip_weight = self.model.skip_weights\n",
    "                \n",
    "            current_datetime = datetime.now()\n",
    "            formatted_string = current_datetime.strftime(\"%Y_%m_%d\")\n",
    "            torch.save(self.model.decoders, \"models/\" + save_path.split(\".pth\")[0] + f\"{formatted_string}\" + \".pth\")\n",
    "            print(\"Saved\", \"models/\" + save_path.split(\".pth\")[0] + f\"{formatted_string}\" + \".pth\")\n",
    "\n",
    "        if not rms_layer and trainable_skip:\n",
    "\n",
    "            print(\"Skip Weight strength after training:\", self.model.skip_weights.detach().cpu().float().numpy())\n",
    "\n",
    "        plt.plot(create_smooth_data(np.array(scores), n=(epoch+1)//10))\n",
    "        #plt.title(f\"Score vs Epoch (smoothing factor = {(epoch+1) // 10})\")\n",
    "        plt.title(f\"Score vs Epoch\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        if verbose:\n",
    "            plt.show()\n",
    "        wandb.log({\"score_vs_epoch\": wandb.Image(plt)})  # Log to wandb\n",
    "        plt.close()\n",
    "\n",
    "        plt.plot(create_smooth_data(np.array(losses), n=(epoch+1)//10))\n",
    "        #plt.title(f\"Loss vs Epoch (smoothing factor = {(epoch+1) // 10})\")\n",
    "        plt.title(f\"Loss vs Epoch\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        if verbose:\n",
    "            plt.show()\n",
    "        wandb.log({\"loss_vs_epoch\": wandb.Image(plt)})  # Log to wandb\n",
    "        plt.close()\n",
    "\n",
    "        if type(problem_type) == list:\n",
    "            for pt in problem_type:\n",
    "                print(\"On problem type:\", pt)\n",
    "                pt_scores = np.array(scores_per_pt[pt])\n",
    "                pt_losses = np.array(losses_per_pt[pt])\n",
    "\n",
    "                plt.plot(create_smooth_data(pt_scores, n=len(pt_scores)//10))\n",
    "                #plt.title(f\"{pt} Score vs Epoch (smoothing factor = {len(pt_scores) // 10})\")\n",
    "                plt.title(f\"{pt} Score vs Epoch\")\n",
    "                plt.xlabel(\"Epochs\")\n",
    "                plt.ylabel(\"Score\")\n",
    "                if verbose:\n",
    "                    plt.show()\n",
    "                wandb.log({f\"{pt}_score_vs_epoch\": wandb.Image(plt)})  # Log to wandb\n",
    "                plt.close()\n",
    "\n",
    "                plt.plot(create_smooth_data(pt_losses, n=len(pt_losses)//10))\n",
    "                #plt.title(f\"{pt} Loss vs Epoch (smoothing factor = {len(pt_losses) // 10})\")\n",
    "                plt.title(f\"{pt} Loss vs Epoch\")\n",
    "                plt.xlabel(\"Epochs\")\n",
    "                plt.ylabel(\"Loss\")\n",
    "                if verbose:\n",
    "                    plt.show()\n",
    "                wandb.log({f\"{pt}_loss_vs_epoch\": wandb.Image(plt)})  # Log to wandb\n",
    "                plt.close()\n",
    "\n",
    "                print(\"Final Score and Loss:\", np.mean(pt_scores[:-10]), np.mean(pt_losses[:-10]))\n",
    "                wandb.log({f\"final_score_{pt}\": np.mean(pt_scores[:-10])})  # Log to wandb\n",
    "                wandb.log({f\"final_loss_{pt}\" : np.mean(pt_losses[:-10])})  # Log to wandb\n",
    "\n",
    "    #####################################################################\n",
    "    ############################## Testing ##############################\n",
    "    #####################################################################\n",
    "\n",
    "    #if type(problem_type) == str:\n",
    "    #    testing_problems = [problem_type]\n",
    "    #else:\n",
    "    #    testing_problems = problem_type\n",
    "    \n",
    "    if record_score_per_problem == 2:\n",
    "        with open(\"outputs/score_per_problem.txt\", \"w\") as file:\n",
    "            file.write(\"actual_problem_type,problem_type,score\\n\")\n",
    "\n",
    "    testing_losses_per_pt_SYM = {}\n",
    "    testing_losses_per_pt_LLM = {}\n",
    "    testing_scores_per_pt_SYM = {}\n",
    "    testing_scores_per_pt_LLM = {}\n",
    "    \n",
    "\n",
    "\n",
    "    for pt in testing_problems:\n",
    "        print(\"~~~~~~~~ Problem Type:\", pt, \"~~~~~~~~\")\n",
    "        if test_baseline != 2:\n",
    "            # Symbolic LLM\n",
    "            bypass_symbolic = False\n",
    "            add_noise       = False\n",
    "            losses, scores, responses = evaluate_model(testing_n_samples=testing_n_samples,\n",
    "                                                       testing_num_epochs=testing_num_epochs,\n",
    "                                                       testing_temperature=testing_temperature, problem_type=pt,\n",
    "                                                       self=self, add_noise=add_noise,\n",
    "                                                       bypass_symbolic=bypass_symbolic, criterion=criterion,\n",
    "                                                       inference_to_backprop_ratio=testing_inference_to_backprop_ratio,\n",
    "                                                       symbolic_encoding_layer=symbolic_encoding_layer,\n",
    "                                                       symbolic_decoding_layers=symbolic_decoding_layers,\n",
    "                                                       normalize_vector=normalize_vector, complexity=complexity,\n",
    "                                                       testing_epochs_to_print=testing_epochs_to_print,\n",
    "                                                       rms_layer=rms_layer, double_rep=double_rep, cot=cot,\n",
    "                                                       testing_verbose=testing_verbose)\n",
    "            testing_losses_per_pt_SYM[pt] = losses\n",
    "            testing_scores_per_pt_SYM[pt] = scores\n",
    "\n",
    "            symbolic_output_text = plot_results(losses, scores, pt, bypass_symbolic)\n",
    "            wandb.log({f\"testing_losses_per_pt_SYM_{pt}\": testing_losses_per_pt_SYM[pt]})\n",
    "            wandb.log({f\"average_testing_loss_SYM_{pt}\": np.mean(testing_losses_per_pt_SYM[pt])})\n",
    "            wandb.log({f\"testing_scores_per_pt_SYM_{pt}\": testing_scores_per_pt_SYM[pt]})\n",
    "            wandb.log({f\"average_testing_score_SYM_{pt}\": np.mean(testing_scores_per_pt_SYM[pt])})\n",
    "            wandb.log({f\"symbolic_output_text_{pt}\": symbolic_output_text})\n",
    "\n",
    "        if test_baseline != 0:\n",
    "            # Standard LLM\n",
    "            bypass_symbolic = True\n",
    "            add_noise       = False\n",
    "            losses, scores, responses = evaluate_model(testing_n_samples=testing_n_samples,\n",
    "                                                       testing_num_epochs=testing_num_epochs,\n",
    "                                                       testing_temperature=testing_temperature, problem_type=pt,\n",
    "                                                       self=self, add_noise=add_noise,\n",
    "                                                       bypass_symbolic=bypass_symbolic, criterion=criterion,\n",
    "                                                       inference_to_backprop_ratio=testing_inference_to_backprop_ratio,\n",
    "                                                       symbolic_encoding_layer=symbolic_encoding_layer,\n",
    "                                                       symbolic_decoding_layers=symbolic_decoding_layers,\n",
    "                                                       normalize_vector=normalize_vector, complexity=complexity,\n",
    "                                                       testing_epochs_to_print=testing_epochs_to_print,\n",
    "                                                       rms_layer=rms_layer, double_rep=double_rep, cot=cot,\n",
    "                                                       testing_verbose=testing_verbose)\n",
    "            testing_losses_per_pt_LLM[pt] = losses\n",
    "            testing_scores_per_pt_LLM[pt] = scores\n",
    "\n",
    "            standard_output_text = plot_results(losses, scores, pt, bypass_symbolic)\n",
    "\n",
    "            wandb.log({f\"testing_losses_per_pt_LLM_{pt}\": testing_losses_per_pt_LLM[pt]})\n",
    "            wandb.log({f\"average_testing_loss_LLM_{pt}\": np.mean(testing_losses_per_pt_LLM[pt])})\n",
    "            wandb.log({f\"testing_scores_per_pt_LLM_{pt}\": testing_scores_per_pt_LLM[pt]})\n",
    "            wandb.log({f\"average_testing_score_LLM_{pt}\": np.mean(testing_scores_per_pt_LLM[pt])})\n",
    "            wandb.log({f\"standard_output_text_{pt}\": standard_output_text})\n",
    "\n",
    "\n",
    "    for pt in testing_problems:\n",
    "        if testing_verbose:\n",
    "            print(\"~~~~~~~~ Problem Type:\", pt, \"~~~~~~~~\")\n",
    "        if test_baseline != 2:\n",
    "            plt.hist(testing_losses_per_pt_SYM[pt], bins=75)\n",
    "            plt.xlabel(\"Loss\")\n",
    "            plt.title(f\"{pt} Symbolic Loss Histogram\")\n",
    "            if testing_verbose:\n",
    "                plt.show()\n",
    "            wandb.log({f\"{pt}_symbolic_loss_histogram\": wandb.Image(plt)})  # Log to wandb\n",
    "            plt.close()\n",
    "\n",
    "        if test_baseline != 0:\n",
    "            plt.hist(testing_losses_per_pt_LLM[pt], bins=75)\n",
    "            plt.xlabel(\"Loss\")\n",
    "            plt.title(f\"{pt} Standard Loss Histogram\")\n",
    "            if testing_verbose:\n",
    "                plt.show()\n",
    "            wandb.log({f\"{pt}_standard_loss_histogram\": wandb.Image(plt)})  # Log to wandb\n",
    "            plt.close()\n",
    "\n",
    "    if record_score_per_problem and test_baseline != 2 and not lora_baseline:\n",
    "        df = pd.read_csv(\"outputs/score_per_problem.txt\")\n",
    "        df['training_item'] = [i for i in range(len(df) // len(self.model.training_problems)) \n",
    "                                 for j in range(len(self.model.training_problems))]\n",
    "\n",
    "        untrained_pts = sorted(list(set(config['testing_problems']) - set(self.model.training_problems)))\n",
    "        trained_pts   = self.model.training_problems\n",
    "        bins = 100\n",
    "        leg = []\n",
    "        for pt in untrained_pts:\n",
    "            leg += [pt]\n",
    "            plt.hist(df[df.actual_problem_type.isin([pt])].pivot_table(\n",
    "                index=\"training_item\", values=\"score\", aggfunc=np.max).score, bins=bins, histtype=\"step\")\n",
    "        for pt in trained_pts:\n",
    "            leg += [pt]\n",
    "            plt.hist(df[df.actual_problem_type.isin([pt])].  pivot_table(\n",
    "                index=\"training_item\", values=\"score\", aggfunc=np.max).score, bins=bins, histtype=\"step\")\n",
    "        plt.xlabel(\"Dot product similarity\")\n",
    "        plt.ylabel(\"Number of Samples\")\n",
    "        plt.legend(leg)\n",
    "        if testing_verbose:\n",
    "            plt.show()\n",
    "        wandb.log({f\"problem_type_similarity_histogram_per_problem_type\": wandb.Image(plt)})  # Log to wandb\n",
    "        plt.close()\n",
    "\n",
    "        bins = 100\n",
    "        if df[df.actual_problem_type.isin(untrained_pts)].shape[0]:\n",
    "            plt.hist(df[df.actual_problem_type.isin(untrained_pts)].pivot_table(\n",
    "                index=\"training_item\", values=\"score\", aggfunc=np.max).score, bins=bins, histtype=\"step\")\n",
    "        plt.hist(df[df.actual_problem_type.isin(trained_pts)].  pivot_table(\n",
    "            index=\"training_item\", values=\"score\", aggfunc=np.max).score, bins=bins, histtype=\"step\")\n",
    "        plt.xlabel(\"Dot product similarity\")\n",
    "        plt.ylabel(\"Number of Samples\")\n",
    "        plt.legend([\"Problems not seen during training\", \"Problems seen during training\"], loc=\"upper left\")\n",
    "        if testing_verbose:\n",
    "            plt.show()\n",
    "        wandb.log({f\"problem_type_similarity_histogram\": wandb.Image(plt)})  # Log to wandb\n",
    "        plt.close()\n",
    "\n",
    "        if df[df.actual_problem_type.isin(untrained_pts)].shape[0]:\n",
    "            wandb.log({\"untrained_problem_scores\": df[df.actual_problem_type.isin(untrained_pts)].pivot_table(\n",
    "                           index=\"training_item\", values=\"score\", aggfunc=np.max).score.tolist(),\n",
    "                       \"trained_problem_scores\":   df[df.actual_problem_type.isin(trained_pts)]  .pivot_table(\n",
    "                           index=\"training_item\", values=\"score\", aggfunc=np.max).score.tolist()})\n",
    "        else:\n",
    "            wandb.log({\"trained_problem_scores\":   df[df.actual_problem_type.isin(trained_pts)]  .pivot_table(\n",
    "                           index=\"training_item\", values=\"score\", aggfunc=np.max).score.tolist()})\n",
    "\n",
    "        result = (\n",
    "            df[df.actual_problem_type.isin(trained_pts)].groupby(\"training_item\")\n",
    "            .apply(lambda group: group.loc[group['score'].idxmax()])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        if testing_verbose:\n",
    "            print(\"Number of trained problems with different actual problem types and maximum score identified problem types:\", \n",
    "                  sum(result['actual_problem_type'] != result['problem_type']))\n",
    "        wandb.log({\"different_actual_problem_types_count\": sum(result['actual_problem_type'] != result['problem_type'])})\n",
    "\n",
    "    if self.model.calculate_encoding_accuracy:\n",
    "        average_accuracy_per_pt = {}\n",
    "\n",
    "        problem_types = list(self.model.encoding_accuracy.keys())\n",
    "        digits = list(self.model.encoding_accuracy[problem_types[0]].keys())\n",
    "        \n",
    "        minval = 100\n",
    "        maxval = 0\n",
    "\n",
    "        for pt in problem_types:\n",
    "            average_accuracy_per_pt[pt] = {}\n",
    "            for d in digits:\n",
    "                average_accuracy_per_pt[pt][d] = [0, 0]\n",
    "                average_accuracy_per_pt[pt][d][0] = np.mean(self.model.encoding_accuracy[pt][d][\"first_number\"])*100\n",
    "                average_accuracy_per_pt[pt][d][1] = np.mean(self.model.encoding_accuracy[pt][d][\"second_number\"])*100\n",
    "                \n",
    "                curr_min = min(np.mean(self.model.encoding_accuracy[pt][d][\"first_number\"])*100,\n",
    "                               np.mean(self.model.encoding_accuracy[pt][d][\"second_number\"])*100)\n",
    "                curr_max = max(np.mean(self.model.encoding_accuracy[pt][d][\"first_number\"])*100,\n",
    "                               np.mean(self.model.encoding_accuracy[pt][d][\"second_number\"])*100)\n",
    "\n",
    "                if minval > curr_min:\n",
    "                    minval = curr_min\n",
    "                if maxval < curr_max:\n",
    "                    maxval = curr_max\n",
    "                    \n",
    "        minval = minval // 5 * 5\n",
    "        maxval = min(100, (maxval // 5 + 1) * 5)\n",
    "\n",
    "        # Define colors for first and second number\n",
    "        colors = ['blue', 'orange']\n",
    "\n",
    "        # Create subplots (one per problem type)\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(18, 10))  # Adjust grid based on the number of problem types\n",
    "        axes = axes.flatten()  # Flatten to index easier\n",
    "\n",
    "        for i, (problem_type, digits_data) in enumerate(average_accuracy_per_pt.items()):\n",
    "            ax = axes[i]\n",
    "\n",
    "            digits = list(digits_data.keys())  # ['digit 0', 'digit 1', ...]\n",
    "            x = np.arange(len(digits))  # X-axis positions\n",
    "\n",
    "            first_num = [digits_data[d][0] for d in digits]  # First number accuracies\n",
    "            second_num = [digits_data[d][1] for d in digits]  # Second number accuracies\n",
    "\n",
    "            width = 0.35  # Width of bars\n",
    "            ax.bar(x - width/2, first_num, width, label='First Number', color=colors[0])\n",
    "            ax.bar(x + width/2, second_num, width, label='Second Number', color=colors[1])\n",
    "\n",
    "            # Formatting\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(digits, rotation=45)\n",
    "            ax.set_ylim(minval, maxval)\n",
    "            ax.set_title(problem_type)\n",
    "            ax.legend()\n",
    "            ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "        # Adjust layout and show the plot\n",
    "        plt.tight_layout()\n",
    "        if testing_verbose:\n",
    "            plt.show()\n",
    "        wandb.log({f\"per_pt_encoding_accuracy\": wandb.Image(plt)})  # Log to wandb\n",
    "        plt.close()\n",
    "\n",
    "        # Compute average per digit across all problem types\n",
    "        avg_first_number = []\n",
    "        avg_second_number = []\n",
    "\n",
    "        for d in digits:\n",
    "            first_vals = [average_accuracy_per_pt[pt][d][0] for pt in average_accuracy_per_pt]\n",
    "            second_vals = [average_accuracy_per_pt[pt][d][1] for pt in average_accuracy_per_pt]\n",
    "            avg_first_number.append(np.mean(first_vals))\n",
    "            avg_second_number.append(np.mean(second_vals))\n",
    "\n",
    "        # Plotting\n",
    "        x = np.arange(len(digits))  # X-axis positions\n",
    "        width = 0.35  # Bar width\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        ax.bar(x - width/2, avg_first_number, width, label='First Number', color='blue')\n",
    "        ax.bar(x + width/2, avg_second_number, width, label='Second Number', color='orange')\n",
    "\n",
    "        # Formatting\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(digits, rotation=45)\n",
    "        ax.set_ylim(minval, maxval)  # Adjust y-axis for better visibility\n",
    "        ax.set_ylabel(\"Average Accuracy\")\n",
    "        ax.set_title(\"Average Encoding Accuracy Across Problem Types\")\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "        # Show the plot\n",
    "        if testing_verbose:\n",
    "            plt.show()\n",
    "        wandb.log({f\"all_pts_encoding_accuracy\": wandb.Image(plt)})  # Log to wandb\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3977db7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initialize_default_config():\n",
    "    encoder_path = \"/home/vdhanraj/llama/Programs/models/encoders_20250225.pth\"\n",
    "    decoder_path = \"/home/vdhanraj/llama/Programs/models/decoders_20241221.pth\"\n",
    "\n",
    "    save_model = True\n",
    "\n",
    "    problem_type = [\"multiplication\", \"modulo\", \"gcd\", \"lcm\", \"square_mod\", \"bitwise_and\", \"bitwise_xor\", \"bitwise_or\"]\n",
    "    complexity   = 2 # Complexity of problems to ask, represented by number of digits + 1 (of x and y)\n",
    "    temperature  = 0 # Temperature of LLM during training\n",
    "\n",
    "    train_model              = True  # If false, then only do testing step\n",
    "    lora_baseline            = False # If True,  instead of running symbolic encoder-decoder architecture, run a lora module\n",
    "    starting_skip_strength   = 0.5   # The the starting strength of skip connections (0 is all symbolic, 1 is all LLM)\n",
    "    problem_score_threshold  = 0.8   # If the similarity between the problem type is less than this value, don't us symbolic model\n",
    "    normalize_SP_before_dot  = False # If true,  normalize SP (from encoder) before doing a dot product with different problem types\n",
    "    initialize_decoders      = True  # If true,  initialize decoders as the pseudo-inverse of the encoders\n",
    "    normalize_vector         = False # If true,  normalize the output vector (whether it's noise or the solution hidden state)\n",
    "    rms_layer                = False # If true,  then fixed_skip is not used\n",
    "    double_rep               = True  # If true,  the solution is represented as n1 bound with the solution plus n2 bound with 0\n",
    "    use_specific_identities  = False # If true,  the solution is represented as n1 bound with the identity of n1 under each specific operation (makes double_rep=False)\n",
    "    trainable_skip           = False # If false, then this will allow the strength of the mixing ratio to be learnable\n",
    "    symbolic_encoding_layer  =  17   # Layer to use while generating symbolic vector of n1 and n2\n",
    "    symbolic_decoding_layers = [17]  # Layer to apply decoding network\n",
    "\n",
    "    # Total batch size is inference_to_backprop_ratio * n_samples\n",
    "    n_samples                   = max_batch_size # should be less or equal to  than params.max_batch_size\n",
    "    inference_to_backprop_ratio = 8 # Batch size is effectively n_samples * inference_to_backprop_ratio\n",
    "    num_epochs                  = 1000\n",
    "\n",
    "    learning_rate                   = 1e-3 # Base learning rate, modified by learning_rate_reduction_factors\n",
    "    learning_rate_reduction_factors = {100: 0.5, 500:  0.5, 1000: 0.4, 2000: 0.1, 4000: 0.5, 6000: 0.5, 8000: 0.5}\n",
    "\n",
    "    epochs_to_print    = num_epochs // 100 # How many epochs to print. If greater than 10, running averages will be printed\n",
    "    print_all_pts_freq = 100 # If multiple problem types are present, this is the frequency to print performance per problem type\n",
    "    verbose            = 0 # verbose=0 means no prints, verbose=1 means print the first row in batch data, verbose=2 means print all batch data\n",
    "\n",
    "    # Testing Hyperparameters\n",
    "    testing_problems                    = ['addition', 'division', 'multiplication', 'modulo', 'gcd',\n",
    "                                           'lcm', 'square_mod', 'bitwise_and', 'bitwise_xor', 'bitwise_or']\n",
    "    testing_num_epochs                  = 100\n",
    "    testing_inference_to_backprop_ratio = 1\n",
    "    testing_n_samples                   = max_batch_size # should be less than or equal to params.max_batch_size\n",
    "\n",
    "    testing_temperature      = 0\n",
    "    testing_epochs_to_print  = 0 # If multiple problem types are present, this is the frequency to print performance per problem type\n",
    "    testing_verbose          = 0 # verbose=0 means no prints, verbose=1 means print the first row in batch data, verbose=2 means print all batch data\n",
    "    record_score_per_problem = 2 # If 2/1/0, during testing/training+testing/neither, store the problem type and score info per sample\n",
    "\n",
    "    test_baseline = 0 # 0 means only test trained LLM, 1 means both test trained LLM and do baseline, 2 means only test baseline\n",
    "    cot           = False # whether to use Chain of Thought prompting\n",
    "    \n",
    "    test_on_unrelated_questions = False\n",
    "    test_with_non_numerical_rep = False\n",
    "    \n",
    "    #####################################################\n",
    "\n",
    "    encoder_input_tokens = 1     # The number of tokens the encoder expects as input (default is 1). If set to \"all\", number of encoder input tokens will be generated dynamically\n",
    "    calculate_end_index  = False # \n",
    "    \n",
    "    multi_token_intervention    = False # If True, perform intervention over multiple output tokens\n",
    "    static_encoding             = True  # If True, instead of recomputing symbolic representation for future tokens, use the initial encoding\n",
    "    calculate_encoding_accuracy = True  # If True, calculate the encoding accuracy per problem type per digit\n",
    "    \n",
    "\n",
    "    config = {\n",
    "        'encoder_path'                        : encoder_path,\n",
    "        'decoder_path'                        : decoder_path,\n",
    "        'save_model'                          : save_model,\n",
    "\n",
    "        'problem_type'                        : problem_type,\n",
    "        'complexity'                          : complexity,\n",
    "        'temperature'                         : temperature,\n",
    "\n",
    "        'train_model'                         : train_model,\n",
    "        'lora_baseline'                       : lora_baseline,\n",
    "        'starting_skip_strength'              : starting_skip_strength,\n",
    "        'problem_score_threshold'             : problem_score_threshold,\n",
    "        'normalize_SP_before_dot'             : normalize_SP_before_dot,\n",
    "        'initialize_decoders'                 : initialize_decoders,\n",
    "        'normalize_vector'                    : normalize_vector,\n",
    "        'rms_layer'                           : rms_layer,\n",
    "        'double_rep'                          : double_rep,\n",
    "        'use_specific_identities'             : use_specific_identities,\n",
    "        'trainable_skip'                      : trainable_skip,\n",
    "        'symbolic_encoding_layer'             : symbolic_encoding_layer,\n",
    "        'symbolic_decoding_layers'            : symbolic_decoding_layers,\n",
    "\n",
    "        'num_epochs'                          : num_epochs,\n",
    "        'n_samples'                           : n_samples,\n",
    "        'inference_to_backprop_ratio'         : inference_to_backprop_ratio,\n",
    "        'learning_rate'                       : learning_rate,\n",
    "        'learning_rate_reduction_factors'     : learning_rate_reduction_factors,\n",
    "\n",
    "        'epochs_to_print'                     : epochs_to_print,\n",
    "        'print_all_pts_freq'                  : print_all_pts_freq,\n",
    "        'verbose'                             : verbose,\n",
    "\n",
    "        'testing_problems'                    : testing_problems,\n",
    "        'testing_num_epochs'                  : testing_num_epochs,\n",
    "        'testing_inference_to_backprop_ratio' : testing_inference_to_backprop_ratio,\n",
    "        'testing_n_samples'                   : testing_n_samples,\n",
    "\n",
    "        'testing_temperature'                 : testing_temperature,\n",
    "        'testing_epochs_to_print'             : testing_epochs_to_print,\n",
    "        'testing_verbose'                     : testing_verbose,\n",
    "        'record_score_per_problem'            : record_score_per_problem,\n",
    "\n",
    "        'test_baseline'                       : test_baseline,\n",
    "        'cot'                                 : cot,\n",
    "        'test_on_unrelated_questions'         : test_on_unrelated_questions,\n",
    "        'test_with_non_numerical_rep'         : test_with_non_numerical_rep,\n",
    "        \n",
    "        'encoder_input_tokens'                : encoder_input_tokens,\n",
    "        'calculate_end_index'                 : calculate_end_index,\n",
    "        \n",
    "        'multi_token_intervention'            : multi_token_intervention,\n",
    "        'static_encoding'                     : static_encoding,\n",
    "        'calculate_encoding_accuracy'         : calculate_encoding_accuracy,\n",
    "    }\n",
    "\n",
    "    with open(\"outputs/score_per_problem_training_and_testing.txt\", \"w\") as file:\n",
    "        file.write(\"actual_problem_type,problem_type,score\\n\")\n",
    "    with open(\"outputs/score_per_problem.txt\", \"w\") as file:\n",
    "        file.write(\"actual_problem_type,problem_type,score\\n\")\n",
    "\n",
    "    return config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc44ede8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vdhanraj/llama/Programs/wandb/run-20250324_132351-ni0ijai3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/ni0ijai3' target=\"_blank\">Transformer encoder - 3 digit input data - AB Test</a></strong> to <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/ni0ijai3' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/ni0ijai3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING NEW EXPERIMENT (run_id = ni0ijai3)\n",
      "\n",
      "\n",
      "Number of trainable parameters: 8388608\n",
      " -------------- Epoch 0, Loss: 4.875, Score: 0.25  -------------- \n",
      "Learning Rate changed to: 0.0005\n",
      " -------------- Epoch 100, Loss: 2.6022393254950495, Score: 0.4430693069306931  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 3.4767995 , Score: 0.19117647058823528\n",
      "    Problem type: modulo\n",
      "    Loss: 1.736988 , Score: 0.6333333333333333\n",
      "    Problem type: gcd\n",
      "    Loss: 0.31323627 , Score: 0.9333333333333333\n",
      "    Problem type: lcm\n",
      "    Loss: 3.0473266 , Score: 0.2916666666666667\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.5161831 , Score: 0.398989898989899\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 2.8727517 , Score: 0.4107142857142857\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 3.8137817 , Score: 0.3202247191011236\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 3.098531 , Score: 0.36893203883495146\n",
      " -------------- Epoch 200, Loss: 1.479241020050808, Score: 0.5640547263681592  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 1.8393803 , Score: 0.22303921568627452\n",
      "    Problem type: modulo\n",
      "    Loss: 0.9580517 , Score: 0.7447916666666666\n",
      "    Problem type: gcd\n",
      "    Loss: 0.19680767 , Score: 0.9597156398104265\n",
      "    Problem type: lcm\n",
      "    Loss: 1.6942166 , Score: 0.37623762376237624\n",
      "    Problem type: square_mod\n",
      "    Loss: 1.69215 , Score: 0.5255102040816326\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 1.6233244 , Score: 0.5740740740740741\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 2.118232 , Score: 0.5957446808510638\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 1.7846264 , Score: 0.507537688442211\n",
      " -------------- Epoch 300, Loss: 1.1439328476341064, Score: 0.543812292358804  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 1.2344754 , Score: 0.2059748427672956\n",
      "    Problem type: modulo\n",
      "    Loss: 0.9938702 , Score: 0.7201986754966887\n",
      "    Problem type: gcd\n",
      "    Loss: 0.14390492 , Score: 0.9654605263157895\n",
      "    Problem type: lcm\n",
      "    Loss: 1.386344 , Score: 0.3574007220216607\n",
      "    Problem type: square_mod\n",
      "    Loss: 1.3605045 , Score: 0.512280701754386\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 1.2062442 , Score: 0.551829268292683\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 1.5551744 , Score: 0.5662020905923345\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 1.3172313 , Score: 0.47068403908794787\n",
      " -------------- Epoch 400, Loss: 0.9457227973978121, Score: 0.5038965087281796  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 0.95681053 , Score: 0.18567961165048544\n",
      "    Problem type: modulo\n",
      "    Loss: 0.8162825 , Score: 0.6898263027295285\n",
      "    Problem type: gcd\n",
      "    Loss: 0.15348695 , Score: 0.96625\n",
      "    Problem type: lcm\n",
      "    Loss: 1.1023932 , Score: 0.30631868131868134\n",
      "    Problem type: square_mod\n",
      "    Loss: 1.0262642 , Score: 0.4947089947089947\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 1.0007749 , Score: 0.5184331797235023\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 1.4345127 , Score: 0.4617283950617284\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 1.0792725 , Score: 0.40048543689320387\n",
      "Learning Rate changed to: 0.0005\n",
      " -------------- Epoch 500, Loss: 0.8351583429602925, Score: 0.48590319361277445  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 0.78841877 , Score: 0.17140151515151514\n",
      "    Problem type: modulo\n",
      "    Loss: 0.69537944 , Score: 0.6666666666666666\n",
      "    Problem type: gcd\n",
      "    Loss: 0.15336537 , Score: 0.9689922480620154\n",
      "    Problem type: lcm\n",
      "    Loss: 0.86482126 , Score: 0.27370689655172414\n",
      "    Problem type: square_mod\n",
      "    Loss: 0.98422116 , Score: 0.48577680525164113\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 0.97123367 , Score: 0.5222222222222223\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 1.282863 , Score: 0.43016194331983804\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 0.96895176 , Score: 0.35700389105058367\n"
     ]
    }
   ],
   "source": [
    "wandb.finish() # If there is an active current run, terminate it\n",
    "config = initialize_default_config()\n",
    "\n",
    "config[\"encoder_input_tokens\"] = 'all'\n",
    "config[\"complexity\"] = 2\n",
    "config[\"initialize_decoders\"] = False\n",
    "config[\"symbolic_encoding_layer\"] = 5\n",
    "config[\"symbolic_decoding_layers\"] = [17]\n",
    "config[\"num_epochs\"] = 1000\n",
    "\n",
    "config[\"encoder_path\"] = \"/home/vdhanraj/llama/Programs/models/encoders_20250307.pth\"\n",
    "config[\"decoder_path\"] = \"/home/vdhanraj/llama/Programs/models/decoders_20250307.pth\"\n",
    "\n",
    "\n",
    "#config[\"verbose\"] = 2\n",
    "#config[\"epochs_to_print\"] = config[\"num_epochs\"]\n",
    "\n",
    "#config['test_with_non_numerical_rep'] = True\n",
    "#config['train_model'] = False\n",
    "\n",
    "#config[\"testing_verbose\"] = 2\n",
    "config[\"multi_token_intervention\"]    = True\n",
    "config['static_encoding']             = True\n",
    "\n",
    "config[\"calculate_encoding_accuracy\"] = True\n",
    "#config[\"n_samples\"] = 1\n",
    "#config[\"verbose\"] = 2\n",
    "\n",
    "wandb.init(\n",
    "    project = \"Symbolic LLM - Fine Tune Decoders\",\n",
    "    #name    = f\"Multiple input tokens - 3 digit transformer encoder, layer 17 intervention\",\n",
    "    name    = f\"Transformer encoder - 3 digit input data - AB Test\",\n",
    "    config  = config\n",
    ")\n",
    "print(f\"STARTING NEW EXPERIMENT (run_id = {wandb.run.id})\\n\\n\")\n",
    "run_experiment(self=self, config=config)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c6c1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.finish() # If there is an active current run, terminate it\n",
    "config = initialize_default_config()\n",
    "\n",
    "config[\"encoder_input_tokens\"] = 1\n",
    "config[\"complexity\"] = 2\n",
    "config[\"initialize_decoders\"] = False\n",
    "config[\"symbolic_encoding_layer\"] = 17\n",
    "config[\"symbolic_decoding_layers\"] = [17]\n",
    "config[\"num_epochs\"] = 1000\n",
    "\n",
    "config[\"encoder_path\"] = \"/home/vdhanraj/llama/Programs/models/encoders_20241221.pth\"\n",
    "config[\"decoder_path\"] = \"/home/vdhanraj/llama/Programs/models/decoders_20241221.pth\"\n",
    "\n",
    "#config[\"verbose\"] = 2\n",
    "#config[\"epochs_to_print\"] = config[\"num_epochs\"]\n",
    "\n",
    "#config['test_with_non_numerical_rep'] = True\n",
    "#config['train_model'] = False\n",
    "\n",
    "#config[\"testing_verbose\"] = 2\n",
    "config[\"multi_token_intervention\"]    = False\n",
    "config['static_encoding']             = False\n",
    "\n",
    "config[\"calculate_encoding_accuracy\"] = True\n",
    "#config[\"n_samples\"] = 1\n",
    "#config[\"verbose\"] = 2\n",
    "\n",
    "wandb.init(\n",
    "    project = \"Symbolic LLM - Fine Tune Decoders\",\n",
    "    #name    = f\"Multiple input tokens - 3 digit transformer encoder, layer 17 intervention\",\n",
    "    name    = f\"Linear encoder - 3 digit input data - AB Test\",\n",
    "    config  = config\n",
    ")\n",
    "print(f\"STARTING NEW EXPERIMENT (run_id = {wandb.run.id})\\n\\n\")\n",
    "run_experiment(self=self, config=config)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c13659",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() # If there is an active current run, terminate it\n",
    "config = initialize_default_config()\n",
    "\n",
    "config[\"encoder_input_tokens\"] = 1\n",
    "config[\"complexity\"] = 2\n",
    "config[\"initialize_decoders\"] = True\n",
    "config[\"symbolic_encoding_layer\"] = 17\n",
    "config[\"symbolic_decoding_layers\"] = [17]\n",
    "config[\"num_epochs\"] = 1000\n",
    "\n",
    "config[\"encoder_path\"] = \"/home/vdhanraj/llama/Programs/models/encoders_20241221.pth\"\n",
    "config[\"decoder_path\"] = \"/home/vdhanraj/llama/Programs/models/decoders_20241221.pth\"\n",
    "\n",
    "#config[\"verbose\"] = 2\n",
    "#config[\"epochs_to_print\"] = config[\"num_epochs\"]\n",
    "\n",
    "#config['test_with_non_numerical_rep'] = True\n",
    "#config['train_model'] = False\n",
    "\n",
    "#config[\"testing_verbose\"] = 2\n",
    "config[\"multi_token_intervention\"]    = False\n",
    "config['static_encoding']             = False\n",
    "\n",
    "config[\"calculate_encoding_accuracy\"] = True\n",
    "#config[\"n_samples\"] = 1\n",
    "#config[\"verbose\"] = 2\n",
    "\n",
    "wandb.init(\n",
    "    project = \"Symbolic LLM - Fine Tune Decoders\",\n",
    "    #name    = f\"Multiple input tokens - 3 digit transformer encoder, layer 17 intervention\",\n",
    "    name    = f\"Transformer encoder (with initialized decoders) - 3 digit input data - AB Test\",\n",
    "    config  = config\n",
    ")\n",
    "print(f\"STARTING NEW EXPERIMENT (run_id = {wandb.run.id})\\n\\n\")\n",
    "run_experiment(self=self, config=config)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d0189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aa63bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707c33c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6ec4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "05cb0d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vdhanraj/llama/Programs/wandb/run-20250228_081716-gi50yb69</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/gi50yb69' target=\"_blank\">Multiple input tokens - decoding layer 5 (all pts)</a></strong> to <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/gi50yb69' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/gi50yb69</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING NEW EXPERIMENT (run_id = gi50yb69)\n",
      "\n",
      "\n",
      "Number of trainable parameters: 8388608\n",
      " -------------- Epoch 0, Loss: 9.8125, Score: 0.25  -------------- \n",
      " -------------- Epoch 25, Loss: 7.076322115384615, Score: 0.12259615384615384  -------------- \n",
      " -------------- Epoch 50, Loss: 6.198223039215686, Score: 0.07720588235294118  -------------- \n",
      " -------------- Epoch 75, Loss: 5.855674342105263, Score: 0.060032894736842105  -------------- \n",
      "Learning Rate changed to: 0.0005\n",
      " -------------- Epoch 100, Loss: 5.49845297029703, Score: 0.05012376237623763  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 5.1259484 , Score: 0.004807692307692308\n",
      "    Problem type: modulo\n",
      "    Loss: 4.7799044 , Score: 0.09\n",
      "    Problem type: gcd\n",
      "    Loss: 3.0930808 , Score: 0.31666666666666665\n",
      "    Problem type: lcm\n",
      "    Loss: 5.241295 , Score: 0.004761904761904762\n",
      "    Problem type: square_mod\n",
      "    Loss: 5.885088 , Score: 0.01\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 7.2925897 , Score: 0.0048543689320388345\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 5.947074 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 6.339149 , Score: 0.004807692307692308\n",
      " -------------- Epoch 125, Loss: 5.323784722222222, Score: 0.04265873015873016  -------------- \n",
      " -------------- Epoch 150, Loss: 5.09375, Score: 0.0380794701986755  -------------- \n",
      " -------------- Epoch 175, Loss: 4.964044744318182, Score: 0.03444602272727273  -------------- \n",
      " -------------- Epoch 200, Loss: 4.836675995024875, Score: 0.0320273631840796  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 4.484899 , Score: 0.0022935779816513763\n",
      "    Problem type: modulo\n",
      "    Loss: 4.68319 , Score: 0.04589371980676329\n",
      "    Problem type: gcd\n",
      "    Loss: 2.50318 , Score: 0.185\n",
      "    Problem type: lcm\n",
      "    Loss: 4.70176 , Score: 0.002688172043010753\n",
      "    Problem type: square_mod\n",
      "    Loss: 5.33668 , Score: 0.005208333333333333\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 6.3433423 , Score: 0.012376237623762377\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 5.604891 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 5.0911174 , Score: 0.0024154589371980675\n",
      " -------------- Epoch 225, Loss: 4.71356471238938, Score: 0.03125  -------------- \n",
      "Saved models/decoders_20241221_post_fine_tuning_gi50yb69_2025_02_28.pth\n",
      "On problem type: multiplication\n",
      "Final Score and Loss: 0.0019455252918287938 4.3358045\n",
      "On problem type: modulo\n",
      "Final Score and Loss: 0.0397489539748954 4.478791\n",
      "On problem type: gcd\n",
      "Final Score and Loss: 0.16600790513833993 2.2402132\n",
      "On problem type: lcm\n",
      "Final Score and Loss: 0.002136752136752137 4.6252337\n",
      "On problem type: square_mod\n",
      "Final Score and Loss: 0.004149377593360996 5.2341723\n",
      "On problem type: bitwise_and\n",
      "Final Score and Loss: 0.018442622950819672 5.960254\n",
      "On problem type: bitwise_xor\n",
      "Final Score and Loss: 0.0 5.3430104\n",
      "On problem type: bitwise_or\n",
      "Final Score and Loss: 0.004016064257028112 4.7637763\n",
      "~~~~~~~~ Problem Type: addition ~~~~~~~~\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for dimension 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 29\u001b[0m\n\u001b[1;32m     23\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m     24\u001b[0m     project \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSymbolic LLM - Fine Tune Decoders\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m     name    \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiple input tokens - decoding layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (all pts)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m     config  \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTARTING NEW EXPERIMENT (run_id = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwandb\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[72], line 392\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    390\u001b[0m bypass_symbolic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    391\u001b[0m add_noise       \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 392\u001b[0m losses, scores, responses \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtesting_n_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtesting_n_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mtesting_num_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtesting_num_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mtesting_temperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtesting_temperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m                                           \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mbypass_symbolic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbypass_symbolic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43minference_to_backprop_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtesting_inference_to_backprop_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43msymbolic_encoding_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msymbolic_encoding_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43msymbolic_decoding_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msymbolic_decoding_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mnormalize_vector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomplexity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplexity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mtesting_epochs_to_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtesting_epochs_to_print\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mrms_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrms_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdouble_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdouble_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mtesting_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtesting_verbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m testing_losses_per_pt_SYM[pt] \u001b[38;5;241m=\u001b[39m losses\n\u001b[1;32m    405\u001b[0m testing_scores_per_pt_SYM[pt] \u001b[38;5;241m=\u001b[39m scores\n",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(testing_n_samples, testing_num_epochs, testing_temperature, problem_type, self, add_noise, criterion, bypass_symbolic, inference_to_backprop_ratio, symbolic_encoding_layer, symbolic_decoding_layers, normalize_vector, complexity, rms_layer, double_rep, cot, testing_epochs_to_print, testing_verbose)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(testing_num_epochs):\n\u001b[0;32m---> 10\u001b[0m         loss, score, response_data \u001b[38;5;241m=\u001b[39m \u001b[43minference_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtesting_n_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtesting_temperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproblem_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43madd_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbypass_symbolic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbypass_symbolic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43minference_to_backprop_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_to_backprop_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43msymbolic_encoding_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msymbolic_encoding_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43msymbolic_decoding_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msymbolic_decoding_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mnormalize_vector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomplexity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplexity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mrms_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrms_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdouble_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdouble_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtesting_verbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m         losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [loss]\n\u001b[1;32m     19\u001b[0m         scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [score]\n",
      "Cell \u001b[0;32mIn[9], line 80\u001b[0m, in \u001b[0;36minference_step\u001b[0;34m(n_samples, self, temperature, problem_type, add_noise, bypass_symbolic, inference_to_backprop_ratio, symbolic_encoding_layer, symbolic_decoding_layers, criterion, cot, normalize_vector, complexity, rms_layer, double_rep, verbose)\u001b[0m\n\u001b[1;32m     74\u001b[0m     mdl \u001b[38;5;241m=\u001b[39m complexity\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m#if verbose:\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m#    print(\"Max decoding length:\", mdl)\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Shape of list_of_probs and list_of_logits is (sequence_output_length, batch_size, num_tokens)\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m h_stack, list_of_probs, list_of_logits, out_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mepisode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdialogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdialogs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43minference_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_symbolic_funnel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43mbypass_symbolic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbypass_symbolic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_decoding_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmdl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43mproblem_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_problem_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43msymbolic_encoding_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msymbolic_encoding_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43msymbolic_decoding_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msymbolic_decoding_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43mnormalize_vector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomplexity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplexity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43mrms_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrms_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdouble_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdouble_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cot \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtest_on_unrelated_questions:\n\u001b[1;32m     90\u001b[0m     token_for_final \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m19918\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 37\u001b[0m, in \u001b[0;36mepisode\u001b[0;34m(dialogs, self, temperature, top_p, inference_mode, max_decoding_length, problem_type, add_noise, symbolic_encoding_layer, symbolic_decoding_layers, normalize_vector, rms_layer, double_rep, complexity, bypass_symbolic, verbose)\u001b[0m\n\u001b[1;32m     35\u001b[0m h_stacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cur_pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(min_prompt_len, total_len):\n\u001b[0;32m---> 37\u001b[0m     logits, h_stack, h \u001b[38;5;241m=\u001b[39m \u001b[43minference_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_pos\u001b[49m\u001b[43m:\u001b[49m\u001b[43mcur_pos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproblem_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mbypass_symbolic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbypass_symbolic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbolic_encoding_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msymbolic_encoding_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43msymbolic_decoding_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msymbolic_decoding_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_vector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize_vector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mrms_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrms_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdouble_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdouble_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomplexity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplexity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mcurr_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Shape of logits are (batch_size, total_sequence_length, num_tokens)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     h_stacks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [h_stack]\n",
      "File \u001b[0;32m~/llama/llama3/llama/model.py:419\u001b[0m, in \u001b[0;36mTransformer.forward_symbolic_funnel\u001b[0;34m(self, tokens, start_pos, symbolic_encoding_layer, symbolic_decoding_layers, bypass_symbolic, normalize_vector, problem_type, add_noise, complexity, double_rep, rms_layer, curr_token, verbose)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m#print(\"AFTR\", appended_h.shape, relevent_h.shape)\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelevent_h \u001b[38;5;241m=\u001b[39m relevent_h \u001b[38;5;66;03m# Save to recalculate relevent_h for next token\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m symbolic_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarting_encoder_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrelevent_h\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# If using lora baseline, simply set the output of the encoder to the input of the decoder without doing any symbolic computation\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_baseline:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llama/llama3/llama/EncoderNetworks.py:140\u001b[0m, in \u001b[0;36mLastTokenTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    137\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(x)  \u001b[38;5;66;03m# Shape: (batch, seq_len, hidden_dim)\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Select the last token's hidden state\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m last_token \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Shape: (batch, hidden_dim)\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Output projection\u001b[39;00m\n\u001b[1;32m    143\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_proj(last_token)  \u001b[38;5;66;03m# Shape: (batch, output_dim)\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index -1 is out of bounds for dimension 1 with size 0"
     ]
    }
   ],
   "source": [
    "decoding_layers = [5, 10, 15, 20, 25, 30]\n",
    "for dl in decoding_layers :\n",
    "    wandb.finish() # If there is an active current run, terminate it\n",
    "    config = initialize_default_config()\n",
    "\n",
    "    config[\"encoder_input_tokens\"] = 'all'\n",
    "    config[\"complexity\"] = 2\n",
    "    config[\"initialize_decoders\"] = False\n",
    "    config[\"symbolic_encoding_layer\"] = 5\n",
    "    config[\"symbolic_decoding_layers\"] = [dl]\n",
    "    config[\"num_epochs\"] = 250\n",
    "\n",
    "    #config[\"verbose\"] = 2\n",
    "    #config[\"epochs_to_print\"] = config[\"num_epochs\"]\n",
    "\n",
    "    #config['test_with_non_numerical_rep'] = True\n",
    "    #config['train_model'] = False\n",
    "\n",
    "    #config[\"testing_verbose\"] = 2\n",
    "    config[\"multi_token_intervention\"]    = True\n",
    "    config[\"calculate_encoding_accuracy\"] = True\n",
    "\n",
    "    wandb.init(\n",
    "        project = \"Symbolic LLM - Fine Tune Decoders\",\n",
    "        name    = f\"Multiple input tokens - decoding layer {dl} (all pts)\",\n",
    "        config  = config\n",
    "    )\n",
    "    print(f\"STARTING NEW EXPERIMENT (run_id = {wandb.run.id})\\n\\n\")\n",
    "    run_experiment(self=self, config=config)\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1e475e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vdhanraj/llama/Programs/wandb/run-20250301_122639-urf5hm8u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/urf5hm8u' target=\"_blank\">Multiple input tokens - decoding layer (multi) (all pts)</a></strong> to <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/urf5hm8u' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/urf5hm8u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING NEW EXPERIMENT (run_id = urf5hm8u)\n",
      "\n",
      "\n",
      "Number of trainable parameters: 50331648\n",
      " -------------- Epoch 0, Loss: 18.25, Score: 0.0  -------------- \n",
      "Learning Rate changed to: 0.0005\n",
      " -------------- Epoch 100, Loss: 3.6376856435643563, Score: 0.01051980198019802  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 2.9693298 , Score: 0.004807692307692308\n",
      "    Problem type: modulo\n",
      "    Loss: 4.155535 , Score: 0.0\n",
      "    Problem type: gcd\n",
      "    Loss: 1.551458 , Score: 0.029411764705882353\n",
      "    Problem type: lcm\n",
      "    Loss: 3.5015838 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 4.6166596 , Score: 0.0\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 4.9158144 , Score: 0.039603960396039604\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 3.9494832 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 4.0166616 , Score: 0.004672897196261682\n",
      " -------------- Epoch 200, Loss: 2.855041200248756, Score: 0.00777363184079602  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 2.1526814 , Score: 0.002403846153846154\n",
      "    Problem type: modulo\n",
      "    Loss: 3.5423348 , Score: 0.0\n",
      "    Problem type: gcd\n",
      "    Loss: 1.2514825 , Score: 0.027196652719665274\n",
      "    Problem type: lcm\n",
      "    Loss: 2.1225371 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 3.873298 , Score: 0.002793296089385475\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 3.8691785 , Score: 0.02368421052631579\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 3.1241758 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 3.4526403 , Score: 0.002577319587628866\n",
      " -------------- Epoch 300, Loss: 2.5161733025332227, Score: 0.00872093023255814  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 1.6692646 , Score: 0.0031545741324921135\n",
      "    Problem type: modulo\n",
      "    Loss: 3.3198411 , Score: 0.0\n",
      "    Problem type: gcd\n",
      "    Loss: 1.2122078 , Score: 0.03162650602409638\n",
      "    Problem type: lcm\n",
      "    Loss: 1.6575508 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 3.3729138 , Score: 0.0035460992907801418\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 3.4267304 , Score: 0.027210884353741496\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 2.859354 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 2.9011328 , Score: 0.0017793594306049821\n",
      " -------------- Epoch 400, Loss: 2.2996999688279303, Score: 0.00841645885286783  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 1.3908168 , Score: 0.0023752969121140144\n",
      "    Problem type: modulo\n",
      "    Loss: 3.1504624 , Score: 0.0\n",
      "    Problem type: gcd\n",
      "    Loss: 1.1683246 , Score: 0.03211009174311927\n",
      "    Problem type: lcm\n",
      "    Loss: 1.3858076 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 3.0071433 , Score: 0.003968253968253968\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 3.3381758 , Score: 0.02610966057441253\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 2.6380575 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 2.5991735 , Score: 0.0012853470437017994\n",
      "Learning Rate changed to: 0.0005\n",
      " -------------- Epoch 500, Loss: 2.148863406000499, Score: 0.008857285429141716  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 1.2065612 , Score: 0.0028735632183908046\n",
      "    Problem type: modulo\n",
      "    Loss: 2.9615903 , Score: 0.0\n",
      "    Problem type: gcd\n",
      "    Loss: 1.1427019 , Score: 0.032567049808429116\n",
      "    Problem type: lcm\n",
      "    Loss: 1.2254056 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.7820706 , Score: 0.003054989816700611\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 3.1098058 , Score: 0.030982905982905984\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 2.503744 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 2.4823055 , Score: 0.002\n",
      " -------------- Epoch 600, Loss: 2.0155421303036607, Score: 0.008423460898502495  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 1.083929 , Score: 0.0023923444976076554\n",
      "    Problem type: modulo\n",
      "    Loss: 2.7648954 , Score: 0.0\n",
      "    Problem type: gcd\n",
      "    Loss: 1.1577122 , Score: 0.033816425120772944\n",
      "    Problem type: lcm\n",
      "    Loss: 1.1616967 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.557729 , Score: 0.0025\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 2.990673 , Score: 0.027678571428571427\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 2.3547533 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 2.2237306 , Score: 0.001639344262295082\n",
      " -------------- Epoch 700, Loss: 1.9166379920426178, Score: 0.008202567760342368  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 0.9752434 , Score: 0.0027137042062415195\n",
      "    Problem type: modulo\n",
      "    Loss: 2.6414084 , Score: 0.0\n",
      "    Problem type: gcd\n",
      "    Loss: 1.0838646 , Score: 0.03125\n",
      "    Problem type: lcm\n",
      "    Loss: 1.0802256 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.453258 , Score: 0.002158273381294964\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 2.9118862 , Score: 0.02846153846153846\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 2.2229624 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 2.1485546 , Score: 0.0013986013986013986\n",
      " -------------- Epoch 800, Loss: 1.8445217404026217, Score: 0.007880774032459426  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 0.9034225 , Score: 0.002366863905325444\n",
      "    Problem type: modulo\n",
      "    Loss: 2.5480542 , Score: 0.0006329113924050633\n",
      "    Problem type: gcd\n",
      "    Loss: 1.0267031 , Score: 0.03102189781021898\n",
      "    Problem type: lcm\n",
      "    Loss: 1.0248332 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.375537 , Score: 0.0031685678073510772\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 2.8568683 , Score: 0.025780189959294438\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 2.1206844 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 2.051516 , Score: 0.0011933174224343676\n",
      " -------------- Epoch 900, Loss: 1.7901058556898586, Score: 0.00776914539400666  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 0.8663922 , Score: 0.002152852529601722\n",
      "    Problem type: modulo\n",
      "    Loss: 2.464211 , Score: 0.0005649717514124294\n",
      "    Problem type: gcd\n",
      "    Loss: 1.0511527 , Score: 0.03017241379310345\n",
      "    Problem type: lcm\n",
      "    Loss: 0.9375266 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.2662568 , Score: 0.0032679738562091504\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 2.7972305 , Score: 0.02599758162031439\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 2.1053073 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 1.9507587 , Score: 0.0010638297872340426\n",
      "Saved models/decoders_20241221_post_fine_tuning_urf5hm8u_2025_03_02.pth\n",
      "On problem type: multiplication\n",
      "Final Score and Loss: 0.001984126984126984 0.8183212\n",
      "On problem type: modulo\n",
      "Final Score and Loss: 0.0005112474437627812 2.3474033\n",
      "On problem type: gcd\n",
      "Final Score and Loss: 0.029872673849167482 1.0678096\n",
      "On problem type: lcm\n",
      "Final Score and Loss: 0.0 0.8840586\n",
      "On problem type: square_mod\n",
      "Final Score and Loss: 0.0029239766081871343 2.1572936\n",
      "On problem type: bitwise_and\n",
      "Final Score and Loss: 0.02459893048128342 2.753307\n",
      "On problem type: bitwise_xor\n",
      "Final Score and Loss: 0.0 2.0660782\n",
      "On problem type: bitwise_or\n",
      "Final Score and Loss: 0.001452081316553727 1.9508389\n",
      "~~~~~~~~ Problem Type: addition ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on addition: 97.0 ± 11.8743, 4.583 ± 7.5173\n",
      "~~~~~~~~ Problem Type: division ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on division: 93.0 ± 17.3494, 7.986 ± 9.6618\n",
      "~~~~~~~~ Problem Type: multiplication ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on multiplication: 0.0 ± 0.0, 8.811 ± 4.5803\n",
      "~~~~~~~~ Problem Type: modulo ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on modulo: 0.5 ± 4.9749, 1.753 ± 3.0644\n",
      "~~~~~~~~ Problem Type: gcd ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on gcd: 3.0 ± 11.8743, 1.273 ± 4.5976\n",
      "~~~~~~~~ Problem Type: lcm ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on lcm: 0.0 ± 0.0, 12.399 ± 3.1223\n",
      "~~~~~~~~ Problem Type: square_mod ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on square_mod: 0.0 ± 0.0, 5.85 ± 4.7779\n",
      "~~~~~~~~ Problem Type: bitwise_and ~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Mean score and loss of symbolic LLM on bitwise_and: 3.5 ± 12.7574, 1.717 ± 3.4285\n",
      "~~~~~~~~ Problem Type: bitwise_xor ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on bitwise_xor: 0.0 ± 0.0, 2.239 ± 2.8852\n",
      "~~~~~~~~ Problem Type: bitwise_or ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on bitwise_or: 0.0 ± 0.0, 1.58 ± 2.3318\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_testing_loss_SYM_addition</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_bitwise_and</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_bitwise_or</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_bitwise_xor</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_division</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_gcd</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_lcm</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_modulo</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_multiplication</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_square_mod</td><td>▁</td></tr><tr><td>average_testing_score_SYM_addition</td><td>▁</td></tr><tr><td>average_testing_score_SYM_bitwise_and</td><td>▁</td></tr><tr><td>average_testing_score_SYM_bitwise_or</td><td>▁</td></tr><tr><td>average_testing_score_SYM_bitwise_xor</td><td>▁</td></tr><tr><td>average_testing_score_SYM_division</td><td>▁</td></tr><tr><td>average_testing_score_SYM_gcd</td><td>▁</td></tr><tr><td>average_testing_score_SYM_lcm</td><td>▁</td></tr><tr><td>average_testing_score_SYM_modulo</td><td>▁</td></tr><tr><td>average_testing_score_SYM_multiplication</td><td>▁</td></tr><tr><td>average_testing_score_SYM_square_mod</td><td>▁</td></tr><tr><td>different_actual_problem_types_count</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>epoch_bitwise_and</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▆▆▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>epoch_bitwise_or</td><td>▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>epoch_bitwise_xor</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇█</td></tr><tr><td>epoch_gcd</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>epoch_lcm</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>epoch_modulo</td><td>▁▁▁▁▂▂▂▂▂▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>epoch_multiplication</td><td>▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇█████</td></tr><tr><td>epoch_square_mod</td><td>▁▁▁▁▁▁▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>final_loss_bitwise_and</td><td>▁</td></tr><tr><td>final_loss_bitwise_or</td><td>▁</td></tr><tr><td>final_loss_bitwise_xor</td><td>▁</td></tr><tr><td>final_loss_gcd</td><td>▁</td></tr><tr><td>final_loss_lcm</td><td>▁</td></tr><tr><td>final_loss_modulo</td><td>▁</td></tr><tr><td>final_loss_multiplication</td><td>▁</td></tr><tr><td>final_loss_square_mod</td><td>▁</td></tr><tr><td>final_score_bitwise_and</td><td>▁</td></tr><tr><td>final_score_bitwise_or</td><td>▁</td></tr><tr><td>final_score_bitwise_xor</td><td>▁</td></tr><tr><td>final_score_gcd</td><td>▁</td></tr><tr><td>final_score_lcm</td><td>▁</td></tr><tr><td>final_score_modulo</td><td>▁</td></tr><tr><td>final_score_multiplication</td><td>▁</td></tr><tr><td>final_score_square_mod</td><td>▁</td></tr><tr><td>loss</td><td>▇▄▇▅▅▆▃▂▂▄▃▃█▃▁▂▂▁▂▂▁▁▄▃▂▂▃▂▁▁▁▃▂▄▅▂▁▂▂▁</td></tr><tr><td>loss_bitwise_and</td><td>▂▁▂▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▂▁</td></tr><tr><td>loss_bitwise_or</td><td>▄▂▂▁▁▁▁▁▁▁▁▁▁▄▄▇▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁</td></tr><tr><td>loss_bitwise_xor</td><td>▃▂▁▃▅▃▁▂▁▃▁▁▁▁▁▁▂▁▁▁█▁▁▃▁▁▆▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>loss_gcd</td><td>▁▃▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>loss_lcm</td><td>▃▄▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁█▁▁▁▁</td></tr><tr><td>loss_modulo</td><td>▅▃▅▃▂▂▆▁▂▂▂▁▁█▁▁▁▁▁▇▁▁▁▁▅▁▁▅▂▁▁▅▁▁▁▁▁▆▁▁</td></tr><tr><td>loss_multiplication</td><td>▆▃▄▁▂▂▁▂▂▁▁▁▁▁▁▁▂▇▁▁▁▃▁▂█▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁</td></tr><tr><td>loss_square_mod</td><td>▃▂▃▁▁▁▂▁▁▁▆▄▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▄▁▁</td></tr><tr><td>score</td><td>▁▁█▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁█</td></tr><tr><td>score_bitwise_and</td><td>▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_bitwise_or</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_bitwise_xor</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_gcd</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_lcm</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_modulo</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_multiplication</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_square_mod</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>skip_weights_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>skip_weights_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>skip_weights_2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>skip_weights_3</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>skip_weights_4</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>skip_weights_5</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_testing_loss_SYM_addition</td><td>4.58306</td></tr><tr><td>average_testing_loss_SYM_bitwise_and</td><td>1.71689</td></tr><tr><td>average_testing_loss_SYM_bitwise_or</td><td>1.58006</td></tr><tr><td>average_testing_loss_SYM_bitwise_xor</td><td>2.23865</td></tr><tr><td>average_testing_loss_SYM_division</td><td>7.98605</td></tr><tr><td>average_testing_loss_SYM_gcd</td><td>1.27312</td></tr><tr><td>average_testing_loss_SYM_lcm</td><td>12.39926</td></tr><tr><td>average_testing_loss_SYM_modulo</td><td>1.753</td></tr><tr><td>average_testing_loss_SYM_multiplication</td><td>8.81061</td></tr><tr><td>average_testing_loss_SYM_square_mod</td><td>5.84981</td></tr><tr><td>average_testing_score_SYM_addition</td><td>0.97</td></tr><tr><td>average_testing_score_SYM_bitwise_and</td><td>0.035</td></tr><tr><td>average_testing_score_SYM_bitwise_or</td><td>0</td></tr><tr><td>average_testing_score_SYM_bitwise_xor</td><td>0</td></tr><tr><td>average_testing_score_SYM_division</td><td>0.93</td></tr><tr><td>average_testing_score_SYM_gcd</td><td>0.03</td></tr><tr><td>average_testing_score_SYM_lcm</td><td>0</td></tr><tr><td>average_testing_score_SYM_modulo</td><td>0.005</td></tr><tr><td>average_testing_score_SYM_multiplication</td><td>0</td></tr><tr><td>average_testing_score_SYM_square_mod</td><td>0</td></tr><tr><td>different_actual_problem_types_count</td><td>0</td></tr><tr><td>epoch</td><td>999</td></tr><tr><td>epoch_bitwise_and</td><td>1879</td></tr><tr><td>epoch_bitwise_or</td><td>2075</td></tr><tr><td>epoch_bitwise_xor</td><td>2013</td></tr><tr><td>epoch_gcd</td><td>2051</td></tr><tr><td>epoch_lcm</td><td>1923</td></tr><tr><td>epoch_modulo</td><td>1965</td></tr><tr><td>epoch_multiplication</td><td>2025</td></tr><tr><td>epoch_square_mod</td><td>2061</td></tr><tr><td>final_loss_bitwise_and</td><td>2.75331</td></tr><tr><td>final_loss_bitwise_or</td><td>1.95084</td></tr><tr><td>final_loss_bitwise_xor</td><td>2.06608</td></tr><tr><td>final_loss_gcd</td><td>1.06781</td></tr><tr><td>final_loss_lcm</td><td>0.88406</td></tr><tr><td>final_loss_modulo</td><td>2.3474</td></tr><tr><td>final_loss_multiplication</td><td>0.81832</td></tr><tr><td>final_loss_square_mod</td><td>2.15729</td></tr><tr><td>final_score_bitwise_and</td><td>0.0246</td></tr><tr><td>final_score_bitwise_or</td><td>0.00145</td></tr><tr><td>final_score_bitwise_xor</td><td>0</td></tr><tr><td>final_score_gcd</td><td>0.02987</td></tr><tr><td>final_score_lcm</td><td>0</td></tr><tr><td>final_score_modulo</td><td>0.00051</td></tr><tr><td>final_score_multiplication</td><td>0.00198</td></tr><tr><td>final_score_square_mod</td><td>0.00292</td></tr><tr><td>loss</td><td>0.91406</td></tr><tr><td>loss_bitwise_and</td><td>0.02173</td></tr><tr><td>loss_bitwise_or</td><td>0.00861</td></tr><tr><td>loss_bitwise_xor</td><td>0.06348</td></tr><tr><td>loss_gcd</td><td>0.00285</td></tr><tr><td>loss_lcm</td><td>0.15137</td></tr><tr><td>loss_modulo</td><td>0.10059</td></tr><tr><td>loss_multiplication</td><td>0.01086</td></tr><tr><td>loss_square_mod</td><td>0.18457</td></tr><tr><td>score</td><td>0</td></tr><tr><td>score_bitwise_and</td><td>0</td></tr><tr><td>score_bitwise_or</td><td>0</td></tr><tr><td>score_bitwise_xor</td><td>0</td></tr><tr><td>score_gcd</td><td>0</td></tr><tr><td>score_lcm</td><td>0</td></tr><tr><td>score_modulo</td><td>0</td></tr><tr><td>score_multiplication</td><td>0</td></tr><tr><td>score_square_mod</td><td>0</td></tr><tr><td>skip_weights_0</td><td>0.5</td></tr><tr><td>skip_weights_1</td><td>0.5</td></tr><tr><td>skip_weights_2</td><td>0.5</td></tr><tr><td>skip_weights_3</td><td>0.5</td></tr><tr><td>skip_weights_4</td><td>0.5</td></tr><tr><td>skip_weights_5</td><td>0.5</td></tr><tr><td>symbolic_output_text_addition</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_bitwise_and</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_bitwise_or</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_bitwise_xor</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_division</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_gcd</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_lcm</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_modulo</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_multiplication</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_square_mod</td><td>Mean score and loss ...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Multiple input tokens - decoding layer (multi) (all pts)</strong> at: <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/urf5hm8u' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/urf5hm8u</a><br> View project at: <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 32 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250301_122639-urf5hm8u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish() # If there is an active current run, terminate it\n",
    "config = initialize_default_config()\n",
    "\n",
    "config[\"encoder_input_tokens\"] = 'all'\n",
    "config[\"complexity\"] = 3\n",
    "config[\"initialize_decoders\"] = False\n",
    "config[\"symbolic_encoding_layer\"] = 5\n",
    "config[\"symbolic_decoding_layers\"] = [5, 10, 15, 20, 25, 30]\n",
    "config[\"num_epochs\"] = 1000\n",
    "\n",
    "#config[\"verbose\"] = 2\n",
    "#config[\"epochs_to_print\"] = config[\"num_epochs\"]\n",
    "\n",
    "#config['test_with_non_numerical_rep'] = True\n",
    "#config['train_model'] = False\n",
    "\n",
    "#config[\"testing_verbose\"] = 2\n",
    "config[\"multi_token_intervention\"]    = True\n",
    "config[\"calculate_encoding_accuracy\"] = True\n",
    "\n",
    "wandb.init(\n",
    "    project = \"Symbolic LLM - Fine Tune Decoders\",\n",
    "    name    = f\"Multiple input tokens - decoding layer (multi) (all pts)\",\n",
    "    config  = config\n",
    ")\n",
    "print(f\"STARTING NEW EXPERIMENT (run_id = {wandb.run.id})\\n\\n\")\n",
    "run_experiment(self=self, config=config)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1af44899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvarun_dhanraj\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vdhanraj/llama/Programs/wandb/run-20250302_235030-pmbxxkrd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/pmbxxkrd' target=\"_blank\">Multiple input tokens - decoding layer (multi) (all pts)</a></strong> to <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/pmbxxkrd' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/pmbxxkrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING NEW EXPERIMENT (run_id = pmbxxkrd)\n",
      "\n",
      "\n",
      "Number of trainable parameters: 50331648\n",
      " -------------- Epoch 0, Loss: 18.75, Score: 0.0  -------------- \n",
      "Learning Rate changed to: 0.0005\n",
      " -------------- Epoch 100, Loss: 3.493889232673267, Score: 0.0049504950495049506  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 3.2452307 , Score: 0.0\n",
      "    Problem type: modulo\n",
      "    Loss: 4.0742335 , Score: 0.0\n",
      "    Problem type: gcd\n",
      "    Loss: 2.265229 , Score: 0.03804347826086957\n",
      "    Problem type: lcm\n",
      "    Loss: 2.3945608 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 4.1461763 , Score: 0.0\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 4.764059 , Score: 0.005050505050505051\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 3.7312486 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 3.3929303 , Score: 0.0\n",
      " -------------- Epoch 200, Loss: 2.7966660836442787, Score: 0.007462686567164179  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 2.2751713 , Score: 0.0\n",
      "    Problem type: modulo\n",
      "    Loss: 3.760524 , Score: 0.0\n",
      "    Problem type: gcd\n",
      "    Loss: 1.525808 , Score: 0.04032258064516129\n",
      "    Problem type: lcm\n",
      "    Loss: 1.7951 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 3.2270427 , Score: 0.0026455026455026454\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 3.88045 , Score: 0.019801980198019802\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 3.1096017 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 2.8439543 , Score: 0.0\n",
      " -------------- Epoch 300, Loss: 2.4700088247508307, Score: 0.00851328903654485  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 1.7956547 , Score: 0.0\n",
      "    Problem type: modulo\n",
      "    Loss: 3.3698392 , Score: 0.0017543859649122807\n",
      "    Problem type: gcd\n",
      "    Loss: 1.4235113 , Score: 0.04225352112676056\n",
      "    Problem type: lcm\n",
      "    Loss: 1.5595897 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.7647598 , Score: 0.0017857142857142857\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 3.3290076 , Score: 0.022151898734177215\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 2.8912737 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 2.6665933 , Score: 0.0014367816091954023\n",
      " -------------- Epoch 400, Loss: 2.238887644170823, Score: 0.007793017456359103  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 1.5311683 , Score: 0.0\n",
      "    Problem type: modulo\n",
      "    Loss: 3.1333735 , Score: 0.0013054830287206266\n",
      "    Problem type: gcd\n",
      "    Loss: 1.1472986 , Score: 0.038860103626943004\n",
      "    Problem type: lcm\n",
      "    Loss: 1.2667311 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.6054332 , Score: 0.0012531328320802004\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 3.1636968 , Score: 0.02014218009478673\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 2.6173017 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 2.4451299 , Score: 0.001152073732718894\n",
      "Learning Rate changed to: 0.0005\n",
      " -------------- Epoch 500, Loss: 2.135720746007984, Score: 0.00873253493013972  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 1.3927943 , Score: 0.0\n",
      "    Problem type: modulo\n",
      "    Loss: 2.9227147 , Score: 0.002074688796680498\n",
      "    Problem type: gcd\n",
      "    Loss: 1.1172106 , Score: 0.0405982905982906\n",
      "    Problem type: lcm\n",
      "    Loss: 1.0966741 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.6255865 , Score: 0.002053388090349076\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 3.0251265 , Score: 0.024299065420560748\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 2.539402 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 2.3264153 , Score: 0.001869158878504673\n",
      " -------------- Epoch 600, Loss: 2.03318362624792, Score: 0.008319467554076539  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 1.2869205 , Score: 0.0\n",
      "    Problem type: modulo\n",
      "    Loss: 2.8843963 , Score: 0.0017152658662092624\n",
      "    Problem type: gcd\n",
      "    Loss: 1.1240962 , Score: 0.037587412587412584\n",
      "    Problem type: lcm\n",
      "    Loss: 1.0072886 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.5122688 , Score: 0.002529510961214165\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 2.9604716 , Score: 0.024232633279483037\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 2.3202593 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 2.1997042 , Score: 0.0016129032258064516\n",
      " -------------- Epoch 700, Loss: 1.9464322034370543, Score: 0.007845934379457917  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 1.1809714 , Score: 0.0\n",
      "    Problem type: modulo\n",
      "    Loss: 2.8314633 , Score: 0.0014577259475218659\n",
      "    Problem type: gcd\n",
      "    Loss: 1.0981295 , Score: 0.03468208092485549\n",
      "    Problem type: lcm\n",
      "    Loss: 0.91957796 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.4246297 , Score: 0.0022123893805309734\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 2.822415 , Score: 0.0232722143864598\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 2.281911 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 2.088338 , Score: 0.0013908205841446453\n",
      " -------------- Epoch 800, Loss: 1.880206199769819, Score: 0.008036828963795256  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 1.0886886 , Score: 0.0\n",
      "    Problem type: modulo\n",
      "    Loss: 2.7505398 , Score: 0.0018820577164366374\n",
      "    Problem type: gcd\n",
      "    Loss: 1.0631522 , Score: 0.036075949367088606\n",
      "    Problem type: lcm\n",
      "    Loss: 0.83910924 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.3360405 , Score: 0.0019230769230769232\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 2.8144698 , Score: 0.02369077306733167\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 2.187444 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 2.0270371 , Score: 0.0012300123001230013\n",
      " -------------- Epoch 900, Loss: 1.8308417079980577, Score: 0.008185349611542731  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 1.0337968 , Score: 0.0005599104143337066\n",
      "    Problem type: modulo\n",
      "    Loss: 2.6516387 , Score: 0.0016611295681063123\n",
      "    Problem type: gcd\n",
      "    Loss: 1.0706515 , Score: 0.03496115427302997\n",
      "    Problem type: lcm\n",
      "    Loss: 0.8325906 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.3077984 , Score: 0.0017123287671232876\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 2.7417011 , Score: 0.024444444444444446\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 2.140847 , Score: 0.000574052812858783\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 1.9395455 , Score: 0.0016198704103671706\n",
      "Saved models/decoders_20241221_post_fine_tuning_pmbxxkrd_2025_03_03.pth\n",
      "On problem type: multiplication\n",
      "Final Score and Loss: 0.0004995004995004995 0.99021107\n",
      "On problem type: modulo\n",
      "Final Score and Loss: 0.001503006012024048 2.6126966\n",
      "On problem type: gcd\n",
      "Final Score and Loss: 0.033600802407221665 1.0121174\n",
      "On problem type: lcm\n",
      "Final Score and Loss: 0.0 0.7862699\n",
      "On problem type: square_mod\n",
      "Final Score and Loss: 0.0015274949083503055 2.2549882\n",
      "On problem type: bitwise_and\n",
      "Final Score and Loss: 0.025576730190571714 2.6494453\n",
      "On problem type: bitwise_xor\n",
      "Final Score and Loss: 0.0005241090146750524 2.0928664\n",
      "On problem type: bitwise_or\n",
      "Final Score and Loss: 0.0014792899408284023 1.9183638\n",
      "~~~~~~~~ Problem Type: addition ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on addition: 98.0 ± 9.798, 4.484 ± 7.3518\n",
      "~~~~~~~~ Problem Type: division ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on division: 92.5 ± 19.2029, 7.912 ± 9.8502\n",
      "~~~~~~~~ Problem Type: multiplication ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on multiplication: 0.0 ± 0.0, 6.368 ± 4.0292\n",
      "~~~~~~~~ Problem Type: modulo ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on modulo: 0.0 ± 0.0, 2.75 ± 4.1428\n",
      "~~~~~~~~ Problem Type: gcd ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on gcd: 4.0 ± 13.5647, 1.915 ± 5.6965\n",
      "~~~~~~~~ Problem Type: lcm ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on lcm: 0.0 ± 0.0, 8.646 ± 3.2609\n",
      "~~~~~~~~ Problem Type: square_mod ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on square_mod: 1.0 ± 7.0, 2.108 ± 4.0348\n",
      "~~~~~~~~ Problem Type: bitwise_and ~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Mean score and loss of symbolic LLM on bitwise_and: 6.0 ± 16.2481, 1.718 ± 3.2385\n",
      "~~~~~~~~ Problem Type: bitwise_xor ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on bitwise_xor: 0.0 ± 0.0, 1.73 ± 2.953\n",
      "~~~~~~~~ Problem Type: bitwise_or ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on bitwise_or: 0.0 ± 0.0, 2.156 ± 4.3858\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_testing_loss_SYM_addition</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_bitwise_and</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_bitwise_or</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_bitwise_xor</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_division</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_gcd</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_lcm</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_modulo</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_multiplication</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_square_mod</td><td>▁</td></tr><tr><td>average_testing_score_SYM_addition</td><td>▁</td></tr><tr><td>average_testing_score_SYM_bitwise_and</td><td>▁</td></tr><tr><td>average_testing_score_SYM_bitwise_or</td><td>▁</td></tr><tr><td>average_testing_score_SYM_bitwise_xor</td><td>▁</td></tr><tr><td>average_testing_score_SYM_division</td><td>▁</td></tr><tr><td>average_testing_score_SYM_gcd</td><td>▁</td></tr><tr><td>average_testing_score_SYM_lcm</td><td>▁</td></tr><tr><td>average_testing_score_SYM_modulo</td><td>▁</td></tr><tr><td>average_testing_score_SYM_multiplication</td><td>▁</td></tr><tr><td>average_testing_score_SYM_square_mod</td><td>▁</td></tr><tr><td>different_actual_problem_types_count</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇████</td></tr><tr><td>epoch_bitwise_and</td><td>▁▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇█████</td></tr><tr><td>epoch_bitwise_or</td><td>▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇█</td></tr><tr><td>epoch_bitwise_xor</td><td>▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>epoch_gcd</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch_lcm</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>epoch_modulo</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▇▇▇▇████</td></tr><tr><td>epoch_multiplication</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇██</td></tr><tr><td>epoch_square_mod</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>final_loss_bitwise_and</td><td>▁</td></tr><tr><td>final_loss_bitwise_or</td><td>▁</td></tr><tr><td>final_loss_bitwise_xor</td><td>▁</td></tr><tr><td>final_loss_gcd</td><td>▁</td></tr><tr><td>final_loss_lcm</td><td>▁</td></tr><tr><td>final_loss_modulo</td><td>▁</td></tr><tr><td>final_loss_multiplication</td><td>▁</td></tr><tr><td>final_loss_square_mod</td><td>▁</td></tr><tr><td>final_score_bitwise_and</td><td>▁</td></tr><tr><td>final_score_bitwise_or</td><td>▁</td></tr><tr><td>final_score_bitwise_xor</td><td>▁</td></tr><tr><td>final_score_gcd</td><td>▁</td></tr><tr><td>final_score_lcm</td><td>▁</td></tr><tr><td>final_score_modulo</td><td>▁</td></tr><tr><td>final_score_multiplication</td><td>▁</td></tr><tr><td>final_score_square_mod</td><td>▁</td></tr><tr><td>loss</td><td>▆▅▃▃▃▃▆▃▆▂▆▁▂█▃▂▂▂▄▁▃▂▆▆▁▅▂▁▁▅▂▃▅▂▃▄▃▁▂▁</td></tr><tr><td>loss_bitwise_and</td><td>▁▁▁▁▁▁▁▁▇▁▇▁▁▁▁▂▁▁▁█▁▁▄▂▇▁▂▁▁▁▃▁▁▁▁▂▁▁▁▁</td></tr><tr><td>loss_bitwise_or</td><td>▄▄▃▅▂▂▂▃▁▁▂▁▁▂▃▁▁▁▁▇▁▁█▇▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▃</td></tr><tr><td>loss_bitwise_xor</td><td>▅▂▃▂▁▃▁▁▁▁▁▆▂▁▁▃▁▁█▁▁▁▅▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▇</td></tr><tr><td>loss_gcd</td><td>▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▂▁▁█▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_lcm</td><td>▇▆▁▅▄▁▁▁▂▁▁▁▁▃▁▂▂▁▁▁▁▁▂▆█▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_modulo</td><td>▂▂▁▁▂▁▁▁▁▁▃▁▁▁█▁▁▁▄▁▁▄▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▃▁</td></tr><tr><td>loss_multiplication</td><td>▃▃▃▄▂▂▇▁▃▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▃▁▁▁▁▁▁█</td></tr><tr><td>loss_square_mod</td><td>▃▁▄▃▄▁█▁▁▂▁▁▁▁▁▁▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁█▁▁▂▁▁█▁</td></tr><tr><td>score</td><td>▁▁▁▃▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁█▁▁▁▁▁▁▁▃▁▁▁▁</td></tr><tr><td>score_bitwise_and</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_bitwise_or</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁</td></tr><tr><td>score_bitwise_xor</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_gcd</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_lcm</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_modulo</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_multiplication</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_square_mod</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>skip_weights_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>skip_weights_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>skip_weights_2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>skip_weights_3</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>skip_weights_4</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>skip_weights_5</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_testing_loss_SYM_addition</td><td>4.48367</td></tr><tr><td>average_testing_loss_SYM_bitwise_and</td><td>1.71756</td></tr><tr><td>average_testing_loss_SYM_bitwise_or</td><td>2.15556</td></tr><tr><td>average_testing_loss_SYM_bitwise_xor</td><td>1.72978</td></tr><tr><td>average_testing_loss_SYM_division</td><td>7.91241</td></tr><tr><td>average_testing_loss_SYM_gcd</td><td>1.91508</td></tr><tr><td>average_testing_loss_SYM_lcm</td><td>8.64561</td></tr><tr><td>average_testing_loss_SYM_modulo</td><td>2.7503</td></tr><tr><td>average_testing_loss_SYM_multiplication</td><td>6.36839</td></tr><tr><td>average_testing_loss_SYM_square_mod</td><td>2.10813</td></tr><tr><td>average_testing_score_SYM_addition</td><td>0.98</td></tr><tr><td>average_testing_score_SYM_bitwise_and</td><td>0.06</td></tr><tr><td>average_testing_score_SYM_bitwise_or</td><td>0</td></tr><tr><td>average_testing_score_SYM_bitwise_xor</td><td>0</td></tr><tr><td>average_testing_score_SYM_division</td><td>0.925</td></tr><tr><td>average_testing_score_SYM_gcd</td><td>0.04</td></tr><tr><td>average_testing_score_SYM_lcm</td><td>0</td></tr><tr><td>average_testing_score_SYM_modulo</td><td>0</td></tr><tr><td>average_testing_score_SYM_multiplication</td><td>0</td></tr><tr><td>average_testing_score_SYM_square_mod</td><td>0.01</td></tr><tr><td>different_actual_problem_types_count</td><td>0</td></tr><tr><td>epoch</td><td>999</td></tr><tr><td>epoch_bitwise_and</td><td>2003</td></tr><tr><td>epoch_bitwise_or</td><td>2037</td></tr><tr><td>epoch_bitwise_xor</td><td>1917</td></tr><tr><td>epoch_gcd</td><td>2003</td></tr><tr><td>epoch_lcm</td><td>2043</td></tr><tr><td>epoch_modulo</td><td>2005</td></tr><tr><td>epoch_multiplication</td><td>2011</td></tr><tr><td>epoch_square_mod</td><td>1973</td></tr><tr><td>final_loss_bitwise_and</td><td>2.64945</td></tr><tr><td>final_loss_bitwise_or</td><td>1.91836</td></tr><tr><td>final_loss_bitwise_xor</td><td>2.09287</td></tr><tr><td>final_loss_gcd</td><td>1.01212</td></tr><tr><td>final_loss_lcm</td><td>0.78627</td></tr><tr><td>final_loss_modulo</td><td>2.6127</td></tr><tr><td>final_loss_multiplication</td><td>0.99021</td></tr><tr><td>final_loss_square_mod</td><td>2.25499</td></tr><tr><td>final_score_bitwise_and</td><td>0.02558</td></tr><tr><td>final_score_bitwise_or</td><td>0.00148</td></tr><tr><td>final_score_bitwise_xor</td><td>0.00052</td></tr><tr><td>final_score_gcd</td><td>0.0336</td></tr><tr><td>final_score_lcm</td><td>0</td></tr><tr><td>final_score_modulo</td><td>0.0015</td></tr><tr><td>final_score_multiplication</td><td>0.0005</td></tr><tr><td>final_score_square_mod</td><td>0.00153</td></tr><tr><td>loss</td><td>0.09961</td></tr><tr><td>loss_bitwise_and</td><td>0.16211</td></tr><tr><td>loss_bitwise_or</td><td>0.19336</td></tr><tr><td>loss_bitwise_xor</td><td>0.12109</td></tr><tr><td>loss_gcd</td><td>0.14453</td></tr><tr><td>loss_lcm</td><td>0.07959</td></tr><tr><td>loss_modulo</td><td>0.16797</td></tr><tr><td>loss_multiplication</td><td>0.04419</td></tr><tr><td>loss_square_mod</td><td>0.08008</td></tr><tr><td>score</td><td>0</td></tr><tr><td>score_bitwise_and</td><td>0</td></tr><tr><td>score_bitwise_or</td><td>0</td></tr><tr><td>score_bitwise_xor</td><td>0</td></tr><tr><td>score_gcd</td><td>0</td></tr><tr><td>score_lcm</td><td>0</td></tr><tr><td>score_modulo</td><td>0</td></tr><tr><td>score_multiplication</td><td>0</td></tr><tr><td>score_square_mod</td><td>0</td></tr><tr><td>skip_weights_0</td><td>0.5</td></tr><tr><td>skip_weights_1</td><td>0.5</td></tr><tr><td>skip_weights_2</td><td>0.5</td></tr><tr><td>skip_weights_3</td><td>0.5</td></tr><tr><td>skip_weights_4</td><td>0.5</td></tr><tr><td>skip_weights_5</td><td>0.5</td></tr><tr><td>symbolic_output_text_addition</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_bitwise_and</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_bitwise_or</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_bitwise_xor</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_division</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_gcd</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_lcm</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_modulo</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_multiplication</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_square_mod</td><td>Mean score and loss ...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Multiple input tokens - decoding layer (multi) (all pts)</strong> at: <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/pmbxxkrd' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/pmbxxkrd</a><br> View project at: <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 32 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250302_235030-pmbxxkrd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish() # If there is an active current run, terminate it\n",
    "config = initialize_default_config()\n",
    "\n",
    "config[\"encoder_input_tokens\"] = 'all'\n",
    "config[\"complexity\"] = 4\n",
    "config[\"initialize_decoders\"] = False\n",
    "config[\"symbolic_encoding_layer\"] = 5\n",
    "config[\"symbolic_decoding_layers\"] = [20, 22, 24, 26, 28, 30]\n",
    "config[\"num_epochs\"] = 1000\n",
    "\n",
    "#config[\"verbose\"] = 2\n",
    "#config[\"epochs_to_print\"] = config[\"num_epochs\"]\n",
    "\n",
    "#config['test_with_non_numerical_rep'] = True\n",
    "#config['train_model'] = False\n",
    "\n",
    "#config[\"testing_verbose\"] = 2\n",
    "config[\"multi_token_intervention\"]    = True\n",
    "config[\"calculate_encoding_accuracy\"] = True\n",
    "\n",
    "wandb.init(\n",
    "    project = \"Symbolic LLM - Fine Tune Decoders\",\n",
    "    name    = f\"Multiple input tokens - decoding layer (multi) (all pts)\",\n",
    "    config  = config\n",
    ")\n",
    "print(f\"STARTING NEW EXPERIMENT (run_id = {wandb.run.id})\\n\\n\")\n",
    "run_experiment(self=self, config=config)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99f9bd6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vdhanraj/llama/Programs/wandb/run-20250304_034448-9db2lscv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/9db2lscv' target=\"_blank\">Multiple input tokens - decoding layer (multi) (all pts)</a></strong> to <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/9db2lscv' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/9db2lscv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING NEW EXPERIMENT (run_id = 9db2lscv)\n",
      "\n",
      "\n",
      "Number of trainable parameters: 50331648\n",
      " -------------- Epoch 0, Loss: 16.875, Score: 0.0  -------------- \n",
      "Learning Rate changed to: 0.0005\n",
      " -------------- Epoch 100, Loss: 5.424814356435643, Score: 0.0018564356435643563  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 7.361828 , Score: 0.0\n",
      "    Problem type: modulo\n",
      "    Loss: 3.6916773 , Score: 0.0\n",
      "    Problem type: gcd\n",
      "    Loss: 1.8906215 , Score: 0.005154639175257732\n",
      "    Problem type: lcm\n",
      "    Loss: 5.8542457 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 5.7671623 , Score: 0.008\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 5.803342 , Score: 0.0\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 6.944023 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 6.1598306 , Score: 0.0\n",
      " -------------- Epoch 200, Loss: 5.099502487562189, Score: 0.003109452736318408  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 6.689183 , Score: 0.002617801047120419\n",
      "    Problem type: modulo\n",
      "    Loss: 3.3419707 , Score: 0.0024271844660194173\n",
      "    Problem type: gcd\n",
      "    Loss: 1.3905085 , Score: 0.005376344086021506\n",
      "    Problem type: lcm\n",
      "    Loss: 5.6233363 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 5.6822624 , Score: 0.006787330316742082\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 5.4830937 , Score: 0.005291005291005291\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 6.511189 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 5.8236814 , Score: 0.0025252525252525255\n",
      " -------------- Epoch 300, Loss: 4.934333471760797, Score: 0.0026993355481727574  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 6.4148273 , Score: 0.0016233766233766235\n",
      "    Problem type: modulo\n",
      "    Loss: 3.2523322 , Score: 0.0016501650165016502\n",
      "    Problem type: gcd\n",
      "    Loss: 1.1498903 , Score: 0.0035842293906810036\n",
      "    Problem type: lcm\n",
      "    Loss: 5.4249134 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 5.5363817 , Score: 0.00778816199376947\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 5.2642913 , Score: 0.005263157894736842\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 6.393753 , Score: 0.0\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 5.69559 , Score: 0.0017543859649122807\n",
      " -------------- Epoch 400, Loss: 4.807629364089776, Score: 0.0029613466334164587  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 6.2334137 , Score: 0.0012594458438287153\n",
      "    Problem type: modulo\n",
      "    Loss: 3.152919 , Score: 0.003676470588235294\n",
      "    Problem type: gcd\n",
      "    Loss: 0.9915382 , Score: 0.004010695187165776\n",
      "    Problem type: lcm\n",
      "    Loss: 5.340062 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 5.5370545 , Score: 0.0069767441860465115\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 5.1363626 , Score: 0.0038461538461538464\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 6.256884 , Score: 0.001176470588235294\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 5.526597 , Score: 0.0026455026455026454\n",
      "Learning Rate changed to: 0.0005\n",
      " -------------- Epoch 500, Loss: 4.721650449101796, Score: 0.003493013972055888  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 6.0942616 , Score: 0.0030120481927710845\n",
      "    Problem type: modulo\n",
      "    Loss: 3.131611 , Score: 0.0029644268774703555\n",
      "    Problem type: gcd\n",
      "    Loss: 0.90172714 , Score: 0.005285412262156448\n",
      "    Problem type: lcm\n",
      "    Loss: 5.2921953 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 5.4680586 , Score: 0.006375227686703097\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 5.0627956 , Score: 0.006024096385542169\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 6.2098446 , Score: 0.0009523809523809524\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 5.3537498 , Score: 0.003125\n",
      " -------------- Epoch 600, Loss: 4.649282445923461, Score: 0.0037437603993344427  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 5.9463854 , Score: 0.002508361204013378\n",
      "    Problem type: modulo\n",
      "    Loss: 3.0890539 , Score: 0.00333889816360601\n",
      "    Problem type: gcd\n",
      "    Loss: 0.8592427 , Score: 0.005263157894736842\n",
      "    Problem type: lcm\n",
      "    Loss: 5.171946 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 5.412469 , Score: 0.008487654320987654\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 5.062386 , Score: 0.006745362563237774\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 6.180835 , Score: 0.0008064516129032258\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 5.211812 , Score: 0.002529510961214165\n",
      " -------------- Epoch 700, Loss: 4.597695256776034, Score: 0.0038338088445078458  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 5.8808904 , Score: 0.002898550724637681\n",
      "    Problem type: modulo\n",
      "    Loss: 3.0424335 , Score: 0.002828854314002829\n",
      "    Problem type: gcd\n",
      "    Loss: 0.81467646 , Score: 0.006033182503770739\n",
      "    Problem type: lcm\n",
      "    Loss: 5.095875 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 5.402889 , Score: 0.009370816599732263\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 5.0177746 , Score: 0.006437768240343348\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 6.148643 , Score: 0.000697350069735007\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 5.1473527 , Score: 0.002186588921282799\n",
      " -------------- Epoch 800, Loss: 4.547733302122347, Score: 0.004057428214731586  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 5.7983603 , Score: 0.0025220680958385876\n",
      "    Problem type: modulo\n",
      "    Loss: 3.0185637 , Score: 0.003109452736318408\n",
      "    Problem type: gcd\n",
      "    Loss: 0.76281875 , Score: 0.005844155844155844\n",
      "    Problem type: lcm\n",
      "    Loss: 5.0393567 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 5.382166 , Score: 0.011255924170616114\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 4.986551 , Score: 0.00685785536159601\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 6.118467 , Score: 0.0006150061500615006\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 5.107369 , Score: 0.0019505851755526658\n",
      " -------------- Epoch 900, Loss: 4.518954633740289, Score: 0.003676470588235294  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 5.716612 , Score: 0.002205071664829107\n",
      "    Problem type: modulo\n",
      "    Loss: 3.0064273 , Score: 0.003293084522502744\n",
      "    Problem type: gcd\n",
      "    Loss: 0.7455369 , Score: 0.005238649592549476\n",
      "    Problem type: lcm\n",
      "    Loss: 4.9679966 , Score: 0.0\n",
      "    Problem type: square_mod\n",
      "    Loss: 5.389373 , Score: 0.0101931330472103\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 4.9685574 , Score: 0.00616591928251121\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 6.094035 , Score: 0.000546448087431694\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 5.06532 , Score: 0.00170261066969353\n",
      "Saved models/decoders_20241221_post_fine_tuning_9db2lscv_2025_03_04.pth\n",
      "On problem type: multiplication\n",
      "Final Score and Loss: 0.0020161290322580645 5.65246\n",
      "On problem type: modulo\n",
      "Final Score and Loss: 0.0029354207436399216 2.9619436\n",
      "On problem type: gcd\n",
      "Final Score and Loss: 0.005759162303664921 0.7405433\n",
      "On problem type: lcm\n",
      "Final Score and Loss: 0.0 4.9292974\n",
      "On problem type: square_mod\n",
      "Final Score and Loss: 0.010304219823356232 5.3848224\n",
      "On problem type: bitwise_and\n",
      "Final Score and Loss: 0.0061162079510703364 4.9696856\n",
      "On problem type: bitwise_xor\n",
      "Final Score and Loss: 0.0004965243296921549 6.065613\n",
      "On problem type: bitwise_or\n",
      "Final Score and Loss: 0.001530612244897959 5.039792\n",
      "~~~~~~~~ Problem Type: addition ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on addition: 100.0 ± 0.0, 0.0 ± 0.0\n",
      "~~~~~~~~ Problem Type: division ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on division: 98.5 ± 8.5294, 0.088 ± 0.4981\n",
      "~~~~~~~~ Problem Type: multiplication ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on multiplication: 0.0 ± 0.0, 5.663 ± 1.7432\n",
      "~~~~~~~~ Problem Type: modulo ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on modulo: 0.5 ± 4.9749, 2.771 ± 1.5889\n",
      "~~~~~~~~ Problem Type: gcd ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on gcd: 0.5 ± 4.9749, 0.598 ± 0.9095\n",
      "~~~~~~~~ Problem Type: lcm ~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Mean score and loss of symbolic LLM on lcm: 0.0 ± 0.0, 5.509 ± 1.2131\n",
      "~~~~~~~~ Problem Type: square_mod ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on square_mod: 1.0 ± 7.0, 5.375 ± 1.285\n",
      "~~~~~~~~ Problem Type: bitwise_and ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on bitwise_and: 1.5 ± 8.5294, 4.849 ± 1.1598\n",
      "~~~~~~~~ Problem Type: bitwise_xor ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on bitwise_xor: 0.5 ± 4.9749, 5.842 ± 1.2443\n",
      "~~~~~~~~ Problem Type: bitwise_or ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on bitwise_or: 0.5 ± 4.9749, 4.592 ± 1.4343\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_testing_loss_SYM_addition</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_bitwise_and</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_bitwise_or</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_bitwise_xor</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_division</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_gcd</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_lcm</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_modulo</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_multiplication</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_square_mod</td><td>▁</td></tr><tr><td>average_testing_score_SYM_addition</td><td>▁</td></tr><tr><td>average_testing_score_SYM_bitwise_and</td><td>▁</td></tr><tr><td>average_testing_score_SYM_bitwise_or</td><td>▁</td></tr><tr><td>average_testing_score_SYM_bitwise_xor</td><td>▁</td></tr><tr><td>average_testing_score_SYM_division</td><td>▁</td></tr><tr><td>average_testing_score_SYM_gcd</td><td>▁</td></tr><tr><td>average_testing_score_SYM_lcm</td><td>▁</td></tr><tr><td>average_testing_score_SYM_modulo</td><td>▁</td></tr><tr><td>average_testing_score_SYM_multiplication</td><td>▁</td></tr><tr><td>average_testing_score_SYM_square_mod</td><td>▁</td></tr><tr><td>different_actual_problem_types_count</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>epoch_bitwise_and</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇█████</td></tr><tr><td>epoch_bitwise_or</td><td>▁▁▁▁▁▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>epoch_bitwise_xor</td><td>▁▁▁▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>epoch_gcd</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>epoch_lcm</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇████</td></tr><tr><td>epoch_modulo</td><td>▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇███</td></tr><tr><td>epoch_multiplication</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch_square_mod</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>final_loss_bitwise_and</td><td>▁</td></tr><tr><td>final_loss_bitwise_or</td><td>▁</td></tr><tr><td>final_loss_bitwise_xor</td><td>▁</td></tr><tr><td>final_loss_gcd</td><td>▁</td></tr><tr><td>final_loss_lcm</td><td>▁</td></tr><tr><td>final_loss_modulo</td><td>▁</td></tr><tr><td>final_loss_multiplication</td><td>▁</td></tr><tr><td>final_loss_square_mod</td><td>▁</td></tr><tr><td>final_score_bitwise_and</td><td>▁</td></tr><tr><td>final_score_bitwise_or</td><td>▁</td></tr><tr><td>final_score_bitwise_xor</td><td>▁</td></tr><tr><td>final_score_gcd</td><td>▁</td></tr><tr><td>final_score_lcm</td><td>▁</td></tr><tr><td>final_score_modulo</td><td>▁</td></tr><tr><td>final_score_multiplication</td><td>▁</td></tr><tr><td>final_score_square_mod</td><td>▁</td></tr><tr><td>loss</td><td>▇█▆▆▆▆▆▆▄▆▇▅▆▆▄▇█▅▆▂▃▆▅▅▅▄▅▁▄▇▆▃▇█▄▅▅▆▇▄</td></tr><tr><td>loss_bitwise_and</td><td>██▃▆▅▆▆▆▅▇▆▃▆▅▆▅▂▃▄▃▇▇▁▁▅▇▃▅▂▄▂▆▅▄▃▄▄▆▅▅</td></tr><tr><td>loss_bitwise_or</td><td>▄▄█▂▄▃▃█▃▂▄▅▃▁▃▄▁▃▃▂▃▄▃▂▃▃▄▃▄▃▃█▂▃▂▂▁▂▂▁</td></tr><tr><td>loss_bitwise_xor</td><td>▆▅▆▄▆▅▆▃▆▆▅▃▁▅▅▅▅▅▄▅▅▄▄▄▂▆█▇▄▃▄▄▄▆▄▆▆▆▆▂</td></tr><tr><td>loss_gcd</td><td>▄▁▁▂▆▁▁▄▂▂▁▁▄▁▁▁▂▁█▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_lcm</td><td>█▆▇▆▅▅▆▃▅▃▅▂▃▂▃▄▃▄▅▃▃▃▄▅▅▃▂▁▂▄▃▂▆▃▄▃▂▂▃▃</td></tr><tr><td>loss_modulo</td><td>▄▅▅█▃▂▂▁▄▃▂▂▁▅▁▁▂▃▃▁▂▁▃▁▂▅▁▁▁▄▃▄▂▁▃▃▁▁▁▂</td></tr><tr><td>loss_multiplication</td><td>█▃▃▃▃▂▂▃▃▂▂▃▃▂▂▃▁▂▂▂▂▂▂▂▃▂▂▃▂▁▂▂▁▂▁▂▂▂▃▂</td></tr><tr><td>loss_square_mod</td><td>█▅▃▅▆▅▆▁▆▄▅▄▆▆▆▆▄▅▇▃▆▅▆▃▅█▅▆▆▃▆▄▆▆▂▃▆▆▄▆</td></tr><tr><td>score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_bitwise_and</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_bitwise_or</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_bitwise_xor</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_gcd</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_lcm</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_modulo</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_multiplication</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>score_square_mod</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁</td></tr><tr><td>skip_weights_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>skip_weights_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>skip_weights_2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>skip_weights_3</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>skip_weights_4</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>skip_weights_5</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_testing_loss_SYM_addition</td><td>0.0</td></tr><tr><td>average_testing_loss_SYM_bitwise_and</td><td>4.84906</td></tr><tr><td>average_testing_loss_SYM_bitwise_or</td><td>4.59227</td></tr><tr><td>average_testing_loss_SYM_bitwise_xor</td><td>5.84234</td></tr><tr><td>average_testing_loss_SYM_division</td><td>0.08751</td></tr><tr><td>average_testing_loss_SYM_gcd</td><td>0.59785</td></tr><tr><td>average_testing_loss_SYM_lcm</td><td>5.50906</td></tr><tr><td>average_testing_loss_SYM_modulo</td><td>2.77146</td></tr><tr><td>average_testing_loss_SYM_multiplication</td><td>5.66328</td></tr><tr><td>average_testing_loss_SYM_square_mod</td><td>5.37461</td></tr><tr><td>average_testing_score_SYM_addition</td><td>1</td></tr><tr><td>average_testing_score_SYM_bitwise_and</td><td>0.015</td></tr><tr><td>average_testing_score_SYM_bitwise_or</td><td>0.005</td></tr><tr><td>average_testing_score_SYM_bitwise_xor</td><td>0.005</td></tr><tr><td>average_testing_score_SYM_division</td><td>0.985</td></tr><tr><td>average_testing_score_SYM_gcd</td><td>0.005</td></tr><tr><td>average_testing_score_SYM_lcm</td><td>0</td></tr><tr><td>average_testing_score_SYM_modulo</td><td>0.005</td></tr><tr><td>average_testing_score_SYM_multiplication</td><td>0</td></tr><tr><td>average_testing_score_SYM_square_mod</td><td>0.01</td></tr><tr><td>different_actual_problem_types_count</td><td>0</td></tr><tr><td>epoch</td><td>999</td></tr><tr><td>epoch_bitwise_and</td><td>1971</td></tr><tr><td>epoch_bitwise_or</td><td>1969</td></tr><tr><td>epoch_bitwise_xor</td><td>2023</td></tr><tr><td>epoch_gcd</td><td>1919</td></tr><tr><td>epoch_lcm</td><td>2017</td></tr><tr><td>epoch_modulo</td><td>2053</td></tr><tr><td>epoch_multiplication</td><td>1993</td></tr><tr><td>epoch_square_mod</td><td>2047</td></tr><tr><td>final_loss_bitwise_and</td><td>4.96969</td></tr><tr><td>final_loss_bitwise_or</td><td>5.03979</td></tr><tr><td>final_loss_bitwise_xor</td><td>6.06561</td></tr><tr><td>final_loss_gcd</td><td>0.74054</td></tr><tr><td>final_loss_lcm</td><td>4.9293</td></tr><tr><td>final_loss_modulo</td><td>2.96194</td></tr><tr><td>final_loss_multiplication</td><td>5.65246</td></tr><tr><td>final_loss_square_mod</td><td>5.38482</td></tr><tr><td>final_score_bitwise_and</td><td>0.00612</td></tr><tr><td>final_score_bitwise_or</td><td>0.00153</td></tr><tr><td>final_score_bitwise_xor</td><td>0.0005</td></tr><tr><td>final_score_gcd</td><td>0.00576</td></tr><tr><td>final_score_lcm</td><td>0</td></tr><tr><td>final_score_modulo</td><td>0.00294</td></tr><tr><td>final_score_multiplication</td><td>0.00202</td></tr><tr><td>final_score_square_mod</td><td>0.0103</td></tr><tr><td>loss</td><td>3.35938</td></tr><tr><td>loss_bitwise_and</td><td>5.59375</td></tr><tr><td>loss_bitwise_or</td><td>3.85938</td></tr><tr><td>loss_bitwise_xor</td><td>4.59375</td></tr><tr><td>loss_gcd</td><td>0.01251</td></tr><tr><td>loss_lcm</td><td>3.26562</td></tr><tr><td>loss_modulo</td><td>1.33594</td></tr><tr><td>loss_multiplication</td><td>2.28125</td></tr><tr><td>loss_square_mod</td><td>4.78125</td></tr><tr><td>score</td><td>0</td></tr><tr><td>score_bitwise_and</td><td>0</td></tr><tr><td>score_bitwise_or</td><td>0</td></tr><tr><td>score_bitwise_xor</td><td>0</td></tr><tr><td>score_gcd</td><td>0</td></tr><tr><td>score_lcm</td><td>0</td></tr><tr><td>score_modulo</td><td>0</td></tr><tr><td>score_multiplication</td><td>0</td></tr><tr><td>score_square_mod</td><td>0</td></tr><tr><td>skip_weights_0</td><td>0.5</td></tr><tr><td>skip_weights_1</td><td>0.5</td></tr><tr><td>skip_weights_2</td><td>0.5</td></tr><tr><td>skip_weights_3</td><td>0.5</td></tr><tr><td>skip_weights_4</td><td>0.5</td></tr><tr><td>skip_weights_5</td><td>0.5</td></tr><tr><td>symbolic_output_text_addition</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_bitwise_and</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_bitwise_or</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_bitwise_xor</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_division</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_gcd</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_lcm</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_modulo</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_multiplication</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_square_mod</td><td>Mean score and loss ...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Multiple input tokens - decoding layer (multi) (all pts)</strong> at: <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/9db2lscv' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/9db2lscv</a><br> View project at: <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 32 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250304_034448-9db2lscv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish() # If there is an active current run, terminate it\n",
    "config = initialize_default_config()\n",
    "\n",
    "config[\"encoder_input_tokens\"] = 'all'\n",
    "config[\"complexity\"] = 2\n",
    "config[\"initialize_decoders\"] = False\n",
    "config[\"symbolic_encoding_layer\"] = 5\n",
    "config[\"symbolic_decoding_layers\"] = [20, 22, 24, 26, 28, 30]\n",
    "config[\"num_epochs\"] = 1000\n",
    "\n",
    "#config[\"verbose\"] = 2\n",
    "#config[\"epochs_to_print\"] = config[\"num_epochs\"]\n",
    "\n",
    "#config['test_with_non_numerical_rep'] = True\n",
    "#config['train_model'] = False\n",
    "\n",
    "#config[\"testing_verbose\"] = 2\n",
    "config[\"multi_token_intervention\"]    = True\n",
    "config[\"calculate_encoding_accuracy\"] = True\n",
    "\n",
    "wandb.init(\n",
    "    project = \"Symbolic LLM - Fine Tune Decoders\",\n",
    "    name    = f\"Multiple input tokens - decoding layer (multi) (all pts)\",\n",
    "    config  = config\n",
    ")\n",
    "print(f\"STARTING NEW EXPERIMENT (run_id = {wandb.run.id})\\n\\n\")\n",
    "run_experiment(self=self, config=config)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e78d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea05e78c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7323fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f2af7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2ce447e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wandb.finish() # If there is an active current run, terminate it\n",
    "# config = initialize_default_config()\n",
    "\n",
    "# config[\"num_epochs\"] = 1000\n",
    "# config[\"encoder_input_tokens\"] = 'all'\n",
    "# config[\"complexity\"] = 3\n",
    "# config[\"initialize_decoders\"] = False\n",
    "# config[\"symbolic_encoding_layer\"] = 5\n",
    "# config[\"symbolic_decoding_layers\"] = [20]\n",
    "\n",
    "# #config[\"verbose\"] = 2\n",
    "# #config[\"epochs_to_print\"] = config[\"num_epochs\"]\n",
    "\n",
    "# #config['test_with_non_numerical_rep'] = True\n",
    "# #config['train_model'] = False\n",
    "\n",
    "# #config[\"problem_type\"] = [\"multiplication\", \"lcm\"]\n",
    "# #config[\"testing_problems\"] = [\"multiplication\", \"lcm\"]\n",
    "# #config[\"testing_verbose\"] = 2\n",
    "\n",
    "# config[\"multi_token_intervention\"] = True\n",
    "# config[\"calculate_encoding_accuracy\"] = True\n",
    "\n",
    "# wandb.init(\n",
    "#     project = \"Symbolic LLM - Fine Tune Decoders\",\n",
    "#     name    = f\"Multiple input tokens - multi token intervention\",\n",
    "#     config  = config\n",
    "# )\n",
    "# print(f\"STARTING NEW EXPERIMENT (run_id = {wandb.run.id})\\n\\n\")\n",
    "# run_experiment(self=self, config=config)\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b608fe8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ae614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64a78b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be8db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeaa6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "023030fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1000 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT DELETE\n",
    "average_accuracy_per_pt = {}\n",
    "\n",
    "digits = list(self.model.encoding_accuracy[problem_types[0]].keys())\n",
    "problem_types = list(self.model.encoding_accuracy.keys())\n",
    "\n",
    "for pt in problem_types:\n",
    "    average_accuracy_per_pt[pt] = {}\n",
    "    for d in digits:\n",
    "        average_accuracy_per_pt[pt][d] = [0, 0]\n",
    "        average_accuracy_per_pt[pt][d][0] = np.mean(self.model.encoding_accuracy[pt][d][\"first_number\"])*100\n",
    "        average_accuracy_per_pt[pt][d][1] = np.mean(self.model.encoding_accuracy[pt][d][\"second_number\"])*100\n",
    "\n",
    "\n",
    "# Define colors for first and second number\n",
    "colors = ['blue', 'orange']\n",
    "\n",
    "# Create subplots (one per problem type)\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(18, 10))  # Adjust grid based on the number of problem types\n",
    "axes = axes.flatten()  # Flatten to index easier\n",
    "\n",
    "for i, (problem_type, digits_data) in enumerate(average_accuracy_per_pt.items()):\n",
    "    ax = axes[i]\n",
    "\n",
    "    digits = list(digits_data.keys())  # ['digit 0', 'digit 1', ...]\n",
    "    x = np.arange(len(digits))  # X-axis positions\n",
    "\n",
    "    first_num = [digits_data[d][0] for d in digits]  # First number accuracies\n",
    "    second_num = [digits_data[d][1] for d in digits]  # Second number accuracies\n",
    "\n",
    "    width = 0.35  # Width of bars\n",
    "    ax.bar(x - width/2, first_num, width, label='First Number', color=colors[0])\n",
    "    ax.bar(x + width/2, second_num, width, label='Second Number', color=colors[1])\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(digits, rotation=45)\n",
    "    ax.set_ylim(75, 95)  # Adjust y-axis for better visibility\n",
    "    ax.set_title(problem_type)\n",
    "    ax.legend()\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute average per digit across all problem types\n",
    "digits = ['digit 0', 'digit 1', 'digit 2', 'digit 3']\n",
    "avg_first_number = []\n",
    "avg_second_number = []\n",
    "\n",
    "for d in digits:\n",
    "    first_vals = [average_accuracy_per_pt[pt][d][0] for pt in average_accuracy_per_pt]\n",
    "    second_vals = [average_accuracy_per_pt[pt][d][1] for pt in average_accuracy_per_pt]\n",
    "    avg_first_number.append(np.mean(first_vals))\n",
    "    avg_second_number.append(np.mean(second_vals))\n",
    "\n",
    "# Plotting\n",
    "x = np.arange(len(digits))  # X-axis positions\n",
    "width = 0.35  # Bar width\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.bar(x - width/2, avg_first_number, width, label='First Number', color='blue')\n",
    "ax.bar(x + width/2, avg_second_number, width, label='Second Number', color='orange')\n",
    "\n",
    "# Formatting\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(digits, rotation=45)\n",
    "ax.set_ylim(75, 95)  # Adjust y-axis for better visibility\n",
    "ax.set_ylabel(\"Average Accuracy\")\n",
    "ax.set_title(\"Average Encoding Accuracy Across Problem Types\")\n",
    "ax.legend()\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230e517f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f05ca5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe9dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54f7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a40ab1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05feb46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d719c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2f37cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cbe96ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (96255774.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 11\u001b[0;36m\u001b[0m\n\u001b[0;31m    config[“num_epochs”] = 250\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c349e407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvarun_dhanraj\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vdhanraj/llama/Programs/wandb/run-20250226_184045-qfjqvsoa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/qfjqvsoa' target=\"_blank\">Multiple input tokens</a></strong> to <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/qfjqvsoa' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/qfjqvsoa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING NEW EXPERIMENT (run_id = qfjqvsoa)\n",
      "\n",
      "\n",
      "Number of trainable parameters: 8388608\n",
      " -------------- Epoch 0, Loss: 8.625, Score: 0.0  -------------- \n",
      " -------------- Epoch 1, Loss: 8.4375, Score: 0.0  -------------- \n",
      " -------------- Epoch 2, Loss: 9.125, Score: 0.0  -------------- \n",
      " -------------- Epoch 3, Loss: 7.0, Score: 0.0  -------------- \n",
      " -------------- Epoch 4, Loss: 7.15625, Score: 0.0  -------------- \n",
      " -------------- Epoch 5, Loss: 8.1875, Score: 0.0  -------------- \n",
      " -------------- Epoch 6, Loss: 4.53125, Score: 0.0  -------------- \n",
      " -------------- Epoch 7, Loss: 7.8125, Score: 0.0  -------------- \n",
      " -------------- Epoch 8, Loss: 8.25, Score: 0.0  -------------- \n",
      " -------------- Epoch 9, Loss: 6.71875, Score: 0.0  -------------- \n",
      " -------------- Epoch 10, Loss: 6.03125, Score: 0.0  -------------- \n",
      " -------------- Epoch 11, Loss: 6.625, Score: 0.0  -------------- \n",
      " -------------- Epoch 12, Loss: 4.78125, Score: 0.0  -------------- \n",
      " -------------- Epoch 13, Loss: 5.90625, Score: 0.0  -------------- \n",
      " -------------- Epoch 14, Loss: 6.3125, Score: 0.0  -------------- \n",
      " -------------- Epoch 15, Loss: 7.3125, Score: 0.0  -------------- \n",
      " -------------- Epoch 16, Loss: 5.1875, Score: 0.0  -------------- \n",
      " -------------- Epoch 17, Loss: 5.0625, Score: 0.0  -------------- \n",
      " -------------- Epoch 18, Loss: 5.0, Score: 0.0  -------------- \n",
      " -------------- Epoch 19, Loss: 5.53125, Score: 0.0  -------------- \n",
      " -------------- Epoch 20, Loss: 4.71875, Score: 0.0  -------------- \n",
      " -------------- Epoch 21, Loss: 6.375, Score: 0.0  -------------- \n",
      " -------------- Epoch 22, Loss: 6.125, Score: 0.0  -------------- \n",
      " -------------- Epoch 23, Loss: 7.0, Score: 0.0  -------------- \n",
      " -------------- Epoch 24, Loss: 6.4375, Score: 0.0  -------------- \n",
      " -------------- Epoch 25, Loss: 5.375, Score: 0.0  -------------- \n",
      " -------------- Epoch 26, Loss: 5.8125, Score: 0.0  -------------- \n",
      " -------------- Epoch 27, Loss: 6.25, Score: 0.0  -------------- \n",
      " -------------- Epoch 28, Loss: 7.25, Score: 0.0625  -------------- \n",
      " -------------- Epoch 29, Loss: 5.59375, Score: 0.0  -------------- \n",
      " -------------- Epoch 30, Loss: 6.1875, Score: 0.0  -------------- \n",
      " -------------- Epoch 31, Loss: 7.625, Score: 0.0  -------------- \n",
      " -------------- Epoch 32, Loss: 8.0, Score: 0.0  -------------- \n",
      " -------------- Epoch 33, Loss: 5.84375, Score: 0.0  -------------- \n",
      " -------------- Epoch 34, Loss: 5.34375, Score: 0.0  -------------- \n",
      " -------------- Epoch 35, Loss: 6.15625, Score: 0.0  -------------- \n",
      " -------------- Epoch 36, Loss: 5.875, Score: 0.0  -------------- \n",
      " -------------- Epoch 37, Loss: 5.0625, Score: 0.0  -------------- \n",
      " -------------- Epoch 38, Loss: 5.5625, Score: 0.0  -------------- \n",
      " -------------- Epoch 39, Loss: 5.875, Score: 0.0  -------------- \n",
      " -------------- Epoch 40, Loss: 5.25, Score: 0.0  -------------- \n",
      " -------------- Epoch 41, Loss: 5.84375, Score: 0.0  -------------- \n",
      " -------------- Epoch 42, Loss: 4.90625, Score: 0.0  -------------- \n",
      " -------------- Epoch 43, Loss: 5.5, Score: 0.0  -------------- \n",
      " -------------- Epoch 44, Loss: 5.5625, Score: 0.0  -------------- \n",
      " -------------- Epoch 45, Loss: 4.84375, Score: 0.0  -------------- \n",
      " -------------- Epoch 46, Loss: 5.1875, Score: 0.0  -------------- \n",
      " -------------- Epoch 47, Loss: 6.1875, Score: 0.0  -------------- \n",
      " -------------- Epoch 48, Loss: 6.59375, Score: 0.0  -------------- \n",
      " -------------- Epoch 49, Loss: 4.34375, Score: 0.0  -------------- \n",
      " -------------- Epoch 50, Loss: 5.25, Score: 0.0  -------------- \n",
      " -------------- Epoch 51, Loss: 4.96875, Score: 0.0  -------------- \n",
      " -------------- Epoch 52, Loss: 4.21875, Score: 0.0  -------------- \n",
      " -------------- Epoch 53, Loss: 6.0, Score: 0.0  -------------- \n",
      " -------------- Epoch 54, Loss: 5.3125, Score: 0.0  -------------- \n",
      " -------------- Epoch 55, Loss: 4.65625, Score: 0.0  -------------- \n",
      " -------------- Epoch 56, Loss: 5.375, Score: 0.0  -------------- \n",
      " -------------- Epoch 57, Loss: 4.75, Score: 0.0  -------------- \n",
      " -------------- Epoch 58, Loss: 4.375, Score: 0.0  -------------- \n",
      " -------------- Epoch 59, Loss: 5.125, Score: 0.0  -------------- \n",
      " -------------- Epoch 60, Loss: 4.875, Score: 0.0  -------------- \n",
      " -------------- Epoch 61, Loss: 5.4375, Score: 0.0  -------------- \n",
      " -------------- Epoch 62, Loss: 5.0, Score: 0.0  -------------- \n",
      " -------------- Epoch 63, Loss: 4.6875, Score: 0.0  -------------- \n",
      " -------------- Epoch 64, Loss: 4.6875, Score: 0.0  -------------- \n",
      " -------------- Epoch 65, Loss: 4.34375, Score: 0.0  -------------- \n",
      " -------------- Epoch 66, Loss: 4.4375, Score: 0.0  -------------- \n",
      " -------------- Epoch 67, Loss: 5.15625, Score: 0.0  -------------- \n",
      " -------------- Epoch 68, Loss: 6.0625, Score: 0.0  -------------- \n",
      " -------------- Epoch 69, Loss: 5.3125, Score: 0.0  -------------- \n",
      " -------------- Epoch 70, Loss: 4.5625, Score: 0.0  -------------- \n",
      " -------------- Epoch 71, Loss: 5.1875, Score: 0.0  -------------- \n",
      " -------------- Epoch 72, Loss: 4.25, Score: 0.0  -------------- \n",
      " -------------- Epoch 73, Loss: 4.75, Score: 0.0  -------------- \n",
      " -------------- Epoch 74, Loss: 4.9375, Score: 0.0  -------------- \n",
      " -------------- Epoch 75, Loss: 4.9375, Score: 0.0  -------------- \n",
      " -------------- Epoch 76, Loss: 5.75, Score: 0.0  -------------- \n",
      " -------------- Epoch 77, Loss: 5.125, Score: 0.0  -------------- \n",
      " -------------- Epoch 78, Loss: 5.03125, Score: 0.0  -------------- \n",
      " -------------- Epoch 79, Loss: 4.0, Score: 0.0625  -------------- \n",
      " -------------- Epoch 80, Loss: 4.5625, Score: 0.0  -------------- \n",
      " -------------- Epoch 81, Loss: 4.8125, Score: 0.0  -------------- \n",
      " -------------- Epoch 82, Loss: 5.40625, Score: 0.0  -------------- \n",
      " -------------- Epoch 83, Loss: 5.15625, Score: 0.125  -------------- \n",
      " -------------- Epoch 84, Loss: 4.6875, Score: 0.0  -------------- \n",
      " -------------- Epoch 85, Loss: 4.28125, Score: 0.0625  -------------- \n",
      " -------------- Epoch 86, Loss: 4.8125, Score: 0.0  -------------- \n",
      " -------------- Epoch 87, Loss: 3.953125, Score: 0.0  -------------- \n",
      " -------------- Epoch 88, Loss: 4.0625, Score: 0.0625  -------------- \n",
      " -------------- Epoch 89, Loss: 4.75, Score: 0.0  -------------- \n",
      " -------------- Epoch 90, Loss: 4.125, Score: 0.0  -------------- \n",
      " -------------- Epoch 91, Loss: 4.0, Score: 0.0625  -------------- \n",
      " -------------- Epoch 92, Loss: 3.8125, Score: 0.125  -------------- \n",
      " -------------- Epoch 93, Loss: 5.21875, Score: 0.0  -------------- \n",
      " -------------- Epoch 94, Loss: 4.0625, Score: 0.0  -------------- \n",
      " -------------- Epoch 95, Loss: 4.125, Score: 0.0  -------------- \n",
      " -------------- Epoch 96, Loss: 4.4375, Score: 0.0  -------------- \n",
      " -------------- Epoch 97, Loss: 4.125, Score: 0.0  -------------- \n",
      " -------------- Epoch 98, Loss: 3.578125, Score: 0.0  -------------- \n",
      " -------------- Epoch 99, Loss: 4.5, Score: 0.125  -------------- \n",
      "Learning Rate changed to: 0.0005\n",
      " -------------- Epoch 100, Loss: 5.0, Score: 0.0  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 5.5587187 , Score: 0.0026666666666666666\n",
      "    Problem type: lcm\n",
      "    Loss: 5.3934984 , Score: 0.010392609699769052\n",
      " -------------- Epoch 101, Loss: 3.8125, Score: 0.0625  -------------- \n",
      " -------------- Epoch 102, Loss: 3.625, Score: 0.0625  -------------- \n",
      " -------------- Epoch 103, Loss: 4.0625, Score: 0.0625  -------------- \n",
      " -------------- Epoch 104, Loss: 4.8125, Score: 0.0625  -------------- \n",
      " -------------- Epoch 105, Loss: 4.03125, Score: 0.125  -------------- \n",
      " -------------- Epoch 106, Loss: 4.625, Score: 0.0  -------------- \n",
      " -------------- Epoch 107, Loss: 3.390625, Score: 0.1875  -------------- \n",
      " -------------- Epoch 108, Loss: 4.34375, Score: 0.0  -------------- \n",
      " -------------- Epoch 109, Loss: 4.0, Score: 0.125  -------------- \n",
      " -------------- Epoch 110, Loss: 5.25, Score: 0.25  -------------- \n",
      " -------------- Epoch 111, Loss: 4.09375, Score: 0.0625  -------------- \n",
      " -------------- Epoch 112, Loss: 3.9375, Score: 0.0  -------------- \n",
      " -------------- Epoch 113, Loss: 4.03125, Score: 0.125  -------------- \n",
      " -------------- Epoch 114, Loss: 4.25, Score: 0.0  -------------- \n",
      " -------------- Epoch 115, Loss: 6.1875, Score: 0.0  -------------- \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------------- Epoch 116, Loss: 4.09375, Score: 0.0625  -------------- \n",
      " -------------- Epoch 117, Loss: 3.96875, Score: 0.0  -------------- \n",
      " -------------- Epoch 118, Loss: 3.46875, Score: 0.0625  -------------- \n",
      " -------------- Epoch 119, Loss: 4.40625, Score: 0.0625  -------------- \n",
      " -------------- Epoch 120, Loss: 4.375, Score: 0.0  -------------- \n",
      " -------------- Epoch 121, Loss: 4.375, Score: 0.125  -------------- \n",
      " -------------- Epoch 122, Loss: 4.09375, Score: 0.0625  -------------- \n",
      " -------------- Epoch 123, Loss: 2.921875, Score: 0.0625  -------------- \n",
      " -------------- Epoch 124, Loss: 4.09375, Score: 0.0625  -------------- \n",
      " -------------- Epoch 125, Loss: 3.984375, Score: 0.0625  -------------- \n",
      " -------------- Epoch 126, Loss: 3.390625, Score: 0.1875  -------------- \n",
      " -------------- Epoch 127, Loss: 4.65625, Score: 0.25  -------------- \n",
      " -------------- Epoch 128, Loss: 3.765625, Score: 0.0625  -------------- \n",
      " -------------- Epoch 129, Loss: 3.5, Score: 0.0  -------------- \n",
      " -------------- Epoch 130, Loss: 3.984375, Score: 0.0625  -------------- \n",
      " -------------- Epoch 131, Loss: 4.40625, Score: 0.125  -------------- \n",
      " -------------- Epoch 132, Loss: 5.53125, Score: 0.0625  -------------- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wandb.finish() # If there is an active current run, terminate it\n",
    "config = initialize_default_config()\n",
    "\n",
    "config[\"encoder_input_tokens\"] = 'all'\n",
    "config[\"complexity\"] = 3\n",
    "config[\"initialize_decoders\"] = False\n",
    "config[\"symbolic_encoding_layer\"] = 5\n",
    "config[\"symbolic_decoding_layers\"] = [10]\n",
    "\n",
    "#config[\"verbose\"] = 2\n",
    "config[\"epochs_to_print\"] = config[\"num_epochs\"]\n",
    "\n",
    "#config['test_with_non_numerical_rep'] = True\n",
    "#config['train_model'] = False\n",
    "\n",
    "config[\"problem_type\"] = [\"multiplication\", \"lcm\"]\n",
    "config[\"testing_problems\"] = [\"multiplication\", \"lcm\"]\n",
    "#config[\"testing_verbose\"] = 2\n",
    "\n",
    "wandb.init(\n",
    "    project = \"Symbolic LLM - Fine Tune Decoders\",\n",
    "    name    = f\"Multiple input tokens\",\n",
    "    config  = config\n",
    ")\n",
    "print(f\"STARTING NEW EXPERIMENT (run_id = {wandb.run.id})\\n\\n\")\n",
    "run_experiment(self=self, config=config)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b861ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in self.model.decoders[10].parameters():\n",
    "    print(p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaf4a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
