{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "764932c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import wandb\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from typing import List, Optional\n",
    "import fire\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from typing import List\n",
    "\n",
    "from fairscale.nn.model_parallel.initialize import (\n",
    "    get_model_parallel_rank,\n",
    "    initialize_model_parallel,\n",
    "    model_parallel_is_initialized,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec1e3450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description=\"Train Encoders and Decoders\")\n",
    "\n",
    "# # Define arguments\n",
    "# parser.add_argument(\"--curr_dir\",       type=str,  required=False, help=\"Directory of Program\",                                                          default=\"~/Neurosymbolic-LLM/Programs\")\n",
    "# parser.add_argument(\"--git_dir\",        type=str,  required=False, help=\"Directory of Main Github\",                                                      default=\"~/Neurosymbolic-LLM\")\n",
    "# parser.add_argument(\"--chpt_dir\",       type=str,  required=False, help=\"Model Checkpoint Directory\",                                                    default=\"~/.llama/checkpoints/Llama3.1-8B-Instruct\")\n",
    "# parser.add_argument(\"--tokenizer_path\", type=str,  required=False, help=\"Tokenizer Checkpoint Directory\",                                                default=\"~/.llama/checkpoints/Llama3.1-8B-Instruct/tokenizer.model\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# curr_dir       = str(Path(args.curr_dir).expanduser())\n",
    "# git_dir        = str(Path(args.git_dir ).expanduser())\n",
    "# ckpt_dir       = str(Path(args.chpt_dir).expanduser())\n",
    "# tokenizer_path = str(Path(args.tokenizer_path).expanduser())\n",
    "\n",
    "curr_dir       = str(Path(\"~/Neurosymbolic-LLM/Programs\").expanduser())\n",
    "git_dir        = str(Path(\"~/Neurosymbolic-LLM\").expanduser())\n",
    "ckpt_dir       = str(Path(\"~/.llama/checkpoints/Llama3.1-8B-Instruct\").expanduser())\n",
    "tokenizer_path = str(Path(\"~/.llama/checkpoints/Llama3.1-8B-Instruct/tokenizer.model\").expanduser())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a80407bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, git_dir)\n",
    "\n",
    "from llama.generation import sample_top_p\n",
    "from llama.encoder_decoder_networks import Encoder, Decoder, Encoder_Deep, Decoder_Deep\n",
    "from llama.vsa_engine import *\n",
    "from llama.utilities import *\n",
    "\n",
    "from llama import Dialog, Llama\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d58eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_date = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "log_wandb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23802adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4090\n",
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n",
      "Loaded in 12.12 seconds\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 10000\n",
    "max_batch_size = 2 # set to 1 if doing CoT to not overload the GPU, otherwise can handle up to 4\n",
    "model_parallel_size = 1\n",
    "\n",
    "top_p = 0.9\n",
    "temperature = 0\n",
    "max_gen_len = None\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['WORLD_SIZE'] = \"1\"\n",
    "os.environ['MASTER_ADDR'] = \"127.0.0.2\"\n",
    "os.environ['MASTER_PORT'] = \"29502\"\n",
    "os.environ['LOCAL_RANK']  = \"0\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "\n",
    "generator = Llama.build(\n",
    "    ckpt_dir=ckpt_dir,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    max_seq_len=max_seq_len,\n",
    "    max_batch_size=max_batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cd721ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_digits = 5 #15 # maximum representable number is 10**max_digits\n",
    "VSA_dim = 2048\n",
    "possible_problems=[\"addition\", \"multiplication\", \"division\", \"modulo\", \"gcd\", \"lcm\", \"square_mod\", \"bitwise_and\", \"bitwise_xor\", \"bitwise_or\"]\n",
    "\n",
    "possible_problems_str = \"_\".join(possible_problems)\n",
    "generator.model.SE = torch.load(f\"{curr_dir}/VSA_library/symbolic_engine_VSA_dim_{VSA_dim}\"\n",
    "                                f\"_max_digits_{max_digits}_problem_types_{possible_problems_str}.pt\", weights_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6daa4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dialog_indices(generator, dialog, calculate_end_index=False):\n",
    "    start_indices = []\n",
    "    end_indices   = []\n",
    "    for i in range(len(dialog)):\n",
    "        # Find the final occurance of user chat (which is the question being asked to the LLM)\n",
    "        start_index = len(generator.parse_chat(dialog)[i]) - generator.parse_chat(dialog)[i][::-1].index(882) + 2\n",
    "        # The final token position to save\n",
    "        if not calculate_end_index:\n",
    "            end_index   = -1 # If end_index is -1, use all tokens up till the end, otherwise calculate based on eot token\n",
    "        else:\n",
    "            end_index   = len(generator.parse_chat(dialog)[i]) - generator.parse_chat(dialog)[i][::-1].index(128009) - 1\n",
    "        start_indices += [start_index]\n",
    "        end_indices   += [end_index]\n",
    "        \n",
    "    return start_indices, end_indices\n",
    "\n",
    "def training_step(n_samples, generator, temperature=0, problem_type=\"addition\", inference_to_backprop_ratio=1, \n",
    "                  optimizer=None, criterion=None, complexity=2, losses_per_pt=None, scores_per_pt=None, verbose=False):\n",
    "\n",
    "    all_logits = []\n",
    "    #all_x     = []\n",
    "    #all_y     = []\n",
    "    all_corr  = []\n",
    "\n",
    "    total_score = 0\n",
    "    outputs = []\n",
    "    pts     = []\n",
    "\n",
    "    for n in range(inference_to_backprop_ratio):\n",
    "        if verbose:\n",
    "            print(\"On sub-epoch iteration:\", n+1)\n",
    "        response_data = []\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        dialogs, x, y, curr_problem_type = generate_dialog(complexity=complexity, samples=n_samples, problem_type=problem_type)\n",
    "\n",
    "        if generator.model.encoder_input_tokens == \"all\":\n",
    "            start_indices, end_indices    = get_dialog_indices(dialogs, calculate_end_index=generator.model.calculate_end_index)\n",
    "            generator.model.curr_start_indices = start_indices\n",
    "            generator.model.curr_end_indices   = end_indices\n",
    "            generator.model.dialogs            = dialogs\n",
    "\n",
    "        if generator.model.calculate_encoding_accuracy:\n",
    "            generator.model.curr_x = x\n",
    "            generator.model.curr_y = y\n",
    "\n",
    "        pts += [curr_problem_type]\n",
    "        if curr_problem_type==\"addition\":\n",
    "            correct_responses = [x[i] + y[i] for i in range(len(x))]\n",
    "        if curr_problem_type==\"multiplication\":\n",
    "            correct_responses = [(x[i] * y[i]) % 10**(complexity+1) for i in range(len(x))]\n",
    "        if curr_problem_type==\"division\":\n",
    "            correct_responses = [int(x[i] // y[i]) for i in range(len(x))]\n",
    "        if curr_problem_type==\"modulo\":\n",
    "            correct_responses = [x[i] % y[i] for i in range(len(x))]\n",
    "        if curr_problem_type==\"gcd\":\n",
    "            correct_responses = [np.gcd(x[i], y[i]) for i in range(len(x))]\n",
    "        if curr_problem_type==\"lcm\":\n",
    "            correct_responses = [np.lcm(x[i], y[i]) % 10**(complexity+1) for i in range(len(x))]\n",
    "        if curr_problem_type==\"square_mod\":\n",
    "            correct_responses = [x[i]**2 % y[i] for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_and\":\n",
    "            correct_responses = [x[i] & y[i] for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_xor\":\n",
    "            correct_responses = [x[i] ^ y[i] for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_or\":\n",
    "            correct_responses = [x[i] | y[i] for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_nand\":\n",
    "            correct_responses = [~(x[i] & y[i]) for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_nxor\":\n",
    "            correct_responses = [~(x[i] ^ y[i]) for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_nor\":\n",
    "            correct_responses = [~(x[i] | y[i]) for i in range(len(x))]\n",
    "\n",
    "        # Shape of list_of_probs and list_of_logits is (sequence_output_length, batch_size, num_tokens)\n",
    "        h_stack, list_of_probs, list_of_logits, out_tokens = episode(dialogs=dialogs, generator=generator, temperature=temperature,\n",
    "                                                                     inference_mode=generator.model.forward_symbolic_funnel, \n",
    "                                                                     max_decoding_length=complexity+5, verbose=verbose)\n",
    "\n",
    "        #print(list_of_logits.shape, list_of_probs.shape)\n",
    "\n",
    "        all_logits = all_logits + [list_of_logits]\n",
    "        #all_x      = all_x      + [torch.tensor(x)]\n",
    "        #all_y      = all_y      + [torch.tensor(y)]\n",
    "        all_corr   = all_corr   + [torch.tensor(correct_responses)]\n",
    "\n",
    "        for i in range(len(out_tokens)): # Iterate over n_samples\n",
    "            try:\n",
    "                output = int(generator.tokenizer.decode(out_tokens[i]))\n",
    "                score  = int(output == correct_responses[i])\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(\"!! Can't convert to int !!:\", e)\n",
    "                output = generator.tokenizer.decode(out_tokens[i])\n",
    "                score = 0\n",
    "            total_score += score\n",
    "            if type(problem_type) == list:\n",
    "                scores_per_pt[curr_problem_type] += [score]\n",
    "            outputs += [output]\n",
    "            if verbose == 2 or (verbose and i == 0):\n",
    "                print(\"Actual values:             \",\n",
    "                      \"first number:\", x[i], \"second number:\", y[i], \n",
    "                      curr_problem_type + \":\", correct_responses[i],\n",
    "                      \"LLM response:\", output, \"score:\", score)\n",
    "\n",
    "        response_data += [\"Model Guesses:\", output]\n",
    "        response_data += [\"Correct Answer:\", correct_responses]\n",
    "\n",
    "    max_len = max(t.size(0) for t in all_logits)\n",
    "    padded_tensors = []\n",
    "    for t in all_logits:\n",
    "        T, B, V = t.shape\n",
    "        pad_amount = max_len - T\n",
    "        t_padded = F.pad(t, (0, 0, 0, 0, 0, pad_amount), value=0)\n",
    "        padded_tensors.append(t_padded)\n",
    "\n",
    "    all_logits = torch.cat(padded_tensors, dim=1)\n",
    "    #all_x      = torch.concat(all_x)\n",
    "    #all_y      = torch.concat(all_y)\n",
    "    all_corr   = torch.concat(all_corr)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "    pt_index = 0\n",
    "    for batch in range(len(all_corr)):\n",
    "        correct_tokens = torch.tensor(generator.tokenizer.encode(str(all_corr[batch].item()), bos=False, eos=False))\n",
    "        correct_sequence_length  = len(correct_tokens)\n",
    "        response_sequence_length = all_logits[:,batch,:].shape[0]\n",
    "        sequence_length = min(correct_sequence_length, response_sequence_length)\n",
    "        batch_loss = criterion(all_logits[:sequence_length,batch,:], correct_tokens[:sequence_length])\n",
    "        loss += batch_loss\n",
    "        losses_per_pt[pts[batch//n_samples]] += [batch_loss.detach().cpu().float()]\n",
    "        if log_wandb:\n",
    "            wandb.log({\n",
    "                f\"epoch_{pts[batch//n_samples]}\": len(losses_per_pt[pts[batch//n_samples]])-1,\n",
    "                f\"loss_{pts[batch//n_samples]}\":  losses_per_pt[pts[batch//n_samples]][-1],\n",
    "                f\"score_{pts[batch//n_samples]}\": scores_per_pt[pts[batch//n_samples]][-1],\n",
    "            })\n",
    "\n",
    "    loss = loss / len(all_corr)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    total_score = total_score / len(all_corr)\n",
    "\n",
    "    if verbose:\n",
    "        tn = 0\n",
    "        for p in generator.model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                tn += param_norm.item() ** 2\n",
    "        total_norm = (tn ** 0.5) / len(dialogs) # normalize by batch size\n",
    "        print(f\"Total gradient norm after clipping: {total_norm}\")\n",
    "\n",
    "    return total_loss, total_score, response_data\n",
    "\n",
    "def inference_step(n_samples, generator, temperature=0, problem_type=\"addition\", inference_to_backprop_ratio=1,\n",
    "                   criterion=None, cot=False, complexity=2, verbose=False):\n",
    "\n",
    "    all_logits = []\n",
    "    #all_x     = []\n",
    "    #all_y     = []\n",
    "    all_corr  = []\n",
    "\n",
    "    total_score = 0\n",
    "    outputs = []\n",
    "    pts     = []\n",
    "\n",
    "    for n in range(inference_to_backprop_ratio):\n",
    "        if verbose:\n",
    "            print(\"On sub-epoch iteration:\", n+1)\n",
    "        response_data = []\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        if generator.model.test_on_unrelated_questions:\n",
    "            dialogs, correct_responses, curr_problem_type = generate_non_math_dialog(samples=n_samples, topic=\"random\", cot=cot,\n",
    "                                                                                     string_nums=generator.model.test_with_non_numerical_rep)\n",
    "        else:\n",
    "            dialogs, x, y, curr_problem_type = generate_dialog(complexity=complexity, samples=n_samples, \n",
    "                                                               problem_type=problem_type, cot=cot,\n",
    "                                                               string_nums=generator.model.test_with_non_numerical_rep)\n",
    "            if generator.model.encoder_input_tokens == \"all\":\n",
    "                start_indices, end_indices    = get_dialog_indices(dialogs, calculate_end_index=generator.model.calculate_end_index)\n",
    "                generator.model.curr_start_indices = start_indices\n",
    "                generator.model.curr_end_indices   = end_indices\n",
    "                generator.model.dialogs            = dialogs\n",
    "\n",
    "            if generator.model.calculate_encoding_accuracy:\n",
    "                generator.model.curr_x = x\n",
    "                generator.model.curr_y = y\n",
    "\n",
    "        if generator.model.test_with_non_numerical_rep:\n",
    "            conv = lambda x: w2n.word_to_num(str(x))\n",
    "            conv_inv = lambda x: n2w.num2words(int(x))\n",
    "            conv_inv = lambda x: x\n",
    "\n",
    "        else:\n",
    "            conv = lambda x: x\n",
    "            conv_inv = lambda x: x\n",
    "\n",
    "\n",
    "\n",
    "        pts += [curr_problem_type]\n",
    "        if curr_problem_type==\"addition\":\n",
    "            correct_responses = [conv_inv(conv(x[i]) + conv(y[i])) for i in range(len(x))]\n",
    "        if curr_problem_type==\"multiplication\":\n",
    "            correct_responses = [conv_inv((conv(x[i]) * conv(y[i])) % 10**(complexity+1)) for i in range(len(x))]\n",
    "        if curr_problem_type==\"division\":\n",
    "            correct_responses = [conv_inv(int(conv(x[i]) // conv(y[i]))) for i in range(len(x))]\n",
    "        if curr_problem_type==\"modulo\":\n",
    "            correct_responses = [conv_inv(conv(x[i]) % conv(y[i])) for i in range(len(x))]\n",
    "        if curr_problem_type==\"gcd\":\n",
    "            correct_responses = [conv_inv(np.gcd(conv(x[i]), conv(y[i]))) for i in range(len(x))]\n",
    "        if curr_problem_type==\"lcm\":\n",
    "            correct_responses = [conv_inv(np.lcm(conv(x[i]), conv(y[i])) % 10**(complexity+1)) for i in range(len(x))]\n",
    "        if curr_problem_type==\"square_mod\":\n",
    "            correct_responses = [conv_inv(conv(x[i])**2 % conv(y[i])) for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_and\":\n",
    "            correct_responses = [conv_inv(conv(x[i]) & conv(y[i])) for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_xor\":\n",
    "            correct_responses = [conv_inv(conv(x[i]) ^ conv(y[i])) for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_or\":\n",
    "            correct_responses = [conv_inv(conv(x[i]) | conv(y[i])) for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_nand\":\n",
    "            correct_responses = [conv_inv(~(conv(x[i]) & conv(y[i]))) for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_nxor\":\n",
    "            correct_responses = [conv_inv(~(conv(x[i]) ^ conv(y[i]))) for i in range(len(x))]\n",
    "        if curr_problem_type==\"bitwise_nor\":\n",
    "            correct_responses = [conv_inv(~(conv(x[i]) | conv(y[i]))) for i in range(len(x))]\n",
    "            \n",
    "\n",
    "        # if using cot, set the max decoding length to a large value, otherwise set it to a small value\n",
    "        if cot:\n",
    "            mdl = min(1000, max_seq_len)\n",
    "        else:\n",
    "            mdl = complexity+5\n",
    "\n",
    "        #if verbose:\n",
    "        #    print(\"Max decoding length:\", mdl)\n",
    "\n",
    "        # Shape of list_of_probs and list_of_logits is (sequence_output_length, batch_size, num_tokens)\n",
    "        h_stack, list_of_probs, list_of_logits, out_tokens = episode(generator=generator, dialogs=dialogs, temperature=temperature,\n",
    "                                                                     inference_mode=generator.model.forward_symbolic_funnel, \n",
    "                                                                     max_decoding_length=mdl, verbose=verbose)\n",
    "\n",
    "        if cot and not generator.model.test_on_unrelated_questions:\n",
    "            token_for_final = 19918\n",
    "            bold_token = 334 # Sometimes llama outputs Final Answer in bold markdown (** symbol)\n",
    "            modified_list_of_logits = []\n",
    "            modified_out_tokens     = []\n",
    "            modified_list_of_probs  = []\n",
    "            for i in range(len(out_tokens)):\n",
    "                if token_for_final not in out_tokens[i]: # 19918 is the token for the word \"Final\":\n",
    "                    if verbose == 1 and i == 0:\n",
    "                        print(\"COT response does not contain the phrase 'Final Answer:' as required:\\n\", \n",
    "                              generator.tokenizer.decode(out_tokens[i]))\n",
    "                    elif verbose == 2:\n",
    "                        print(\"COT response does not contain the phrase 'Final Answer:' as required:\\n\", \n",
    "                              generator.tokenizer.decode(out_tokens[i]))\n",
    "                    modified_out_tokens     += [out_tokens[i]]\n",
    "                    modified_list_of_logits += [list_of_logits[:,i,:]]\n",
    "                    modified_list_of_probs  += [list_of_probs [:,i,:]]\n",
    "                else:\n",
    "                    if verbose == 1 and i == 0:\n",
    "                        print(\"COT response:\\n\", \n",
    "                              generator.tokenizer.decode(out_tokens[i]))\n",
    "                    if verbose == 2:\n",
    "                        print(\"COT response:\\n\", \n",
    "                              generator.tokenizer.decode(out_tokens[i]))\n",
    "                    # The phrase \"Final Answer: \" should be 4 tokens long, so we skip that many tokens to get the answer\n",
    "                    if bold_token in out_tokens[i][(out_tokens[i].index(token_for_final) + 4):   ]:\n",
    "                        fp = (out_tokens[i][(out_tokens[i].index(token_for_final) + 4):].index(bold_token) + \n",
    "                              out_tokens[i].index(token_for_final) + 4)\n",
    "                        modified_out_tokens     += [out_tokens    [  i  ][(out_tokens[i].index(token_for_final) + 4):fp   ]]\n",
    "                        modified_list_of_logits += [list_of_logits[:,i,:][(out_tokens[i].index(token_for_final) + 4):fp,:,]]\n",
    "                        modified_list_of_probs  += [list_of_probs [:,i,:][(out_tokens[i].index(token_for_final) + 4):fp,:,]]\n",
    "                    else:\n",
    "                        modified_out_tokens     += [out_tokens    [  i  ][(out_tokens[i].index(token_for_final) + 4):   ]]\n",
    "                        modified_list_of_logits += [list_of_logits[:,i,:][(out_tokens[i].index(token_for_final) + 4):,:,]]\n",
    "                        modified_list_of_probs  += [list_of_probs [:,i,:][(out_tokens[i].index(token_for_final) + 4):,:,]]\n",
    "                    #print(\"Truncated response :\\n\",\n",
    "                    #      generator.tokenizer.decode(modified_out_tokens[i]))\n",
    "\n",
    "                    #print(\"Modified Shapes:\", len(modified_out_tokens[-1]), modified_list_of_logits[-1].shape)\n",
    "\n",
    "            list_of_logits = torch.stack(modified_list_of_logits, axis=1)\n",
    "            list_of_probs  = torch.stack(modified_list_of_probs,  axis=1)\n",
    "            out_tokens     = modified_out_tokens\n",
    "\n",
    "        elif generator.model.test_on_unrelated_questions:\n",
    "            for i in range(len(out_tokens)):\n",
    "                output = generator.tokenizer.decode(out_tokens[i])\n",
    "\n",
    "                if verbose == 1 and i == 0:\n",
    "                    print(\"COT response on non math problems:\\n\", output)\n",
    "                if verbose == 2:\n",
    "                    print(\"COT response on non math problems:\\n\", output)\n",
    "\n",
    "                response_data += [\"Model Guesses:\", output]\n",
    "                response_data += [\"Correct Answer:\", correct_responses[i]]\n",
    "                \n",
    "            return 0, 0, response_data\n",
    "\n",
    "\n",
    "\n",
    "        all_logits = all_logits + [list_of_logits]\n",
    "        #all_x      = all_x      + [torch.tensor(x)]\n",
    "        #all_y      = all_y      + [torch.tensor(y)]\n",
    "        all_corr   = all_corr   + [torch.tensor(correct_responses)]\n",
    "\n",
    "        for i in range(len(out_tokens)): # Iterate over n_samples\n",
    "            try:\n",
    "                output = int(generator.tokenizer.decode(out_tokens[i]))\n",
    "                score  = int(output == correct_responses[i])\n",
    "            except Exception as e:\n",
    "                if verbose == 1 and i == 0:\n",
    "                    print(\"!! Can't convert to int !!:\", e)\n",
    "                elif verbose == 2:\n",
    "                    print(\"!! Can't convert to int !!:\", e)\n",
    "                output = generator.tokenizer.decode(out_tokens[i])\n",
    "                score = 0\n",
    "            total_score += score\n",
    "            outputs += [output]\n",
    "            if verbose == 2 or (verbose and i == 0):\n",
    "                print(\"Actual values:             \",\n",
    "                      \"first number:\", x[i], \"second number:\", y[i], \n",
    "                      curr_problem_type + \":\", correct_responses[i],\n",
    "                      \"LLM response:\", output, \"score:\", score)\n",
    "\n",
    "            response_data += [\"Model Guesses:\", output]\n",
    "            response_data += [\"Correct Answer:\", correct_responses[i]]\n",
    "\n",
    "    max_len = max(t.size(0) for t in all_logits)\n",
    "    padded_tensors = []\n",
    "    for t in all_logits:\n",
    "        T, B, V = t.shape\n",
    "        pad_amount = max_len - T\n",
    "        t_padded = F.pad(t, (0, 0, 0, 0, 0, pad_amount), value=0)\n",
    "        padded_tensors.append(t_padded)\n",
    "\n",
    "    all_logits = torch.cat(padded_tensors, dim=1)\n",
    "    #all_x      = torch.concat(all_x)\n",
    "    #all_y      = torch.concat(all_y)\n",
    "    all_corr   = torch.concat(all_corr)\n",
    "\n",
    "    loss = 0\n",
    "    for batch in range(len(all_corr)):\n",
    "        correct_tokens = torch.tensor(generator.tokenizer.encode(str(all_corr[batch].item()), bos=False, eos=False))\n",
    "        correct_sequence_length  = len(correct_tokens)\n",
    "        response_sequence_length = all_logits[:,batch,:].shape[0]\n",
    "        sequence_length = min(correct_sequence_length, response_sequence_length)\n",
    "        batch_loss = criterion(all_logits[:sequence_length,batch,:], correct_tokens[:sequence_length])\n",
    "        loss += batch_loss\n",
    "    loss = loss / len(all_corr)\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    total_score = total_score / len(all_corr)\n",
    "\n",
    "    return total_loss, total_score, response_data\n",
    "\n",
    "def evaluate_model(testing_n_samples, testing_num_epochs, testing_temperature, problem_type, generator, criterion=None, \n",
    "                   inference_to_backprop_ratio=1, complexity=2, cot=False, testing_epochs_to_print=10, testing_verbose=0):\n",
    "    losses    = []\n",
    "    scores    = []\n",
    "    responses = []\n",
    "    with torch.no_grad():\n",
    "        for epoch in range(testing_num_epochs):\n",
    "            loss, score, response_data = inference_step(n_samples=testing_n_samples, generator=generator,\n",
    "                                                        temperature=testing_temperature, problem_type=problem_type, \n",
    "                                                        inference_to_backprop_ratio=inference_to_backprop_ratio, \n",
    "                                                        criterion=criterion, cot=cot, complexity=complexity, \n",
    "                                                        verbose=testing_verbose)\n",
    "            losses += [loss]\n",
    "            scores += [score]\n",
    "            responses += [response_data]\n",
    "            if testing_epochs_to_print and testing_num_epochs // testing_epochs_to_print and not epoch % (testing_num_epochs // testing_epochs_to_print):\n",
    "                #print(f\" -------------- Epoch {epoch}, Loss: {np.mean(losses)}, Score: {np.mean(scores)}  -------------- \")\n",
    "                print(f\" -------------- Epoch {epoch}, Loss: {loss}, Score: {score}  -------------- \")\n",
    "    return losses, scores, responses\n",
    "\n",
    "\n",
    "def plot_results(losses, scores, problem_type, bypass_symbolic):\n",
    "    losses = np.array(losses)\n",
    "    scores = np.array(scores)\n",
    "\n",
    "    if bypass_symbolic == 1:\n",
    "        output_text = f\"Mean score and loss of standard LLM on {problem_type}: \" + str(round(scores.mean()*100, 3)) + \" ± \" + str(round(scores.std()*100, 4)) + \", \" + str(round(losses.mean(), 3)) + \" ± \" + str(round(losses.std(), 4))\n",
    "    else:\n",
    "        output_text = f\"Mean score and loss of symbolic LLM on {problem_type}: \" + str(round(scores.mean()*100, 3)) + \" ± \" + str(round(scores.std()*100, 4)) + \", \" + str(round(losses.mean(), 3)) + \" ± \" + str(round(losses.std(), 4))\n",
    "    print(f\"\\n\", output_text)\n",
    "    return output_text\n",
    "\n",
    "def create_smooth_data(curve, n=3, starting_index=10):\n",
    "    data = curve[starting_index:]\n",
    "    smoothed_list = []\n",
    "    length = len(data)\n",
    "    for i in range(length):\n",
    "        # Determine the range of indices to average\n",
    "        start = max(0, i - n)\n",
    "        end = min(length, i + n + 1)\n",
    "        # Calculate the average of the surrounding elements\n",
    "        smoothed_list.append(sum(data[start:end]) / (end - start))\n",
    "    return smoothed_list\n",
    "\n",
    "def run_experiment(generator, config):\n",
    "    encoder_path                        = config['encoder_path']\n",
    "    decoder_path                        = config['decoder_path']\n",
    "    save_model                          = config['save_model']\n",
    "    problem_type                        = config['problem_type']\n",
    "    complexity                          = config['complexity']\n",
    "    temperature                         = config['temperature']\n",
    "\n",
    "    train_model                         = config['train_model']\n",
    "    lora_baseline                       = config['lora_baseline']\n",
    "    starting_skip_strength              = config['starting_skip_strength']\n",
    "    problem_score_threshold             = config['problem_score_threshold']\n",
    "    normalize_VSA_before_dot            = config['normalize_VSA_before_dot']\n",
    "    rms_layer                           = config['rms_layer']\n",
    "    double_rep                          = config['double_rep']\n",
    "    use_specific_identities             = config['use_specific_identities']\n",
    "    initialize_decoders                 = config['initialize_decoders']\n",
    "    normalize_vector                    = config['normalize_vector']\n",
    "    symbolic_encoding_layer             = config['symbolic_encoding_layer']\n",
    "    symbolic_decoding_layers            = config['symbolic_decoding_layers']\n",
    "\n",
    "    num_epochs                          = config['num_epochs']\n",
    "    n_samples                           = config['n_samples']\n",
    "    inference_to_backprop_ratio         = config['inference_to_backprop_ratio']\n",
    "    trainable_skip                      = config['trainable_skip']\n",
    "    learning_rate                       = config['learning_rate']\n",
    "    learning_rate_reduction_factors     = config['learning_rate_reduction_factors']\n",
    "    epochs_to_print                     = config['epochs_to_print']\n",
    "    print_all_pts_freq                  = config['print_all_pts_freq']\n",
    "    verbose                             = config['verbose']\n",
    "\n",
    "    testing_problems                    = config['testing_problems']\n",
    "    testing_num_epochs                  = config['testing_num_epochs']\n",
    "    testing_inference_to_backprop_ratio = config['testing_inference_to_backprop_ratio']\n",
    "    testing_n_samples                   = config['testing_n_samples']\n",
    "    testing_temperature                 = config['testing_temperature']\n",
    "    testing_epochs_to_print             = config['testing_epochs_to_print']\n",
    "    testing_verbose                     = config['testing_verbose']\n",
    "    record_score_per_problem            = config['record_score_per_problem']\n",
    "    test_baseline                       = config['test_baseline']\n",
    "    cot                                 = config['cot']\n",
    "    \n",
    "    test_on_unrelated_questions         = config['test_on_unrelated_questions']\n",
    "    test_with_non_numerical_rep         = config['test_with_non_numerical_rep']\n",
    "\n",
    "    encoder_input_tokens                = config['encoder_input_tokens']\n",
    "    calculate_end_index                 = config['calculate_end_index']\n",
    "    \n",
    "    multi_token_intervention            = config[\"multi_token_intervention\"]\n",
    "    static_encoding                     = config[\"static_encoding\"]\n",
    "    calculate_encoding_accuracy         = config[\"calculate_encoding_accuracy\"]\n",
    "    encode_counter                      = config[\"encode_counter\"]\n",
    "\n",
    "    #######################################################################################\n",
    "    ############################## Hyperparameter Definition ##############################\n",
    "    #######################################################################################\n",
    "\n",
    "    if \"post_fine_tuning\" in decoder_path:\n",
    "        initialize_decoders = False\n",
    "    \n",
    "    if test_baseline == 2:\n",
    "        train_model = False\n",
    "\n",
    "    if cot == True:\n",
    "        test_baseline = 2\n",
    "        train_model = False\n",
    "\n",
    "    if test_on_unrelated_questions == True:\n",
    "        cot = True\n",
    "        testing_problems = ['philosophy', 'ethics', 'history', 'psychology', 'science_fiction', 'technology', 'art_and_culture']\n",
    "        test_baseline = 0\n",
    "        train_model = False\n",
    "\n",
    "\n",
    "    if type(problem_type) == list:\n",
    "        losses_per_pt = {pt: [] for pt in problem_type}\n",
    "        scores_per_pt = {pt: [] for pt in problem_type}\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if log_wandb:\n",
    "        save_path  = decoder_path.split(\"/\")[-1].split(\".pth\")[0] + f\"_post_fine_tuning_{wandb.run.id}_\"\n",
    "    else:\n",
    "        save_path  = decoder_path.split(\"/\")[-1].split(\".pth\")[0] + f\"_post_fine_tuning_\"\n",
    "\n",
    "    resume    = False\n",
    "    \n",
    "\n",
    "    #################################################################################\n",
    "    ############################## Model preprocessing ##############################\n",
    "    #################################################################################\n",
    "    \n",
    "\n",
    "    generator.model.encoders = torch.load(encoder_path, weights_only=False)\n",
    "    generator.model.decoders = torch.load(decoder_path, weights_only=False)\n",
    "\n",
    "    if lora_baseline:\n",
    "        lora_encoders = nn.ModuleList()\n",
    "        lora_decoders = nn.ModuleList()\n",
    "        for layer_id in torch.stack([generator.model.encoders[i].layer_id for i in range(len(generator.model.encoders))]):\n",
    "            # Assume that linear encoder and decoder networks are used\n",
    "            lora_encoder = Encoder(layer_id, generator.model.output.weight.shape[1], generator.model.SE.VSA_dim).to(device)\n",
    "            lora_decoder = Decoder(layer_id, generator.model.SE.VSA_dim, generator.model.output.weight.shape[1]).to(device)\n",
    "            lora_encoders.append(lora_encoder)\n",
    "            lora_decoders.append(lora_decoder)\n",
    "\n",
    "        generator.model.encoders = lora_encoders\n",
    "        generator.model.decoders = lora_decoders\n",
    "\n",
    "        initialize_decoders = False\n",
    "        rms_layer = True\n",
    "\n",
    "        \n",
    "    generator.model.bypass_symbolic             = False\n",
    "    generator.model.add_noise                   = False\n",
    "\n",
    "    generator.model.curr_dir                    = curr_dir\n",
    "    generator.model.problem_type                = problem_type\n",
    "    generator.model.symbolic_encoding_layer     = symbolic_encoding_layer\n",
    "    generator.model.symbolic_decoding_layers    = symbolic_decoding_layers\n",
    "    generator.model.normalize_vector            = normalize_vector\n",
    "    generator.model.rms_layer                   = rms_layer\n",
    "    generator.model.double_rep                  = double_rep\n",
    "    generator.model.complexity                  = complexity\n",
    "        \n",
    "    generator.model.lora_baseline               = lora_baseline\n",
    "\n",
    "    generator.model.problem_score_threshold     = problem_score_threshold\n",
    "    generator.model.training_problems           = problem_type\n",
    "    generator.model.record_score_per_problem    = record_score_per_problem\n",
    "    generator.model.normalize_VSA_before_dot    = normalize_VSA_before_dot\n",
    "    generator.model.use_specific_identities     = use_specific_identities\n",
    "    generator.model.test_on_unrelated_questions = test_on_unrelated_questions\n",
    "    generator.model.test_with_non_numerical_rep = test_with_non_numerical_rep\n",
    "    \n",
    "    generator.model.encoder_input_tokens        = encoder_input_tokens\n",
    "    generator.model.calculate_end_index         = calculate_end_index\n",
    "\n",
    "    generator.model.multi_token_intervention    = multi_token_intervention\n",
    "    generator.model.static_encoding             = static_encoding\n",
    "    generator.model.calculate_encoding_accuracy = calculate_encoding_accuracy\n",
    "    generator.model.encode_counter              = encode_counter\n",
    "\n",
    "    if generator.model.calculate_encoding_accuracy:\n",
    "        # During training, calculate accuracy per problem type, per digit, per input number\n",
    "        generator.model.encoding_accuracy = {}\n",
    "        for pt in problem_type:\n",
    "            generator.model.encoding_accuracy[pt] = {}\n",
    "            for digit in range(complexity + 1):\n",
    "                generator.model.encoding_accuracy[pt][\"digit \" + str(digit)] = {}\n",
    "                generator.model.encoding_accuracy[pt][\"digit \" + str(digit)][\"first_number\"]  = []\n",
    "                generator.model.encoding_accuracy[pt][\"digit \" + str(digit)][\"second_number\"] = []\n",
    "    \n",
    "\n",
    "    starting_encoder_layer = 0\n",
    "    for i in range(len(generator.model.encoders)):\n",
    "        if (generator.model.encoders[i]) != type(None):\n",
    "            starting_encoder_layer = i\n",
    "            break\n",
    "\n",
    "    starting_decoder_layer = 0\n",
    "    for i in range(len(generator.model.decoders)):\n",
    "        if (generator.model.decoders[i]) != type(None):\n",
    "            starting_decoder_layer = i\n",
    "            break\n",
    "\n",
    "    generator.model.starting_encoder_layer = starting_encoder_layer\n",
    "    generator.model.starting_decoder_layer = starting_decoder_layer\n",
    "\n",
    "    generator.model.encoders.eval()\n",
    "\n",
    "    if rms_layer:\n",
    "        generator.model.rms_layers = [] \n",
    "        for sl in symbolic_decoding_layers:\n",
    "            if sl != 33:\n",
    "                generator.model.rms_layers.append(RMSNorm(generator.model.output.weight.shape[1], eps=1e-05)) # params.dim, eps=params.norm_eps\n",
    "            else:\n",
    "                generator.model.rms_layers.append(RMSNorm(generator.model.output.weight.shape[0], eps=1e-05)) # num_tokens, eps=params.norm_eps\n",
    "    else:\n",
    "        generator.model.skip_weights = nn.Parameter(torch.zeros(len(symbolic_decoding_layers)) + starting_skip_strength)\n",
    "\n",
    "    if initialize_decoders:\n",
    "        pseudo_inverses = {}\n",
    "        for sl in symbolic_decoding_layers:\n",
    "            for p in generator.model.encoders[sl-generator.model.starting_decoder_layer].parameters():\n",
    "                pseudo_inverses[sl-generator.model.starting_decoder_layer] = torch.linalg.pinv(p.float()).to(torch.bfloat16)\n",
    "\n",
    "        for sl in symbolic_decoding_layers:\n",
    "            for p in generator.model.decoders[sl-generator.model.starting_decoder_layer].parameters():\n",
    "                p = pseudo_inverses[sl-generator.model.starting_decoder_layer]\n",
    "\n",
    "    if 33 in symbolic_decoding_layers:\n",
    "        generator.model.decoders.append(ColumnParallelLinear(\n",
    "            generator.model.SE.VSA_dim, generator.model.output.weight.shape[0], bias=False, init_method=lambda x: x\n",
    "        ))\n",
    "        print(\"Created 33rd decoder network\")\n",
    "\n",
    "    # Delete unnecessary layers to save memory\n",
    "    for i in range(len(generator.model.decoders)):\n",
    "        if i not in symbolic_decoding_layers:\n",
    "            del generator.model.decoders[i]  # Delete layer\n",
    "            generator.model.decoders.insert(i, None)  # Insert None to maintain indexing\n",
    "\n",
    "    for i in range(len(generator.model.encoders)):\n",
    "        if i != symbolic_encoding_layer:\n",
    "            del generator.model.encoders[i]  # Delete layer\n",
    "            generator.model.encoders.insert(i, None)  # Insert None to maintain indexing\n",
    "\n",
    "\n",
    "\n",
    "    if record_score_per_problem == 1:\n",
    "        with open(f\"{curr_dir}/outputs/score_per_problem_training_and_testing.txt\", \"w\") as file:\n",
    "            file.write(\"actual_problem_type,problem_type,score\\n\")\n",
    "\n",
    "\n",
    "    if train_model:\n",
    "        #############################################################################\n",
    "        ############################## Train the model ##############################\n",
    "        #############################################################################\n",
    "\n",
    "        if not resume:\n",
    "            losses = []\n",
    "            scores = []\n",
    "            responses = []\n",
    "\n",
    "            for param in generator.model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for sl in symbolic_decoding_layers:\n",
    "                for param in generator.model.decoders[sl-generator.model.starting_decoder_layer].parameters():\n",
    "                    param.requires_grad = True\n",
    "            if lora_baseline:\n",
    "                for param in generator.model.encoders[symbolic_encoding_layer-generator.model.starting_encoder_layer].parameters():\n",
    "                    param.requires_grad=True\n",
    "            if rms_layer:\n",
    "                for r_layer in generator.model.rms_layers:\n",
    "                    for param in r_layer.parameters():\n",
    "                        param.requires_grad = True\n",
    "\n",
    "\n",
    "            if not rms_layer:\n",
    "                generator.model.skip_weights.requires_grad = trainable_skip\n",
    "\n",
    "            #original_weights = {}\n",
    "            #for name, param in generator.model.named_parameters():\n",
    "            #    for sl in symbolic_decoding_layers:\n",
    "            #        if f\"decoders.{sl-generator.model.starting_decoder_layer}.decoder_layer\" in name  or name == \"layers.0.feed_forward.w1.weight\":\n",
    "            #            original_weights[name] = param.clone().detach()\n",
    "\n",
    "\n",
    "\n",
    "            # Training loop\n",
    "\n",
    "            params = list(filter(lambda p: p.requires_grad, generator.model.parameters()))\n",
    "            print(\"Number of trainable parameters:\", sum(p.numel() for p in params))\n",
    "        else:\n",
    "            print(\"Resuming training\")\n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer = optim.Adam(filter(lambda p: p.requires_grad, generator.model.parameters()), lr=learning_rate)\n",
    "            torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "            generator.model.train()\n",
    "            if epoch in learning_rate_reduction_factors.keys():\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = param_group['lr'] * learning_rate_reduction_factors[epoch]  # Set new learning rate\n",
    "                    print(\"Learning Rate changed to:\", param_group['lr'])\n",
    "\n",
    "            loss, score, response_data = training_step(n_samples, generator, temperature=temperature, problem_type=problem_type, \n",
    "                                                       inference_to_backprop_ratio=inference_to_backprop_ratio, optimizer=optimizer, criterion=criterion,\n",
    "                                                       complexity=complexity, losses_per_pt=losses_per_pt, scores_per_pt=scores_per_pt, verbose=verbose)\n",
    "\n",
    "            losses += [loss]\n",
    "            scores += [score]\n",
    "            responses += [response_data]\n",
    "\n",
    "            if log_wandb:\n",
    "                wandb.log({\n",
    "                    \"epoch\": epoch,\n",
    "                    \"loss\":  loss,\n",
    "                    \"score\": score\n",
    "                })\n",
    "\n",
    "            for n, sw in enumerate(generator.model.skip_weights.detach().cpu().float().numpy()):\n",
    "                if log_wandb:\n",
    "                    wandb.log({f\"skip_weights_{n}\": sw})\n",
    "\n",
    "            if epochs_to_print and num_epochs // epochs_to_print and not epoch % (num_epochs // epochs_to_print):\n",
    "                if num_epochs // epochs_to_print >= 10:\n",
    "                    if not rms_layer and trainable_skip:\n",
    "                        print(f\" -------------- Epoch {epoch}, Loss: {np.mean(losses)}, Score: {np.mean(scores)}, Skip Weight: {generator.model.skip_weights.detach().cpu().float().numpy()}\"\"  -------------- \", flush=True)\n",
    "                    else:\n",
    "                        print(f\" -------------- Epoch {epoch}, Loss: {np.mean(losses)}, Score: {np.mean(scores)}  -------------- \", flush=True)\n",
    "                else:\n",
    "                    if not rms_layer and trainable_skip:\n",
    "                        print(f\" -------------- Epoch {epoch}, Loss: {loss}, Score: {score}, Skip Weight: {generator.model.skip_weights.detach().cpu().float().numpy()}\"\"  -------------- \", flush=True)\n",
    "                    else:\n",
    "                        print(f\" -------------- Epoch {epoch}, Loss: {loss}, Score: {score}  -------------- \", flush=True)\n",
    "\n",
    "            if epoch and not epoch % print_all_pts_freq:\n",
    "                print(\"~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\")\n",
    "                for pt in losses_per_pt:\n",
    "                    if len(losses_per_pt[pt]):\n",
    "                        print(\"    Problem type:\", pt)\n",
    "                        print(\"    Loss:\", np.mean(losses_per_pt[pt]), \", Score:\", np.mean(scores_per_pt[pt]))\n",
    "\n",
    "\n",
    "\n",
    "        #wandb.log({\"losses\": losses})\n",
    "        #wandb.log({\"scores\": scores})\n",
    "        #wandb.log({\"losses_per_pt\": losses_per_pt})\n",
    "        #wandb.log({\"scores_per_pt\": scores_per_pt})\n",
    "\n",
    "        ###################################################################################\n",
    "        ############################## Plot Training Metrics ##############################\n",
    "        ###################################################################################\n",
    "\n",
    "        if save_model:\n",
    "            # Add rms and skip connection paramters to decoder to be saved\n",
    "            if rms_layer:\n",
    "                generator.model.decoders.rms_layer   = nn.ModuleList(generator.model.rms_layers)\n",
    "            if trainable_skip:\n",
    "                generator.model.decoders.skip_weight = generator.model.skip_weights\n",
    "                \n",
    "            current_datetime = datetime.datetime.now()\n",
    "            formatted_string = current_datetime.strftime(\"%Y_%m_%d\")\n",
    "            torch.save(generator.model.decoders, f\"{curr_dir}/models/\" + save_path.split(\".pth\")[0] + f\"{formatted_string}\" + \".pth\")\n",
    "            print(\"Saved\", f\"{curr_dir}/models/\" + save_path.split(\".pth\")[0] + f\"{formatted_string}\" + \".pth\")\n",
    "\n",
    "        if not rms_layer and trainable_skip:\n",
    "\n",
    "            print(\"Skip Weight strength after training:\", generator.model.skip_weights.detach().cpu().float().numpy())\n",
    "\n",
    "        plt.plot(create_smooth_data(np.array(scores), n=(epoch+1)//10))\n",
    "        #plt.title(f\"Score vs Epoch (smoothing factor = {(epoch+1) // 10})\")\n",
    "        plt.title(f\"Score vs Epoch\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        if verbose:\n",
    "            plt.show()\n",
    "        if log_wandb:\n",
    "            wandb.log({\"score_vs_epoch\": wandb.Image(plt)})  # Log to wandb\n",
    "        plt.close()\n",
    "\n",
    "        plt.plot(create_smooth_data(np.array(losses), n=(epoch+1)//10))\n",
    "        #plt.title(f\"Loss vs Epoch (smoothing factor = {(epoch+1) // 10})\")\n",
    "        plt.title(f\"Loss vs Epoch\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        if verbose:\n",
    "            plt.show()\n",
    "        if log_wandb:\n",
    "            wandb.log({\"loss_vs_epoch\": wandb.Image(plt)})  # Log to wandb\n",
    "        plt.close()\n",
    "\n",
    "        if type(problem_type) == list:\n",
    "            for pt in problem_type:\n",
    "                print(\"On problem type:\", pt)\n",
    "                pt_scores = np.array(scores_per_pt[pt])\n",
    "                pt_losses = np.array(losses_per_pt[pt])\n",
    "\n",
    "                plt.plot(create_smooth_data(pt_scores, n=len(pt_scores)//10))\n",
    "                #plt.title(f\"{pt} Score vs Epoch (smoothing factor = {len(pt_scores) // 10})\")\n",
    "                plt.title(f\"{pt} Score vs Epoch\")\n",
    "                plt.xlabel(\"Epochs\")\n",
    "                plt.ylabel(\"Score\")\n",
    "                if verbose:\n",
    "                    plt.show()\n",
    "                if log_wandb:\n",
    "                    wandb.log({f\"{pt}_score_vs_epoch\": wandb.Image(plt)})  # Log to wandb\n",
    "                plt.close()\n",
    "\n",
    "                plt.plot(create_smooth_data(pt_losses, n=len(pt_losses)//10))\n",
    "                #plt.title(f\"{pt} Loss vs Epoch (smoothing factor = {len(pt_losses) // 10})\")\n",
    "                plt.title(f\"{pt} Loss vs Epoch\")\n",
    "                plt.xlabel(\"Epochs\")\n",
    "                plt.ylabel(\"Loss\")\n",
    "                if verbose:\n",
    "                    plt.show()\n",
    "                if log_wandb:\n",
    "                    wandb.log({f\"{pt}_loss_vs_epoch\": wandb.Image(plt)})  # Log to wandb\n",
    "                plt.close()\n",
    "\n",
    "                print(\"Final Score and Loss:\", np.mean(pt_scores[:-10]), np.mean(pt_losses[:-10]))\n",
    "                if log_wandb:\n",
    "                    wandb.log({f\"final_score_{pt}\": np.mean(pt_scores[:-10])})  # Log to wandb\n",
    "                    wandb.log({f\"final_loss_{pt}\" : np.mean(pt_losses[:-10])})  # Log to wandb\n",
    "\n",
    "    #####################################################################\n",
    "    ############################## Testing ##############################\n",
    "    #####################################################################\n",
    "\n",
    "    #if type(problem_type) == str:\n",
    "    #    testing_problems = [problem_type]\n",
    "    #else:\n",
    "    #    testing_problems = problem_type\n",
    "    \n",
    "    if record_score_per_problem == 2:\n",
    "        with open(f\"{curr_dir}/outputs/score_per_problem.txt\", \"w\") as file:\n",
    "            file.write(\"actual_problem_type,problem_type,score\\n\")\n",
    "\n",
    "    testing_losses_per_pt_SYM = {}\n",
    "    testing_losses_per_pt_LLM = {}\n",
    "    testing_scores_per_pt_SYM = {}\n",
    "    testing_scores_per_pt_LLM = {}\n",
    "    \n",
    "\n",
    "\n",
    "    for pt in testing_problems:\n",
    "        print(\"~~~~~~~~ Problem Type:\", pt, \"~~~~~~~~\")\n",
    "        if test_baseline != 2:\n",
    "            # Symbolic LLM\n",
    "            generator.model.bypass_symbolic = False\n",
    "            generator.model.add_noise       = False\n",
    "            losses, scores, responses  = evaluate_model(testing_n_samples=testing_n_samples,\n",
    "                                                        testing_num_epochs=testing_num_epochs,\n",
    "                                                        testing_temperature=testing_temperature,\n",
    "                                                        problem_type=pt, generator=generator, criterion=criterion,\n",
    "                                                        inference_to_backprop_ratio=testing_inference_to_backprop_ratio,\n",
    "                                                        complexity=complexity, cot=cot,\n",
    "                                                        testing_epochs_to_print=testing_epochs_to_print, testing_verbose=testing_verbose)\n",
    "\n",
    "            testing_losses_per_pt_SYM[pt] = losses\n",
    "            testing_scores_per_pt_SYM[pt] = scores\n",
    "\n",
    "            symbolic_output_text = plot_results(losses, scores, pt, generator.model.bypass_symbolic)\n",
    "            if log_wandb:\n",
    "                wandb.log({f\"testing_losses_per_pt_SYM_{pt}\": testing_losses_per_pt_SYM[pt]})\n",
    "                wandb.log({f\"average_testing_loss_SYM_{pt}\": np.mean(testing_losses_per_pt_SYM[pt])})\n",
    "                wandb.log({f\"testing_scores_per_pt_SYM_{pt}\": testing_scores_per_pt_SYM[pt]})\n",
    "                wandb.log({f\"average_testing_score_SYM_{pt}\": np.mean(testing_scores_per_pt_SYM[pt])})\n",
    "                wandb.log({f\"symbolic_output_text_{pt}\": symbolic_output_text})\n",
    "\n",
    "        if test_baseline != 0:\n",
    "            # Standard LLM\n",
    "            generator.model.bypass_symbolic = True\n",
    "            generator.model.add_noise       = False\n",
    "            losses, scores, responses  = evaluate_model(testing_n_samples=testing_n_samples,\n",
    "                                                        testing_num_epochs=testing_num_epochs,\n",
    "                                                        testing_temperature=testing_temperature,\n",
    "                                                        problem_type=pt, generator=generator, criterion=criterion,\n",
    "                                                        inference_to_backprop_ratio=testing_inference_to_backprop_ratio,\n",
    "                                                        complexity=complexity, cot=cot,\n",
    "                                                        testing_epochs_to_print=testing_epochs_to_print, testing_verbose=testing_verbose)\n",
    "            testing_losses_per_pt_LLM[pt] = losses\n",
    "            testing_scores_per_pt_LLM[pt] = scores\n",
    "\n",
    "            standard_output_text = plot_results(losses, scores, pt, generator.model.bypass_symbolic)\n",
    "            if log_wandb:\n",
    "                wandb.log({f\"testing_losses_per_pt_LLM_{pt}\": testing_losses_per_pt_LLM[pt]})\n",
    "                wandb.log({f\"average_testing_loss_LLM_{pt}\": np.mean(testing_losses_per_pt_LLM[pt])})\n",
    "                wandb.log({f\"testing_scores_per_pt_LLM_{pt}\": testing_scores_per_pt_LLM[pt]})\n",
    "                wandb.log({f\"average_testing_score_LLM_{pt}\": np.mean(testing_scores_per_pt_LLM[pt])})\n",
    "                wandb.log({f\"standard_output_text_{pt}\": standard_output_text})\n",
    "\n",
    "\n",
    "    for pt in testing_problems:\n",
    "        if testing_verbose:\n",
    "            print(\"~~~~~~~~ Problem Type:\", pt, \"~~~~~~~~\")\n",
    "        if test_baseline != 2:\n",
    "            plt.hist(testing_losses_per_pt_SYM[pt], bins=75)\n",
    "            plt.xlabel(\"Loss\")\n",
    "            plt.title(f\"{pt} Symbolic Loss Histogram\")\n",
    "            if testing_verbose:\n",
    "                plt.show()\n",
    "            if log_wandb:\n",
    "                wandb.log({f\"{pt}_symbolic_loss_histogram\": wandb.Image(plt)})  # Log to wandb\n",
    "            plt.close()\n",
    "\n",
    "        if test_baseline != 0:\n",
    "            plt.hist(testing_losses_per_pt_LLM[pt], bins=75)\n",
    "            plt.xlabel(\"Loss\")\n",
    "            plt.title(f\"{pt} Standard Loss Histogram\")\n",
    "            if testing_verbose:\n",
    "                plt.show()\n",
    "            if log_wandb:\n",
    "                wandb.log({f\"{pt}_standard_loss_histogram\": wandb.Image(plt)})  # Log to wandb\n",
    "            plt.close()\n",
    "\n",
    "    if record_score_per_problem and test_baseline != 2 and not lora_baseline:\n",
    "        df = pd.read_csv(f\"{curr_dir}/outputs/score_per_problem.txt\")\n",
    "        df['training_item'] = [i for i in range(len(df) // len(generator.model.training_problems)) \n",
    "                                 for j in range(len(generator.model.training_problems))]\n",
    "\n",
    "        untrained_pts = sorted(list(set(config['testing_problems']) - set(generator.model.training_problems)))\n",
    "        trained_pts   = generator.model.training_problems\n",
    "        bins = 100\n",
    "        leg = []\n",
    "        for pt in untrained_pts:\n",
    "            if len(df[df.actual_problem_type.isin([pt])]) == 0:\n",
    "                continue\n",
    "            leg += [pt]\n",
    "            plt.hist(df[df.actual_problem_type.isin([pt])].pivot_table(\n",
    "                index=\"training_item\", values=\"score\", aggfunc=np.max).score, bins=bins, histtype=\"step\")\n",
    "        for pt in trained_pts:\n",
    "            if len(df[df.actual_problem_type.isin([pt])]) == 0:\n",
    "                continue\n",
    "            leg += [pt]\n",
    "            plt.hist(df[df.actual_problem_type.isin([pt])].  pivot_table(\n",
    "                index=\"training_item\", values=\"score\", aggfunc=np.max).score, bins=bins, histtype=\"step\")\n",
    "        plt.xlabel(\"Dot product similarity\")\n",
    "        plt.ylabel(\"Number of Samples\")\n",
    "        plt.legend(leg)\n",
    "        if testing_verbose:\n",
    "            plt.show()\n",
    "        if log_wandb:\n",
    "            wandb.log({f\"problem_type_similarity_histogram_per_problem_type\": wandb.Image(plt)})  # Log to wandb\n",
    "        plt.close()\n",
    "\n",
    "        bins = 100\n",
    "        if df[df.actual_problem_type.isin(untrained_pts)].shape[0]:\n",
    "            plt.hist(df[df.actual_problem_type.isin(untrained_pts)].pivot_table(\n",
    "                index=\"training_item\", values=\"score\", aggfunc=np.max).score, bins=bins, histtype=\"step\")\n",
    "        plt.hist(df[df.actual_problem_type.isin(trained_pts)].  pivot_table(\n",
    "            index=\"training_item\", values=\"score\", aggfunc=np.max).score, bins=bins, histtype=\"step\")\n",
    "        plt.xlabel(\"Dot product similarity\")\n",
    "        plt.ylabel(\"Number of Samples\")\n",
    "        plt.legend([\"Problems not seen during training\", \"Problems seen during training\"], loc=\"upper left\")\n",
    "        if testing_verbose:\n",
    "            plt.show()\n",
    "        if log_wandb:\n",
    "            wandb.log({f\"problem_type_similarity_histogram\": wandb.Image(plt)})  # Log to wandb\n",
    "        plt.close()\n",
    "\n",
    "        if df[df.actual_problem_type.isin(untrained_pts)].shape[0]:\n",
    "            if log_wandb:\n",
    "                wandb.log({\"untrained_problem_scores\": df[df.actual_problem_type.isin(untrained_pts)].pivot_table(\n",
    "                               index=\"training_item\", values=\"score\", aggfunc=np.max).score.tolist(),\n",
    "                           \"trained_problem_scores\":   df[df.actual_problem_type.isin(trained_pts)]  .pivot_table(\n",
    "                               index=\"training_item\", values=\"score\", aggfunc=np.max).score.tolist()})\n",
    "        else:\n",
    "            if log_wandb:\n",
    "                wandb.log({\"trained_problem_scores\":   df[df.actual_problem_type.isin(trained_pts)]  .pivot_table(\n",
    "                               index=\"training_item\", values=\"score\", aggfunc=np.max).score.tolist()})\n",
    "\n",
    "        result = (\n",
    "            df[df.actual_problem_type.isin(trained_pts)].groupby(\"training_item\")\n",
    "            .apply(lambda group: group.loc[group['score'].idxmax()])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        if testing_verbose:\n",
    "            print(\"Number of trained problems with different actual problem types and maximum score identified problem types:\", \n",
    "                  sum(result['actual_problem_type'] != result['problem_type']))\n",
    "        if log_wandb:\n",
    "            wandb.log({\"different_actual_problem_types_count\": sum(result['actual_problem_type'] != result['problem_type'])})\n",
    "\n",
    "    if generator.model.calculate_encoding_accuracy:\n",
    "        average_accuracy_per_pt = {}\n",
    "\n",
    "        problem_types = list(generator.model.encoding_accuracy.keys())\n",
    "        digits = list(generator.model.encoding_accuracy[problem_types[0]].keys())\n",
    "        \n",
    "        minval = 100\n",
    "        maxval = 0\n",
    "\n",
    "        for pt in problem_types:\n",
    "            average_accuracy_per_pt[pt] = {}\n",
    "            for d in digits:\n",
    "                average_accuracy_per_pt[pt][d] = [0, 0]\n",
    "                average_accuracy_per_pt[pt][d][0] = np.mean(generator.model.encoding_accuracy[pt][d][\"first_number\"])*100\n",
    "                average_accuracy_per_pt[pt][d][1] = np.mean(generator.model.encoding_accuracy[pt][d][\"second_number\"])*100\n",
    "                \n",
    "                curr_min = min(np.mean(generator.model.encoding_accuracy[pt][d][\"first_number\"])*100,\n",
    "                               np.mean(generator.model.encoding_accuracy[pt][d][\"second_number\"])*100)\n",
    "                curr_max = max(np.mean(generator.model.encoding_accuracy[pt][d][\"first_number\"])*100,\n",
    "                               np.mean(generator.model.encoding_accuracy[pt][d][\"second_number\"])*100)\n",
    "\n",
    "                if minval > curr_min:\n",
    "                    minval = curr_min\n",
    "                if maxval < curr_max:\n",
    "                    maxval = curr_max\n",
    "                    \n",
    "        minval = minval // 5 * 5\n",
    "        maxval = min(100, (maxval // 5 + 1) * 5)\n",
    "\n",
    "        # Define colors for first and second number\n",
    "        colors = ['blue', 'orange']\n",
    "\n",
    "        # Create subplots (one per problem type)\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(18, 10))  # Adjust grid based on the number of problem types\n",
    "        axes = axes.flatten()  # Flatten to index easier\n",
    "\n",
    "        for i, (problem_type, digits_data) in enumerate(average_accuracy_per_pt.items()):\n",
    "            ax = axes[i]\n",
    "\n",
    "            digits = list(digits_data.keys())  # ['digit 0', 'digit 1', ...]\n",
    "            x = np.arange(len(digits))  # X-axis positions\n",
    "\n",
    "            first_num = [digits_data[d][0] for d in digits]  # First number accuracies\n",
    "            second_num = [digits_data[d][1] for d in digits]  # Second number accuracies\n",
    "\n",
    "            width = 0.35  # Width of bars\n",
    "            ax.bar(x - width/2, first_num, width, label='First Number', color=colors[0])\n",
    "            ax.bar(x + width/2, second_num, width, label='Second Number', color=colors[1])\n",
    "\n",
    "            # Formatting\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(digits, rotation=45)\n",
    "            ax.set_ylim(minval, maxval)\n",
    "            ax.set_title(problem_type)\n",
    "            ax.legend()\n",
    "            ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "        # Adjust layout and show the plot\n",
    "        plt.tight_layout()\n",
    "        if testing_verbose:\n",
    "            plt.show()\n",
    "        if log_wandb:\n",
    "            wandb.log({f\"per_pt_encoding_accuracy\": wandb.Image(plt)})  # Log to wandb\n",
    "        plt.close()\n",
    "\n",
    "        # Compute average per digit across all problem types\n",
    "        avg_first_number = []\n",
    "        avg_second_number = []\n",
    "\n",
    "        for d in digits:\n",
    "            first_vals = [average_accuracy_per_pt[pt][d][0] for pt in average_accuracy_per_pt]\n",
    "            second_vals = [average_accuracy_per_pt[pt][d][1] for pt in average_accuracy_per_pt]\n",
    "            avg_first_number.append(np.mean(first_vals))\n",
    "            avg_second_number.append(np.mean(second_vals))\n",
    "\n",
    "        # Plotting\n",
    "        x = np.arange(len(digits))  # X-axis positions\n",
    "        width = 0.35  # Bar width\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        ax.bar(x - width/2, avg_first_number, width, label='First Number', color='blue')\n",
    "        ax.bar(x + width/2, avg_second_number, width, label='Second Number', color='orange')\n",
    "\n",
    "        # Formatting\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(digits, rotation=45)\n",
    "        ax.set_ylim(minval, maxval)  # Adjust y-axis for better visibility\n",
    "        ax.set_ylabel(\"Average Accuracy\")\n",
    "        ax.set_title(\"Average Encoding Accuracy Across Problem Types\")\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "        # Show the plot\n",
    "        if testing_verbose:\n",
    "            plt.show()\n",
    "        if log_wandb:\n",
    "            wandb.log({f\"all_pts_encoding_accuracy\": wandb.Image(plt)})  # Log to wandb\n",
    "        plt.close()\n",
    "\n",
    "def initialize_default_config():\n",
    "    encoder_path = f\"{curr_dir}/models/encoders_20250225.pth\"\n",
    "    decoder_path = f\"{curr_dir}/models/decoders_20241221.pth\"\n",
    "\n",
    "    save_model = True # Whether or not to save the model\n",
    "\n",
    "    # Training problem types\n",
    "    problem_type = [\"multiplication\", \"modulo\", \"gcd\", \"lcm\", \"square_mod\", \"bitwise_and\", \"bitwise_xor\", \"bitwise_or\"]\n",
    "    complexity   = 2 # Complexity of problems to ask, represented by number of digits + 1 (of x and y)\n",
    "    temperature  = 0 # Temperature of LLM during training\n",
    "\n",
    "    train_model              = True  # If false, then only do testing step\n",
    "    lora_baseline            = False # If True,  instead of running symbolic encoder-decoder architecture, run a lora module\n",
    "    starting_skip_strength   = 0.5   # The the starting strength of skip connections (0 is all symbolic, 1 is all LLM)\n",
    "    problem_score_threshold  = 0.8   # If the similarity between the problem type is less than this value, don't us symbolic model\n",
    "    normalize_VSA_before_dot = False # If true,  normalize VSA (from encoder) before doing a dot product with different problem types\n",
    "    initialize_decoders      = True  # If true,  initialize decoders as the pseudo-inverse of the encoders\n",
    "    normalize_vector         = False # If true,  normalize the output vector (whether it's noise or the solution hidden state)\n",
    "    rms_layer                = False # If true,  then fixed_skip is not used\n",
    "    double_rep               = True  # If true,  the solution is represented as n1 bound with the solution plus n2 bound with 0\n",
    "    use_specific_identities  = False # If true,  the solution is represented as n1 bound with the identity of n1 under each specific operation (makes double_rep=False)\n",
    "    trainable_skip           = False # If false, then this will allow the strength of the mixing ratio to be learnable\n",
    "    symbolic_encoding_layer  =  17   # Layer to use while generating symbolic vector of n1 and n2\n",
    "    symbolic_decoding_layers = [17]  # Layer to apply decoding network\n",
    "\n",
    "    # Total batch size is inference_to_backprop_ratio * n_samples\n",
    "    n_samples                   = max_batch_size # should be less or equal to  than params.max_batch_size\n",
    "    inference_to_backprop_ratio = 8 # Batch size is effectively n_samples * inference_to_backprop_ratio\n",
    "    num_epochs                  = 1000\n",
    "\n",
    "    learning_rate                   = 1e-3 # Base learning rate, modified by learning_rate_reduction_factors\n",
    "    learning_rate_reduction_factors = {100: 0.5, 500:  0.5, 1000: 0.4, 2000: 0.1, 4000: 0.5, 6000: 0.5, 8000: 0.5}\n",
    "\n",
    "    epochs_to_print    = num_epochs // 100 # How many epochs to print. If greater than 10, running averages will be printed\n",
    "    print_all_pts_freq = 100 # If multiple problem types are present, this is the frequency to print performance per problem type\n",
    "    verbose            = 0 # verbose=0 means no prints, verbose=1 means print the first row in batch data, verbose=2 means print all batch data\n",
    "\n",
    "    # Testing Hyperparameters\n",
    "    testing_problems                    = ['addition', 'division', 'multiplication', 'modulo', 'gcd',\n",
    "                                           'lcm', 'square_mod', 'bitwise_and', 'bitwise_xor', 'bitwise_or']\n",
    "    testing_num_epochs                  = 100\n",
    "    testing_inference_to_backprop_ratio = 1\n",
    "    testing_n_samples                   = max_batch_size # should be less than or equal to params.max_batch_size\n",
    "\n",
    "    testing_temperature      = 0 # Temperature to use when testing model \n",
    "    testing_epochs_to_print  = 0 # If multiple problem types are present, this is the frequency to print performance per problem type\n",
    "    testing_verbose          = 0 # verbose=0 means no prints, verbose=1 means print the first row in batch data, verbose=2 means print all batch data\n",
    "    record_score_per_problem = 2 # If 2/1/0, during testing/training+testing/neither, store the problem type and score info per sample\n",
    "\n",
    "    test_baseline = 0 # 0 means only test trained LLM, 1 means both test trained LLM and do baseline, 2 means only test baseline\n",
    "    cot           = False # whether to use Chain of Thought prompting\n",
    "    \n",
    "    test_on_unrelated_questions = False\n",
    "    test_with_non_numerical_rep = False\n",
    "    \n",
    "    #####################################################\n",
    "\n",
    "    encoder_input_tokens = 1     # The number of tokens the encoder expects as input (default is 1)\n",
    "    calculate_end_index  = False # If set to \"all\", number of encoder input tokens will be generated dynamically\n",
    "    \n",
    "    multi_token_intervention    = False # If True, perform intervention over multiple output tokens\n",
    "    static_encoding             = True  # If True, instead of recomputing symbolic representation for future tokens, use the initial encoding\n",
    "    calculate_encoding_accuracy = True  # If True, calculate the encoding accuracy per problem type per digit\n",
    "    encode_counter              = False # If True, the input to the decoder gets the output token number added onto it\n",
    "\n",
    "    config = {\n",
    "        'encoder_path'                        : encoder_path,\n",
    "        'decoder_path'                        : decoder_path,\n",
    "        'save_model'                          : save_model,\n",
    "\n",
    "        'problem_type'                        : problem_type,\n",
    "        'complexity'                          : complexity,\n",
    "        'temperature'                         : temperature,\n",
    "\n",
    "        'train_model'                         : train_model,\n",
    "        'lora_baseline'                       : lora_baseline,\n",
    "        'starting_skip_strength'              : starting_skip_strength,\n",
    "        'problem_score_threshold'             : problem_score_threshold,\n",
    "        'normalize_VSA_before_dot'            : normalize_VSA_before_dot,\n",
    "        'initialize_decoders'                 : initialize_decoders,\n",
    "        'normalize_vector'                    : normalize_vector,\n",
    "        'rms_layer'                           : rms_layer,\n",
    "        'double_rep'                          : double_rep,\n",
    "        'use_specific_identities'             : use_specific_identities,\n",
    "        'trainable_skip'                      : trainable_skip,\n",
    "        'symbolic_encoding_layer'             : symbolic_encoding_layer,\n",
    "        'symbolic_decoding_layers'            : symbolic_decoding_layers,\n",
    "\n",
    "        'num_epochs'                          : num_epochs,\n",
    "        'n_samples'                           : n_samples,\n",
    "        'inference_to_backprop_ratio'         : inference_to_backprop_ratio,\n",
    "        'learning_rate'                       : learning_rate,\n",
    "        'learning_rate_reduction_factors'     : learning_rate_reduction_factors,\n",
    "\n",
    "        'epochs_to_print'                     : epochs_to_print,\n",
    "        'print_all_pts_freq'                  : print_all_pts_freq,\n",
    "        'verbose'                             : verbose,\n",
    "\n",
    "        'testing_problems'                    : testing_problems,\n",
    "        'testing_num_epochs'                  : testing_num_epochs,\n",
    "        'testing_inference_to_backprop_ratio' : testing_inference_to_backprop_ratio,\n",
    "        'testing_n_samples'                   : testing_n_samples,\n",
    "\n",
    "        'testing_temperature'                 : testing_temperature,\n",
    "        'testing_epochs_to_print'             : testing_epochs_to_print,\n",
    "        'testing_verbose'                     : testing_verbose,\n",
    "        'record_score_per_problem'            : record_score_per_problem,\n",
    "\n",
    "        'test_baseline'                       : test_baseline,\n",
    "        'cot'                                 : cot,\n",
    "        'test_on_unrelated_questions'         : test_on_unrelated_questions,\n",
    "        'test_with_non_numerical_rep'         : test_with_non_numerical_rep,\n",
    "        \n",
    "        'encoder_input_tokens'                : encoder_input_tokens,\n",
    "        'calculate_end_index'                 : calculate_end_index,\n",
    "        \n",
    "        'multi_token_intervention'            : multi_token_intervention,\n",
    "        'static_encoding'                     : static_encoding,\n",
    "        'calculate_encoding_accuracy'         : calculate_encoding_accuracy,\n",
    "        'encode_counter'                      : encode_counter,\n",
    "    }\n",
    "\n",
    "    if not os.path.exists(f\"{curr_dir}/outputs\"):\n",
    "        os.mkdir(f\"{curr_dir}/outputs\")\n",
    "    with open(f\"{curr_dir}/score_per_problem_training_and_testing.txt\", \"w\") as file:\n",
    "        file.write(\"actual_problem_type,problem_type,score\\n\")\n",
    "    with open(f\"{curr_dir}/score_per_problem.txt\", \"w\") as file:\n",
    "        file.write(\"actual_problem_type,problem_type,score\\n\")\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ef1507b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_testing_loss_SYM_addition</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_bitwise_and</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_bitwise_or</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_bitwise_xor</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_division</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_gcd</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_lcm</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_modulo</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_multiplication</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_square_mod</td><td>▁</td></tr><tr><td>average_testing_score_SYM_addition</td><td>▁</td></tr><tr><td>average_testing_score_SYM_bitwise_and</td><td>▁</td></tr><tr><td>average_testing_score_SYM_bitwise_or</td><td>▁</td></tr><tr><td>average_testing_score_SYM_bitwise_xor</td><td>▁</td></tr><tr><td>average_testing_score_SYM_division</td><td>▁</td></tr><tr><td>average_testing_score_SYM_gcd</td><td>▁</td></tr><tr><td>average_testing_score_SYM_lcm</td><td>▁</td></tr><tr><td>average_testing_score_SYM_modulo</td><td>▁</td></tr><tr><td>average_testing_score_SYM_multiplication</td><td>▁</td></tr><tr><td>average_testing_score_SYM_square_mod</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>epoch_bitwise_and</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇████</td></tr><tr><td>epoch_bitwise_or</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>epoch_bitwise_xor</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>epoch_gcd</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>epoch_lcm</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch_modulo</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇█████</td></tr><tr><td>epoch_multiplication</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>epoch_square_mod</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>final_loss_bitwise_and</td><td>▁</td></tr><tr><td>final_loss_bitwise_or</td><td>▁</td></tr><tr><td>final_loss_bitwise_xor</td><td>▁</td></tr><tr><td>final_loss_gcd</td><td>▁</td></tr><tr><td>final_loss_lcm</td><td>▁</td></tr><tr><td>final_loss_modulo</td><td>▁</td></tr><tr><td>final_loss_multiplication</td><td>▁</td></tr><tr><td>final_loss_square_mod</td><td>▁</td></tr><tr><td>final_score_bitwise_and</td><td>▁</td></tr><tr><td>final_score_bitwise_or</td><td>▁</td></tr><tr><td>final_score_bitwise_xor</td><td>▁</td></tr><tr><td>final_score_gcd</td><td>▁</td></tr><tr><td>final_score_lcm</td><td>▁</td></tr><tr><td>final_score_modulo</td><td>▁</td></tr><tr><td>final_score_multiplication</td><td>▁</td></tr><tr><td>final_score_square_mod</td><td>▁</td></tr><tr><td>loss</td><td>█▆▅▅▄▃▂▂▃▄▃▂▂▂▂▂▁▁▁▃▁▂▁▁▂▂▁▃▂▁▃▂▁▃▂▂▇▂▃▂</td></tr><tr><td>loss_bitwise_and</td><td>█▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_bitwise_or</td><td>█▆▆▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_bitwise_xor</td><td>██▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_gcd</td><td>▁▄▁▁▁▂▁▆▁▁▆▁▁█▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_lcm</td><td>▄▄▄▅▃▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁█▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_modulo</td><td>▁▂▂█▆▆▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_multiplication</td><td>█▇▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_square_mod</td><td>▄▃▃▂▁▅▅▇▁▂▁▂▄▆▁▇▅▁▁▁▁▁▁▁▁▁▁▇▁▁▆█▁▄▁▁▁▁▅▃</td></tr><tr><td>score</td><td>▃▁▂▁▁▅▇▅▇█▇▇▇█▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▅▇▇█▇▇▇▇▇▇█</td></tr><tr><td>score_bitwise_and</td><td>█▁▁▁▁▁▁█████████████▁███▁███████████████</td></tr><tr><td>score_bitwise_or</td><td>▁▁▁▁▁▁██▁████████████████▁██▁███████████</td></tr><tr><td>score_bitwise_xor</td><td>█▁██████████████████████████████████████</td></tr><tr><td>score_gcd</td><td>▁████▁██████▁██████▁█████▁████████████▁█</td></tr><tr><td>score_lcm</td><td>▁▁█████████████▁██████████▁██▁██████████</td></tr><tr><td>score_modulo</td><td>████▁███████████████████████████████████</td></tr><tr><td>score_multiplication</td><td>▁▁▁▁█████▁██████████████████████████████</td></tr><tr><td>score_square_mod</td><td>▁▁▁███▁██▁▁▁███▁▁█▁▁███▁█▁▁███▁▁███▁▁███</td></tr><tr><td>skip_weights_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_testing_loss_SYM_addition</td><td>0.14501</td></tr><tr><td>average_testing_loss_SYM_bitwise_and</td><td>1.14485</td></tr><tr><td>average_testing_loss_SYM_bitwise_or</td><td>0.41465</td></tr><tr><td>average_testing_loss_SYM_bitwise_xor</td><td>0.17344</td></tr><tr><td>average_testing_loss_SYM_division</td><td>0.08671</td></tr><tr><td>average_testing_loss_SYM_gcd</td><td>0.61987</td></tr><tr><td>average_testing_loss_SYM_lcm</td><td>1.10432</td></tr><tr><td>average_testing_loss_SYM_modulo</td><td>0.08641</td></tr><tr><td>average_testing_loss_SYM_multiplication</td><td>0.44809</td></tr><tr><td>average_testing_loss_SYM_square_mod</td><td>3.37946</td></tr><tr><td>average_testing_score_SYM_addition</td><td>0.995</td></tr><tr><td>average_testing_score_SYM_bitwise_and</td><td>0.915</td></tr><tr><td>average_testing_score_SYM_bitwise_or</td><td>0.89</td></tr><tr><td>average_testing_score_SYM_bitwise_xor</td><td>0.975</td></tr><tr><td>average_testing_score_SYM_division</td><td>0.98</td></tr><tr><td>average_testing_score_SYM_gcd</td><td>0.925</td></tr><tr><td>average_testing_score_SYM_lcm</td><td>0.92</td></tr><tr><td>average_testing_score_SYM_modulo</td><td>0.99</td></tr><tr><td>average_testing_score_SYM_multiplication</td><td>0.965</td></tr><tr><td>average_testing_score_SYM_square_mod</td><td>0.69</td></tr><tr><td>epoch</td><td>999</td></tr><tr><td>epoch_bitwise_and</td><td>1939</td></tr><tr><td>epoch_bitwise_or</td><td>2083</td></tr><tr><td>epoch_bitwise_xor</td><td>2089</td></tr><tr><td>epoch_gcd</td><td>1983</td></tr><tr><td>epoch_lcm</td><td>1911</td></tr><tr><td>epoch_modulo</td><td>2005</td></tr><tr><td>epoch_multiplication</td><td>2007</td></tr><tr><td>epoch_square_mod</td><td>1975</td></tr><tr><td>final_loss_bitwise_and</td><td>1.07782</td></tr><tr><td>final_loss_bitwise_or</td><td>0.84841</td></tr><tr><td>final_loss_bitwise_xor</td><td>0.57807</td></tr><tr><td>final_loss_gcd</td><td>0.39776</td></tr><tr><td>final_loss_lcm</td><td>1.302</td></tr><tr><td>final_loss_modulo</td><td>0.40014</td></tr><tr><td>final_loss_multiplication</td><td>0.79988</td></tr><tr><td>final_loss_square_mod</td><td>3.10084</td></tr><tr><td>final_score_bitwise_and</td><td>0.85181</td></tr><tr><td>final_score_bitwise_or</td><td>0.79412</td></tr><tr><td>final_score_bitwise_xor</td><td>0.8976</td></tr><tr><td>final_score_gcd</td><td>0.923</td></tr><tr><td>final_score_lcm</td><td>0.81809</td></tr><tr><td>final_score_modulo</td><td>0.92585</td></tr><tr><td>final_score_multiplication</td><td>0.88388</td></tr><tr><td>final_score_square_mod</td><td>0.62157</td></tr><tr><td>loss</td><td>0.78516</td></tr><tr><td>loss_bitwise_and</td><td>0.00084</td></tr><tr><td>loss_bitwise_or</td><td>0.0</td></tr><tr><td>loss_bitwise_xor</td><td>0.0</td></tr><tr><td>loss_gcd</td><td>0.00363</td></tr><tr><td>loss_lcm</td><td>6e-05</td></tr><tr><td>loss_modulo</td><td>0.00038</td></tr><tr><td>loss_multiplication</td><td>0.00029</td></tr><tr><td>loss_square_mod</td><td>12.5625</td></tr><tr><td>score</td><td>0.9375</td></tr><tr><td>score_bitwise_and</td><td>1</td></tr><tr><td>score_bitwise_or</td><td>1</td></tr><tr><td>score_bitwise_xor</td><td>1</td></tr><tr><td>score_gcd</td><td>1</td></tr><tr><td>score_lcm</td><td>1</td></tr><tr><td>score_modulo</td><td>1</td></tr><tr><td>score_multiplication</td><td>1</td></tr><tr><td>score_square_mod</td><td>0</td></tr><tr><td>skip_weights_0</td><td>0.5</td></tr><tr><td>symbolic_output_text_addition</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_bitwise_and</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_bitwise_or</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_bitwise_xor</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_division</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_gcd</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_lcm</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_modulo</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_multiplication</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_square_mod</td><td>Mean score and loss ...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Linear encoder - 3 Digits - AB Test</strong> at: <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/v5mi9edm' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/v5mi9edm</a><br> View project at: <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders</a><br>Synced 5 W&B file(s), 28 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250413_212153-v5mi9edm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vdhanraj/Neurosymbolic-LLM/Programs/wandb/run-20250413_230118-sq3gr7id</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/sq3gr7id' target=\"_blank\">colorful-donkey-326</a></strong> to <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/sq3gr7id' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/sq3gr7id</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">colorful-donkey-326</strong> at: <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/sq3gr7id' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/sq3gr7id</a><br> View project at: <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250413_230118-sq3gr7id/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vdhanraj/Neurosymbolic-LLM/Programs/wandb/run-20250413_230118-9bkxkl3q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/9bkxkl3q' target=\"_blank\">Linear encoder - 3 Digits - AB Test</a></strong> to <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/9bkxkl3q' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/9bkxkl3q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING NEW EXPERIMENT (run_id = 9bkxkl3q)\n",
      "\n",
      "\n",
      "Number of trainable parameters: 8388608\n",
      " -------------- Epoch 0, Loss: 6.625, Score: 0.0625  -------------- \n",
      "Learning Rate changed to: 0.0005\n",
      " -------------- Epoch 100, Loss: 3.5911200495049505, Score: 0.35086633663366334  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 4.7073164 , Score: 0.22023809523809523\n",
      "    Problem type: modulo\n",
      "    Loss: 2.5193164 , Score: 0.4947916666666667\n",
      "    Problem type: gcd\n",
      "    Loss: 0.51587707 , Score: 0.8979591836734694\n",
      "    Problem type: lcm\n",
      "    Loss: 4.3914795 , Score: 0.2196969696969697\n",
      "    Problem type: square_mod\n",
      "    Loss: 4.2828965 , Score: 0.17857142857142858\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 3.3029082 , Score: 0.38613861386138615\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 4.621868 , Score: 0.21171171171171171\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 4.1875243 , Score: 0.23295454545454544\n",
      " -------------- Epoch 200, Loss: 2.345791792988184, Score: 0.5904850746268657  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 2.402581 , Score: 0.5688775510204082\n",
      "    Problem type: modulo\n",
      "    Loss: 1.4295558 , Score: 0.7146739130434783\n",
      "    Problem type: gcd\n",
      "    Loss: 0.5068676 , Score: 0.904891304347826\n",
      "    Problem type: lcm\n",
      "    Loss: 3.2206967 , Score: 0.4878048780487805\n",
      "    Problem type: square_mod\n",
      "    Loss: 3.4825757 , Score: 0.4527027027027027\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 2.0587883 , Score: 0.6287128712871287\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 2.8026085 , Score: 0.5155440414507773\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 2.3475435 , Score: 0.5138121546961326\n",
      " -------------- Epoch 300, Loss: 1.8544536603249584, Score: 0.690406976744186  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 1.6709725 , Score: 0.7032786885245902\n",
      "    Problem type: modulo\n",
      "    Loss: 1.0094608 , Score: 0.8017857142857143\n",
      "    Problem type: gcd\n",
      "    Loss: 0.5950477 , Score: 0.8978494623655914\n",
      "    Problem type: lcm\n",
      "    Loss: 2.738342 , Score: 0.5835820895522388\n",
      "    Problem type: square_mod\n",
      "    Loss: 3.3086786 , Score: 0.5269461077844312\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 1.5777326 , Score: 0.7298657718120806\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 1.8007597 , Score: 0.6885245901639344\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 1.7255226 , Score: 0.6397058823529411\n",
      " -------------- Epoch 400, Loss: 1.5818717556998616, Score: 0.7418952618453866  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 1.3802234 , Score: 0.76125\n",
      "    Problem type: modulo\n",
      "    Loss: 0.83162224 , Score: 0.839095744680851\n",
      "    Problem type: gcd\n",
      "    Loss: 0.503233 , Score: 0.9070680628272252\n",
      "    Problem type: lcm\n",
      "    Loss: 2.3054924 , Score: 0.6648230088495575\n",
      "    Problem type: square_mod\n",
      "    Loss: 3.0727754 , Score: 0.5743707093821511\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 1.411826 , Score: 0.7737789203084833\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 1.4138037 , Score: 0.7568922305764411\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 1.4049429 , Score: 0.6943699731903485\n",
      "Learning Rate changed to: 0.0005\n",
      " -------------- Epoch 500, Loss: 1.4373359147184148, Score: 0.7729540918163673  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 1.1848013 , Score: 0.8013833992094862\n",
      "    Problem type: modulo\n",
      "    Loss: 0.67697257 , Score: 0.86875\n",
      "    Problem type: gcd\n",
      "    Loss: 0.4917831 , Score: 0.9126315789473685\n",
      "    Problem type: lcm\n",
      "    Loss: 2.1029963 , Score: 0.7016423357664233\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.9793591 , Score: 0.5985663082437276\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 1.3124497 , Score: 0.8008048289738431\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 1.1900039 , Score: 0.7967145790554415\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 1.2292998 , Score: 0.7385120350109409\n",
      " -------------- Epoch 600, Loss: 1.321603746461789, Score: 0.7947171381031614  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 1.0157564 , Score: 0.8303130148270181\n",
      "    Problem type: modulo\n",
      "    Loss: 0.5976755 , Score: 0.8863636363636364\n",
      "    Problem type: gcd\n",
      "    Loss: 0.46508494 , Score: 0.9168126094570929\n",
      "    Problem type: lcm\n",
      "    Loss: 1.9857376 , Score: 0.7217806041335453\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.911596 , Score: 0.6138032305433186\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 1.2115736 , Score: 0.8225806451612904\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 1.0067784 , Score: 0.82996632996633\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 1.0508636 , Score: 0.7734513274336283\n",
      " -------------- Epoch 700, Loss: 1.2374124948716538, Score: 0.8127674750356634  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 0.924224 , Score: 0.8509142053445851\n",
      "    Problem type: modulo\n",
      "    Loss: 0.52436095 , Score: 0.9007352941176471\n",
      "    Problem type: gcd\n",
      "    Loss: 0.46622613 , Score: 0.917298937784522\n",
      "    Problem type: lcm\n",
      "    Loss: 1.8734224 , Score: 0.7455048409405256\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.8494456 , Score: 0.6318471337579618\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 1.1357212 , Score: 0.841534008683068\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 0.89993274 , Score: 0.8520625889046942\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 0.9416535 , Score: 0.7934451219512195\n",
      " -------------- Epoch 800, Loss: 1.1764114370357976, Score: 0.8259207240948814  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 0.84024906 , Score: 0.867359413202934\n",
      "    Problem type: modulo\n",
      "    Loss: 0.49269316 , Score: 0.9099616858237548\n",
      "    Problem type: gcd\n",
      "    Loss: 0.46036294 , Score: 0.9187418086500655\n",
      "    Problem type: lcm\n",
      "    Loss: 1.7482468 , Score: 0.7666666666666667\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.8337543 , Score: 0.6412556053811659\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 1.100792 , Score: 0.851145038167939\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 0.81146026 , Score: 0.8682896379525593\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 0.86237365 , Score: 0.8114864864864865\n",
      " -------------- Epoch 900, Loss: 1.12132151833385, Score: 0.8367092119866815  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 0.7910842 , Score: 0.8768980477223427\n",
      "    Problem type: modulo\n",
      "    Loss: 0.46159452 , Score: 0.9165714285714286\n",
      "    Problem type: gcd\n",
      "    Loss: 0.4489849 , Score: 0.9215686274509803\n",
      "    Problem type: lcm\n",
      "    Loss: 1.6596835 , Score: 0.7821888412017167\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.812072 , Score: 0.650153217568948\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 1.0324647 , Score: 0.8631813125695217\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 0.7427491 , Score: 0.8801791713325868\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 0.8074507 , Score: 0.8252080856123662\n",
      "Saved /home/vdhanraj/Neurosymbolic-LLM/Programs/models/decoders_20250413_post_fine_tuning_9bkxkl3q_2025_04_14.pth\n",
      "On problem type: multiplication\n",
      "Final Score and Loss: 0.8862745098039215 0.74583584\n",
      "On problem type: modulo\n",
      "Final Score and Loss: 0.9241486068111455 0.42230356\n",
      "On problem type: gcd\n",
      "Final Score and Loss: 0.9252873563218391 0.429678\n",
      "On problem type: lcm\n",
      "Final Score and Loss: 0.7942307692307692 1.6188849\n",
      "On problem type: square_mod\n",
      "Final Score and Loss: 0.659217877094972 2.8081145\n",
      "On problem type: bitwise_and\n",
      "Final Score and Loss: 0.8706720977596741 1.0005031\n",
      "On problem type: bitwise_xor\n",
      "Final Score and Loss: 0.8919597989949749 0.66897535\n",
      "On problem type: bitwise_or\n",
      "Final Score and Loss: 0.8374864572047671 0.7567397\n",
      "~~~~~~~~ Problem Type: addition ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on addition: 100.0 ± 0.0, 0.0 ± 0.0\n",
      "~~~~~~~~ Problem Type: division ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on division: 96.5 ± 12.7574, 0.152 ± 0.6486\n",
      "~~~~~~~~ Problem Type: multiplication ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on multiplication: 98.5 ± 8.5294, 0.194 ± 1.1421\n",
      "~~~~~~~~ Problem Type: modulo ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on modulo: 96.0 ± 15.2971, 0.278 ± 1.0639\n",
      "~~~~~~~~ Problem Type: gcd ~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Mean score and loss of symbolic LLM on gcd: 94.5 ± 15.6445, 0.356 ± 1.2597\n",
      "~~~~~~~~ Problem Type: lcm ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on lcm: 87.5 ± 25.8602, 1.525 ± 3.6281\n",
      "~~~~~~~~ Problem Type: square_mod ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on square_mod: 72.0 ± 33.4066, 2.82 ± 3.6083\n",
      "~~~~~~~~ Problem Type: bitwise_and ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on bitwise_and: 92.0 ± 19.6469, 1.11 ± 2.6114\n",
      "~~~~~~~~ Problem Type: bitwise_xor ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on bitwise_xor: 100.0 ± 0.0, 0.004 ± 0.0101\n",
      "~~~~~~~~ Problem Type: bitwise_or ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on bitwise_or: 94.5 ± 15.6445, 0.224 ± 1.1983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24303/1217783667.py:908: FutureWarning: The provided callable <function max at 0x7fdf3804a200> is currently using DataFrameGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  plt.hist(df[df.actual_problem_type.isin([pt])].  pivot_table(\n",
      "/tmp/ipykernel_24303/1217783667.py:908: FutureWarning: The provided callable <function max at 0x7fdf3804a200> is currently using DataFrameGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  plt.hist(df[df.actual_problem_type.isin([pt])].  pivot_table(\n",
      "/tmp/ipykernel_24303/1217783667.py:908: FutureWarning: The provided callable <function max at 0x7fdf3804a200> is currently using DataFrameGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  plt.hist(df[df.actual_problem_type.isin([pt])].  pivot_table(\n",
      "/tmp/ipykernel_24303/1217783667.py:908: FutureWarning: The provided callable <function max at 0x7fdf3804a200> is currently using DataFrameGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  plt.hist(df[df.actual_problem_type.isin([pt])].  pivot_table(\n",
      "/tmp/ipykernel_24303/1217783667.py:908: FutureWarning: The provided callable <function max at 0x7fdf3804a200> is currently using DataFrameGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  plt.hist(df[df.actual_problem_type.isin([pt])].  pivot_table(\n",
      "/tmp/ipykernel_24303/1217783667.py:908: FutureWarning: The provided callable <function max at 0x7fdf3804a200> is currently using DataFrameGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  plt.hist(df[df.actual_problem_type.isin([pt])].  pivot_table(\n",
      "/tmp/ipykernel_24303/1217783667.py:908: FutureWarning: The provided callable <function max at 0x7fdf3804a200> is currently using DataFrameGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  plt.hist(df[df.actual_problem_type.isin([pt])].  pivot_table(\n",
      "/tmp/ipykernel_24303/1217783667.py:908: FutureWarning: The provided callable <function max at 0x7fdf3804a200> is currently using DataFrameGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  plt.hist(df[df.actual_problem_type.isin([pt])].  pivot_table(\n",
      "/tmp/ipykernel_24303/1217783667.py:923: FutureWarning: The provided callable <function max at 0x7fdf3804a200> is currently using DataFrameGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  plt.hist(df[df.actual_problem_type.isin(trained_pts)].  pivot_table(\n",
      "/tmp/ipykernel_24303/1217783667.py:942: FutureWarning: The provided callable <function max at 0x7fdf3804a200> is currently using DataFrameGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  wandb.log({\"trained_problem_scores\":   df[df.actual_problem_type.isin(trained_pts)]  .pivot_table(\n",
      "/tmp/ipykernel_24303/1217783667.py:947: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda group: group.loc[group['score'].idxmax()])\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_testing_loss_SYM_addition</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_bitwise_and</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_bitwise_or</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_bitwise_xor</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_division</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_gcd</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_lcm</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_modulo</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_multiplication</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_square_mod</td><td>▁</td></tr><tr><td>average_testing_score_SYM_addition</td><td>▁</td></tr><tr><td>average_testing_score_SYM_bitwise_and</td><td>▁</td></tr><tr><td>average_testing_score_SYM_bitwise_or</td><td>▁</td></tr><tr><td>average_testing_score_SYM_bitwise_xor</td><td>▁</td></tr><tr><td>average_testing_score_SYM_division</td><td>▁</td></tr><tr><td>average_testing_score_SYM_gcd</td><td>▁</td></tr><tr><td>average_testing_score_SYM_lcm</td><td>▁</td></tr><tr><td>average_testing_score_SYM_modulo</td><td>▁</td></tr><tr><td>average_testing_score_SYM_multiplication</td><td>▁</td></tr><tr><td>average_testing_score_SYM_square_mod</td><td>▁</td></tr><tr><td>different_actual_problem_types_count</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>epoch_bitwise_and</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>epoch_bitwise_or</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>epoch_bitwise_xor</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>epoch_gcd</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch_lcm</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>epoch_modulo</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>epoch_multiplication</td><td>▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇███</td></tr><tr><td>epoch_square_mod</td><td>▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇███</td></tr><tr><td>final_loss_bitwise_and</td><td>▁</td></tr><tr><td>final_loss_bitwise_or</td><td>▁</td></tr><tr><td>final_loss_bitwise_xor</td><td>▁</td></tr><tr><td>final_loss_gcd</td><td>▁</td></tr><tr><td>final_loss_lcm</td><td>▁</td></tr><tr><td>final_loss_modulo</td><td>▁</td></tr><tr><td>final_loss_multiplication</td><td>▁</td></tr><tr><td>final_loss_square_mod</td><td>▁</td></tr><tr><td>final_score_bitwise_and</td><td>▁</td></tr><tr><td>final_score_bitwise_or</td><td>▁</td></tr><tr><td>final_score_bitwise_xor</td><td>▁</td></tr><tr><td>final_score_gcd</td><td>▁</td></tr><tr><td>final_score_lcm</td><td>▁</td></tr><tr><td>final_score_modulo</td><td>▁</td></tr><tr><td>final_score_multiplication</td><td>▁</td></tr><tr><td>final_score_square_mod</td><td>▁</td></tr><tr><td>loss</td><td>█▇▆▅▇▆▅▄▆▂▆▁▃▅▁▃▁▄▆▂▄▂▄▁▂▁▁▁▁▁█▂▁▂▃▁▃▅▁█</td></tr><tr><td>loss_bitwise_and</td><td>▄▅▂▃▅▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁</td></tr><tr><td>loss_bitwise_or</td><td>█▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_bitwise_xor</td><td>▃▂▃▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁█▁▁▁▁▁▁▆▁▁▁▁</td></tr><tr><td>loss_gcd</td><td>▂▁▁▆▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_lcm</td><td>▅▄▅▄▁▁▁▁▄▁▁▁▁▁▁▁▆▁▂▁▁▁▁▁█▁▁▁▇▁▁▁▁▁▁▃█▁▁▁</td></tr><tr><td>loss_modulo</td><td>▃█▃▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_multiplication</td><td>▂▃▃▅▄▁▂▁▁▂▁▁▁▁▁▂▁▁▆▁▁▁▁▅▁▂▁▁▁▁▁▁▁▁▁▁█▁▁▁</td></tr><tr><td>loss_square_mod</td><td>▃▃▂▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▃▃▁▁▅▁▁▁▁▁▁▁▇▁▄▁▁▁▁█▁</td></tr><tr><td>score</td><td>▁▄▅▆▆▇▇▆▆▆▇█▅███▇█▇▇█▇▇██▇▇▇▇▇█▆▇█▇▇█▇█▇</td></tr><tr><td>score_bitwise_and</td><td>▁▁█████████████████████▁██▁█████████████</td></tr><tr><td>score_bitwise_or</td><td>▁▁██▁█▁█████▁████████▁█▁████████████████</td></tr><tr><td>score_bitwise_xor</td><td>▁▁▁▁▁█████████████████▁█████████████████</td></tr><tr><td>score_gcd</td><td>▁██████▁████▁████████████████████▁▁█████</td></tr><tr><td>score_lcm</td><td>▁▁▁▁█████████▁██████████▁█▁█████████████</td></tr><tr><td>score_modulo</td><td>▁█▁██████████████████████████████▁██████</td></tr><tr><td>score_multiplication</td><td>▁▁█▁▁▁█████████▁██▁█████████████████████</td></tr><tr><td>score_square_mod</td><td>▁▁▁█▁██████▁▁███▁███▁█████▁███████████▁▁</td></tr><tr><td>skip_weights_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_testing_loss_SYM_addition</td><td>1e-05</td></tr><tr><td>average_testing_loss_SYM_bitwise_and</td><td>1.10967</td></tr><tr><td>average_testing_loss_SYM_bitwise_or</td><td>0.22357</td></tr><tr><td>average_testing_loss_SYM_bitwise_xor</td><td>0.00408</td></tr><tr><td>average_testing_loss_SYM_division</td><td>0.15241</td></tr><tr><td>average_testing_loss_SYM_gcd</td><td>0.35637</td></tr><tr><td>average_testing_loss_SYM_lcm</td><td>1.52509</td></tr><tr><td>average_testing_loss_SYM_modulo</td><td>0.27763</td></tr><tr><td>average_testing_loss_SYM_multiplication</td><td>0.19407</td></tr><tr><td>average_testing_loss_SYM_square_mod</td><td>2.82008</td></tr><tr><td>average_testing_score_SYM_addition</td><td>1</td></tr><tr><td>average_testing_score_SYM_bitwise_and</td><td>0.92</td></tr><tr><td>average_testing_score_SYM_bitwise_or</td><td>0.945</td></tr><tr><td>average_testing_score_SYM_bitwise_xor</td><td>1</td></tr><tr><td>average_testing_score_SYM_division</td><td>0.965</td></tr><tr><td>average_testing_score_SYM_gcd</td><td>0.945</td></tr><tr><td>average_testing_score_SYM_lcm</td><td>0.875</td></tr><tr><td>average_testing_score_SYM_modulo</td><td>0.96</td></tr><tr><td>average_testing_score_SYM_multiplication</td><td>0.985</td></tr><tr><td>average_testing_score_SYM_square_mod</td><td>0.72</td></tr><tr><td>different_actual_problem_types_count</td><td>0</td></tr><tr><td>epoch</td><td>999</td></tr><tr><td>epoch_bitwise_and</td><td>1973</td></tr><tr><td>epoch_bitwise_or</td><td>1855</td></tr><tr><td>epoch_bitwise_xor</td><td>1999</td></tr><tr><td>epoch_gcd</td><td>1923</td></tr><tr><td>epoch_lcm</td><td>2089</td></tr><tr><td>epoch_modulo</td><td>1947</td></tr><tr><td>epoch_multiplication</td><td>2049</td></tr><tr><td>epoch_square_mod</td><td>2157</td></tr><tr><td>final_loss_bitwise_and</td><td>1.0005</td></tr><tr><td>final_loss_bitwise_or</td><td>0.75674</td></tr><tr><td>final_loss_bitwise_xor</td><td>0.66898</td></tr><tr><td>final_loss_gcd</td><td>0.42968</td></tr><tr><td>final_loss_lcm</td><td>1.61888</td></tr><tr><td>final_loss_modulo</td><td>0.4223</td></tr><tr><td>final_loss_multiplication</td><td>0.74584</td></tr><tr><td>final_loss_square_mod</td><td>2.80811</td></tr><tr><td>final_score_bitwise_and</td><td>0.87067</td></tr><tr><td>final_score_bitwise_or</td><td>0.83749</td></tr><tr><td>final_score_bitwise_xor</td><td>0.89196</td></tr><tr><td>final_score_gcd</td><td>0.92529</td></tr><tr><td>final_score_lcm</td><td>0.79423</td></tr><tr><td>final_score_modulo</td><td>0.92415</td></tr><tr><td>final_score_multiplication</td><td>0.88627</td></tr><tr><td>final_score_square_mod</td><td>0.65922</td></tr><tr><td>loss</td><td>2.51562</td></tr><tr><td>loss_bitwise_and</td><td>8e-05</td></tr><tr><td>loss_bitwise_or</td><td>20.5</td></tr><tr><td>loss_bitwise_xor</td><td>2e-05</td></tr><tr><td>loss_gcd</td><td>6e-05</td></tr><tr><td>loss_lcm</td><td>12.625</td></tr><tr><td>loss_modulo</td><td>0.00096</td></tr><tr><td>loss_multiplication</td><td>0.0001</td></tr><tr><td>loss_square_mod</td><td>13.25</td></tr><tr><td>score</td><td>0.8125</td></tr><tr><td>score_bitwise_and</td><td>1</td></tr><tr><td>score_bitwise_or</td><td>0</td></tr><tr><td>score_bitwise_xor</td><td>1</td></tr><tr><td>score_gcd</td><td>1</td></tr><tr><td>score_lcm</td><td>0</td></tr><tr><td>score_modulo</td><td>1</td></tr><tr><td>score_multiplication</td><td>1</td></tr><tr><td>score_square_mod</td><td>0</td></tr><tr><td>skip_weights_0</td><td>0.5</td></tr><tr><td>symbolic_output_text_addition</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_bitwise_and</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_bitwise_or</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_bitwise_xor</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_division</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_gcd</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_lcm</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_modulo</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_multiplication</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_square_mod</td><td>Mean score and loss ...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Linear encoder - 3 Digits - AB Test</strong> at: <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/9bkxkl3q' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/9bkxkl3q</a><br> View project at: <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders</a><br>Synced 5 W&B file(s), 32 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250413_230118-9bkxkl3q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish() # If there is an active current run, terminate it\n",
    "config = initialize_default_config()\n",
    "\n",
    "config[\"encoder_input_tokens\"] = 1\n",
    "config[\"complexity\"] = 2\n",
    "config[\"initialize_decoders\"] = True\n",
    "config[\"symbolic_encoding_layer\"] = 17\n",
    "config[\"symbolic_decoding_layers\"] = [17]\n",
    "config[\"num_epochs\"] = 1000\n",
    "\n",
    "config[\"encoder_path\"] = f\"{curr_dir}/models/encoders_20250413.pth\"\n",
    "config[\"decoder_path\"] = f\"{curr_dir}/models/decoders_20250413.pth\"\n",
    "\n",
    "#config[\"verbose\"] = 2\n",
    "#config[\"epochs_to_print\"] = config[\"num_epochs\"]\n",
    "\n",
    "#config['test_with_non_numerical_rep'] = True\n",
    "#config['train_model'] = False\n",
    "\n",
    "#config[\"testing_verbose\"] = 2\n",
    "config[\"multi_token_intervention\"]    = False\n",
    "config['static_encoding']             = False\n",
    "\n",
    "config[\"calculate_encoding_accuracy\"] = True\n",
    "#config[\"n_samples\"] = 1\n",
    "#config[\"verbose\"] = 2\n",
    "\n",
    "\n",
    "if log_wandb:\n",
    "    wandb.finish() # If there is an active current run, terminate it\n",
    "    wandb.init(\n",
    "        project = \"Symbolic LLM - Fine Tune Decoders\",\n",
    "        config  = config\n",
    "    )\n",
    "    wandb.init(\n",
    "        project = \"Symbolic LLM - Fine Tune Decoders\",\n",
    "        #name    = f\"Transformer encoder - 3 Digits\",\n",
    "        #name    = f\"Transformer encoder - 4 Digits\",\n",
    "        name    = f\"Linear encoder - 3 Digits - AB Test\",\n",
    "        #name    = f\"Linear encoder - 4 Digits\",\n",
    "        config  = config\n",
    "    )\n",
    "\n",
    "    print(f\"STARTING NEW EXPERIMENT (run_id = {wandb.run.id})\\n\\n\")\n",
    "\n",
    "run_experiment(generator=generator, config=config)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07d0596e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vdhanraj/Neurosymbolic-LLM/Programs/wandb/run-20250414_003655-5za7egpm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/5za7egpm' target=\"_blank\">colorful-water-328</a></strong> to <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/5za7egpm' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/5za7egpm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">colorful-water-328</strong> at: <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/5za7egpm' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/5za7egpm</a><br> View project at: <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_003655-5za7egpm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vdhanraj/Neurosymbolic-LLM/Programs/wandb/run-20250414_003656-i6by1q7h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/i6by1q7h' target=\"_blank\">Linear encoder - 3 Digits - AB Test (Initialize Decoder False)</a></strong> to <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/i6by1q7h' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/i6by1q7h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING NEW EXPERIMENT (run_id = i6by1q7h)\n",
      "\n",
      "\n",
      "Number of trainable parameters: 8388608\n",
      " -------------- Epoch 0, Loss: 5.75, Score: 0.4375  -------------- \n",
      "Learning Rate changed to: 0.0005\n",
      " -------------- Epoch 100, Loss: 3.123046875, Score: 0.41955445544554454  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 3.8741915 , Score: 0.2871287128712871\n",
      "    Problem type: modulo\n",
      "    Loss: 2.3435252 , Score: 0.5097087378640777\n",
      "    Problem type: gcd\n",
      "    Loss: 0.53601366 , Score: 0.9040404040404041\n",
      "    Problem type: lcm\n",
      "    Loss: 3.6528494 , Score: 0.27419354838709675\n",
      "    Problem type: square_mod\n",
      "    Loss: 4.0677266 , Score: 0.27522935779816515\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 3.048782 , Score: 0.4068627450980392\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 4.1828647 , Score: 0.3404255319148936\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 3.2830725 , Score: 0.3644859813084112\n",
      " -------------- Epoch 200, Loss: 2.03862644783893, Score: 0.6318407960199005  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 2.3191261 , Score: 0.5698924731182796\n",
      "    Problem type: modulo\n",
      "    Loss: 1.2501762 , Score: 0.7391304347826086\n",
      "    Problem type: gcd\n",
      "    Loss: 0.5285571 , Score: 0.9035532994923858\n",
      "    Problem type: lcm\n",
      "    Loss: 2.6291566 , Score: 0.5247252747252747\n",
      "    Problem type: square_mod\n",
      "    Loss: 3.5345418 , Score: 0.44047619047619047\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 2.029515 , Score: 0.6384976525821596\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 2.1136982 , Score: 0.648989898989899\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 1.9284725 , Score: 0.5883720930232558\n",
      " -------------- Epoch 300, Loss: 1.6325240531237022, Score: 0.7167774086378738  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 1.66381 , Score: 0.6996466431095406\n",
      "    Problem type: modulo\n",
      "    Loss: 0.9034056 , Score: 0.8144654088050315\n",
      "    Problem type: gcd\n",
      "    Loss: 0.46992415 , Score: 0.9030612244897959\n",
      "    Problem type: lcm\n",
      "    Loss: 2.0962136 , Score: 0.650709219858156\n",
      "    Problem type: square_mod\n",
      "    Loss: 3.2717955 , Score: 0.5147058823529411\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 1.8290102 , Score: 0.7075163398692811\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 1.3940718 , Score: 0.7672131147540984\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 1.4680327 , Score: 0.6751592356687898\n",
      " -------------- Epoch 400, Loss: 1.4406844065373676, Score: 0.760286783042394  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 1.3029926 , Score: 0.7661498708010336\n",
      "    Problem type: modulo\n",
      "    Loss: 0.7345722 , Score: 0.849502487562189\n",
      "    Problem type: gcd\n",
      "    Loss: 0.46541807 , Score: 0.9057591623036649\n",
      "    Problem type: lcm\n",
      "    Loss: 1.8179877 , Score: 0.7135549872122762\n",
      "    Problem type: square_mod\n",
      "    Loss: 3.100775 , Score: 0.5689655172413793\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 1.6326191 , Score: 0.7537128712871287\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 1.0679903 , Score: 0.8202933985330073\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 1.2385293 , Score: 0.7248743718592965\n",
      "Learning Rate changed to: 0.0005\n",
      " -------------- Epoch 500, Loss: 1.2997107896024358, Score: 0.7915419161676647  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 1.1130581 , Score: 0.8062880324543611\n",
      "    Problem type: modulo\n",
      "    Loss: 0.6015212 , Score: 0.8759920634920635\n",
      "    Problem type: gcd\n",
      "    Loss: 0.45169264 , Score: 0.907707910750507\n",
      "    Problem type: lcm\n",
      "    Loss: 1.6597421 , Score: 0.7565922920892495\n",
      "    Problem type: square_mod\n",
      "    Loss: 3.0404775 , Score: 0.5965909090909091\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 1.4968553 , Score: 0.7841584158415842\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 0.89088553 , Score: 0.8488843813387424\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 1.0448323 , Score: 0.7685370741482966\n",
      " -------------- Epoch 600, Loss: 1.1963013253870503, Score: 0.812603993344426  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 0.97087145 , Score: 0.8330494037478705\n",
      "    Problem type: modulo\n",
      "    Loss: 0.5277966 , Score: 0.8928571428571429\n",
      "    Problem type: gcd\n",
      "    Loss: 0.43086448 , Score: 0.9120689655172414\n",
      "    Problem type: lcm\n",
      "    Loss: 1.5500212 , Score: 0.7820299500831946\n",
      "    Problem type: square_mod\n",
      "    Loss: 3.0170872 , Score: 0.6095238095238096\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 1.2891746 , Score: 0.8169129720853858\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 0.76716024 , Score: 0.871404399323181\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 0.90444773 , Score: 0.7976973684210527\n",
      " -------------- Epoch 700, Loss: 1.1368225903041693, Score: 0.826765335235378  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 0.894321 , Score: 0.8517441860465116\n",
      "    Problem type: modulo\n",
      "    Loss: 0.481972 , Score: 0.9045584045584045\n",
      "    Problem type: gcd\n",
      "    Loss: 0.41458872 , Score: 0.9146884272997032\n",
      "    Problem type: lcm\n",
      "    Loss: 1.4618995 , Score: 0.7995839112343966\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.9831822 , Score: 0.6219346049046321\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 1.2202415 , Score: 0.8304597701149425\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 0.6921307 , Score: 0.886231884057971\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 0.82417905 , Score: 0.8200568990042674\n",
      " -------------- Epoch 800, Loss: 1.0773250851291842, Score: 0.8396535580524345  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 0.849335 , Score: 0.8645699614890886\n",
      "    Problem type: modulo\n",
      "    Loss: 0.42986414 , Score: 0.9156403940886699\n",
      "    Problem type: gcd\n",
      "    Loss: 0.3992083 , Score: 0.9174311926605505\n",
      "    Problem type: lcm\n",
      "    Loss: 1.3470305 , Score: 0.818562874251497\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.933887 , Score: 0.6383874849578821\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 1.1757407 , Score: 0.8418114143920595\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 0.6237897 , Score: 0.8983375959079284\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 0.7485776 , Score: 0.835625\n",
      " -------------- Epoch 900, Loss: 1.0295767154333726, Score: 0.849472807991121  -------------- \n",
      "~~~~~~~~~~~~~ Printing Stats Per Problem Type: ~~~~~~~~~~~~~\n",
      "    Problem type: multiplication\n",
      "    Loss: 0.7774783 , Score: 0.8774011299435028\n",
      "    Problem type: modulo\n",
      "    Loss: 0.39808777 , Score: 0.9226519337016574\n",
      "    Problem type: gcd\n",
      "    Loss: 0.39567095 , Score: 0.9197459584295612\n",
      "    Problem type: lcm\n",
      "    Loss: 1.3095843 , Score: 0.8269435569755058\n",
      "    Problem type: square_mod\n",
      "    Loss: 2.8801942 , Score: 0.6502145922746781\n",
      "    Problem type: bitwise_and\n",
      "    Loss: 1.1436087 , Score: 0.8498883928571429\n",
      "    Problem type: bitwise_xor\n",
      "    Loss: 0.56374866 , Score: 0.9087799315849487\n",
      "    Problem type: bitwise_or\n",
      "    Loss: 0.6685491 , Score: 0.8524229074889867\n",
      "Saved /home/vdhanraj/Neurosymbolic-LLM/Programs/models/decoders_20250413_post_fine_tuning_i6by1q7h_2025_04_14.pth\n",
      "On problem type: multiplication\n",
      "Final Score and Loss: 0.8864337101747174 0.7292558\n",
      "On problem type: modulo\n",
      "Final Score and Loss: 0.926605504587156 0.3905993\n",
      "On problem type: gcd\n",
      "Final Score and Loss: 0.9205020920502092 0.38767233\n",
      "On problem type: lcm\n",
      "Final Score and Loss: 0.8359298928919182 1.2589339\n",
      "On problem type: square_mod\n",
      "Final Score and Loss: 0.6526315789473685 2.8892858\n",
      "On problem type: bitwise_and\n",
      "Final Score and Loss: 0.856 1.1256033\n",
      "On problem type: bitwise_xor\n",
      "Final Score and Loss: 0.9167528438469493 0.51241964\n",
      "On problem type: bitwise_or\n",
      "Final Score and Loss: 0.8625123639960435 0.6418202\n",
      "~~~~~~~~ Problem Type: addition ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on addition: 100.0 ± 0.0, 0.0 ± 0.0\n",
      "~~~~~~~~ Problem Type: division ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on division: 95.5 ± 14.3091, 0.171 ± 0.6748\n",
      "~~~~~~~~ Problem Type: multiplication ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on multiplication: 96.0 ± 13.5647, 0.292 ± 1.2145\n",
      "~~~~~~~~ Problem Type: modulo ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on modulo: 98.0 ± 9.798, 0.155 ± 0.7468\n",
      "~~~~~~~~ Problem Type: gcd ~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Mean score and loss of symbolic LLM on gcd: 94.0 ± 16.2481, 0.339 ± 1.0807\n",
      "~~~~~~~~ Problem Type: lcm ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on lcm: 91.5 ± 18.7816, 0.796 ± 1.8436\n",
      "~~~~~~~~ Problem Type: square_mod ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on square_mod: 73.0 ± 33.4813, 2.348 ± 2.9293\n",
      "~~~~~~~~ Problem Type: bitwise_and ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on bitwise_and: 93.5 ± 18.2414, 0.573 ± 1.6601\n",
      "~~~~~~~~ Problem Type: bitwise_xor ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on bitwise_xor: 99.0 ± 7.0, 0.13 ± 0.8555\n",
      "~~~~~~~~ Problem Type: bitwise_or ~~~~~~~~\n",
      "\n",
      " Mean score and loss of symbolic LLM on bitwise_or: 95.0 ± 15.0, 0.195 ± 1.0664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24303/1217783667.py:908: FutureWarning: The provided callable <function max at 0x7fdf3804a200> is currently using DataFrameGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  plt.hist(df[df.actual_problem_type.isin([pt])].  pivot_table(\n",
      "/tmp/ipykernel_24303/1217783667.py:908: FutureWarning: The provided callable <function max at 0x7fdf3804a200> is currently using DataFrameGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  plt.hist(df[df.actual_problem_type.isin([pt])].  pivot_table(\n",
      "/tmp/ipykernel_24303/1217783667.py:908: FutureWarning: The provided callable <function max at 0x7fdf3804a200> is currently using DataFrameGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  plt.hist(df[df.actual_problem_type.isin([pt])].  pivot_table(\n",
      "/tmp/ipykernel_24303/1217783667.py:908: FutureWarning: The provided callable <function max at 0x7fdf3804a200> is currently using DataFrameGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  plt.hist(df[df.actual_problem_type.isin([pt])].  pivot_table(\n",
      "/tmp/ipykernel_24303/1217783667.py:908: FutureWarning: The provided callable <function max at 0x7fdf3804a200> is currently using DataFrameGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  plt.hist(df[df.actual_problem_type.isin([pt])].  pivot_table(\n",
      "/tmp/ipykernel_24303/1217783667.py:908: FutureWarning: The provided callable <function max at 0x7fdf3804a200> is currently using DataFrameGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  plt.hist(df[df.actual_problem_type.isin([pt])].  pivot_table(\n",
      "/tmp/ipykernel_24303/1217783667.py:908: FutureWarning: The provided callable <function max at 0x7fdf3804a200> is currently using DataFrameGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  plt.hist(df[df.actual_problem_type.isin([pt])].  pivot_table(\n",
      "/tmp/ipykernel_24303/1217783667.py:908: FutureWarning: The provided callable <function max at 0x7fdf3804a200> is currently using DataFrameGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  plt.hist(df[df.actual_problem_type.isin([pt])].  pivot_table(\n",
      "/tmp/ipykernel_24303/1217783667.py:923: FutureWarning: The provided callable <function max at 0x7fdf3804a200> is currently using DataFrameGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  plt.hist(df[df.actual_problem_type.isin(trained_pts)].  pivot_table(\n",
      "/tmp/ipykernel_24303/1217783667.py:942: FutureWarning: The provided callable <function max at 0x7fdf3804a200> is currently using DataFrameGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  wandb.log({\"trained_problem_scores\":   df[df.actual_problem_type.isin(trained_pts)]  .pivot_table(\n",
      "/tmp/ipykernel_24303/1217783667.py:947: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda group: group.loc[group['score'].idxmax()])\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_testing_loss_SYM_addition</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_bitwise_and</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_bitwise_or</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_bitwise_xor</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_division</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_gcd</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_lcm</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_modulo</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_multiplication</td><td>▁</td></tr><tr><td>average_testing_loss_SYM_square_mod</td><td>▁</td></tr><tr><td>average_testing_score_SYM_addition</td><td>▁</td></tr><tr><td>average_testing_score_SYM_bitwise_and</td><td>▁</td></tr><tr><td>average_testing_score_SYM_bitwise_or</td><td>▁</td></tr><tr><td>average_testing_score_SYM_bitwise_xor</td><td>▁</td></tr><tr><td>average_testing_score_SYM_division</td><td>▁</td></tr><tr><td>average_testing_score_SYM_gcd</td><td>▁</td></tr><tr><td>average_testing_score_SYM_lcm</td><td>▁</td></tr><tr><td>average_testing_score_SYM_modulo</td><td>▁</td></tr><tr><td>average_testing_score_SYM_multiplication</td><td>▁</td></tr><tr><td>average_testing_score_SYM_square_mod</td><td>▁</td></tr><tr><td>different_actual_problem_types_count</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>epoch_bitwise_and</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>epoch_bitwise_or</td><td>▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇█</td></tr><tr><td>epoch_bitwise_xor</td><td>▁▁▁▂▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>epoch_gcd</td><td>▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>epoch_lcm</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>epoch_modulo</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>epoch_multiplication</td><td>▁▁▁▁▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>epoch_square_mod</td><td>▁▁▁▁▂▂▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███████</td></tr><tr><td>final_loss_bitwise_and</td><td>▁</td></tr><tr><td>final_loss_bitwise_or</td><td>▁</td></tr><tr><td>final_loss_bitwise_xor</td><td>▁</td></tr><tr><td>final_loss_gcd</td><td>▁</td></tr><tr><td>final_loss_lcm</td><td>▁</td></tr><tr><td>final_loss_modulo</td><td>▁</td></tr><tr><td>final_loss_multiplication</td><td>▁</td></tr><tr><td>final_loss_square_mod</td><td>▁</td></tr><tr><td>final_score_bitwise_and</td><td>▁</td></tr><tr><td>final_score_bitwise_or</td><td>▁</td></tr><tr><td>final_score_bitwise_xor</td><td>▁</td></tr><tr><td>final_score_gcd</td><td>▁</td></tr><tr><td>final_score_lcm</td><td>▁</td></tr><tr><td>final_score_modulo</td><td>▁</td></tr><tr><td>final_score_multiplication</td><td>▁</td></tr><tr><td>final_score_square_mod</td><td>▁</td></tr><tr><td>loss</td><td>█▇▃▅▄▅▂▂▃▃▁▁▂▁▇▂▄▃▁▁▂▃▅▁▆▂▃▁▂▁▁▇▁▂▁▆▂▂▅▁</td></tr><tr><td>loss_bitwise_and</td><td>▄▅▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_bitwise_or</td><td>█▇▁▁▁▆▁▄▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_bitwise_xor</td><td>▅█▆▇▁▅▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_gcd</td><td>▁▁▁▄▁▁▁▁▁▁▁▁▁▇▁▁▃▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_lcm</td><td>▄▃▆▃▂▁▁▅▁▁▁▁▁▁▅▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_modulo</td><td>▄▃▁█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>loss_multiplication</td><td>▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁</td></tr><tr><td>loss_square_mod</td><td>▅▃▁▁▆▅▁▁▂▅▁▆▁▅█▁▆▁▁▄▆▁▅▁▁▁▁▁▂▁▁▁▆▁▁▄▁▁▂▁</td></tr><tr><td>score</td><td>▂▁▂▅▅▆▇▇▇███▇███████▇▇▇██▇▇▇████▆▅███▇█▇</td></tr><tr><td>score_bitwise_and</td><td>▁▁██▁████████████████▁███████████▁██████</td></tr><tr><td>score_bitwise_or</td><td>▁▁▁▁███████▁████████████████████████████</td></tr><tr><td>score_bitwise_xor</td><td>█▁█████████████████▁███▁████████████████</td></tr><tr><td>score_gcd</td><td>▁████████████████▁██████████████████████</td></tr><tr><td>score_lcm</td><td>▁▁▁▁▁████▁▁████▁████████████████▁██▁████</td></tr><tr><td>score_modulo</td><td>▁▁▁█▁█▁█████████████████████████████████</td></tr><tr><td>score_multiplication</td><td>▁▁██████████████████████████████████▁███</td></tr><tr><td>score_square_mod</td><td>▁▁▁▁▁█████▁████████▁███████████████▁████</td></tr><tr><td>skip_weights_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_testing_loss_SYM_addition</td><td>1e-05</td></tr><tr><td>average_testing_loss_SYM_bitwise_and</td><td>0.57323</td></tr><tr><td>average_testing_loss_SYM_bitwise_or</td><td>0.19512</td></tr><tr><td>average_testing_loss_SYM_bitwise_xor</td><td>0.12993</td></tr><tr><td>average_testing_loss_SYM_division</td><td>0.17053</td></tr><tr><td>average_testing_loss_SYM_gcd</td><td>0.33892</td></tr><tr><td>average_testing_loss_SYM_lcm</td><td>0.79604</td></tr><tr><td>average_testing_loss_SYM_modulo</td><td>0.15463</td></tr><tr><td>average_testing_loss_SYM_multiplication</td><td>0.29175</td></tr><tr><td>average_testing_loss_SYM_square_mod</td><td>2.34805</td></tr><tr><td>average_testing_score_SYM_addition</td><td>1</td></tr><tr><td>average_testing_score_SYM_bitwise_and</td><td>0.935</td></tr><tr><td>average_testing_score_SYM_bitwise_or</td><td>0.95</td></tr><tr><td>average_testing_score_SYM_bitwise_xor</td><td>0.99</td></tr><tr><td>average_testing_score_SYM_division</td><td>0.955</td></tr><tr><td>average_testing_score_SYM_gcd</td><td>0.94</td></tr><tr><td>average_testing_score_SYM_lcm</td><td>0.915</td></tr><tr><td>average_testing_score_SYM_modulo</td><td>0.98</td></tr><tr><td>average_testing_score_SYM_multiplication</td><td>0.96</td></tr><tr><td>average_testing_score_SYM_square_mod</td><td>0.73</td></tr><tr><td>different_actual_problem_types_count</td><td>0</td></tr><tr><td>epoch</td><td>999</td></tr><tr><td>epoch_bitwise_and</td><td>2009</td></tr><tr><td>epoch_bitwise_or</td><td>2031</td></tr><tr><td>epoch_bitwise_xor</td><td>1943</td></tr><tr><td>epoch_gcd</td><td>1921</td></tr><tr><td>epoch_lcm</td><td>2063</td></tr><tr><td>epoch_modulo</td><td>1971</td></tr><tr><td>epoch_multiplication</td><td>1955</td></tr><tr><td>epoch_square_mod</td><td>2099</td></tr><tr><td>final_loss_bitwise_and</td><td>1.1256</td></tr><tr><td>final_loss_bitwise_or</td><td>0.64182</td></tr><tr><td>final_loss_bitwise_xor</td><td>0.51242</td></tr><tr><td>final_loss_gcd</td><td>0.38767</td></tr><tr><td>final_loss_lcm</td><td>1.25893</td></tr><tr><td>final_loss_modulo</td><td>0.3906</td></tr><tr><td>final_loss_multiplication</td><td>0.72926</td></tr><tr><td>final_loss_square_mod</td><td>2.88929</td></tr><tr><td>final_score_bitwise_and</td><td>0.856</td></tr><tr><td>final_score_bitwise_or</td><td>0.86251</td></tr><tr><td>final_score_bitwise_xor</td><td>0.91675</td></tr><tr><td>final_score_gcd</td><td>0.9205</td></tr><tr><td>final_score_lcm</td><td>0.83593</td></tr><tr><td>final_score_modulo</td><td>0.92661</td></tr><tr><td>final_score_multiplication</td><td>0.88643</td></tr><tr><td>final_score_square_mod</td><td>0.65263</td></tr><tr><td>loss</td><td>0.27344</td></tr><tr><td>loss_bitwise_and</td><td>10.4375</td></tr><tr><td>loss_bitwise_or</td><td>1e-05</td></tr><tr><td>loss_bitwise_xor</td><td>0.00048</td></tr><tr><td>loss_gcd</td><td>7e-05</td></tr><tr><td>loss_lcm</td><td>0.02917</td></tr><tr><td>loss_modulo</td><td>0.02856</td></tr><tr><td>loss_multiplication</td><td>0.01459</td></tr><tr><td>loss_square_mod</td><td>0.1709</td></tr><tr><td>score</td><td>0.9375</td></tr><tr><td>score_bitwise_and</td><td>0</td></tr><tr><td>score_bitwise_or</td><td>1</td></tr><tr><td>score_bitwise_xor</td><td>1</td></tr><tr><td>score_gcd</td><td>1</td></tr><tr><td>score_lcm</td><td>1</td></tr><tr><td>score_modulo</td><td>1</td></tr><tr><td>score_multiplication</td><td>1</td></tr><tr><td>score_square_mod</td><td>1</td></tr><tr><td>skip_weights_0</td><td>0.5</td></tr><tr><td>symbolic_output_text_addition</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_bitwise_and</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_bitwise_or</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_bitwise_xor</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_division</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_gcd</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_lcm</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_modulo</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_multiplication</td><td>Mean score and loss ...</td></tr><tr><td>symbolic_output_text_square_mod</td><td>Mean score and loss ...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Linear encoder - 3 Digits - AB Test (Initialize Decoder False)</strong> at: <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/i6by1q7h' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders/runs/i6by1q7h</a><br> View project at: <a href='https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders' target=\"_blank\">https://wandb.ai/varun_dhanraj/Symbolic%20LLM%20-%20Fine%20Tune%20Decoders</a><br>Synced 5 W&B file(s), 32 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_003656-i6by1q7h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish() # If there is an active current run, terminate it\n",
    "config = initialize_default_config()\n",
    "\n",
    "config[\"encoder_input_tokens\"] = 1\n",
    "config[\"complexity\"] = 2\n",
    "config[\"initialize_decoders\"] = False\n",
    "config[\"symbolic_encoding_layer\"] = 17\n",
    "config[\"symbolic_decoding_layers\"] = [17]\n",
    "config[\"num_epochs\"] = 1000\n",
    "\n",
    "config[\"encoder_path\"] = f\"{curr_dir}/models/encoders_20250413.pth\"\n",
    "config[\"decoder_path\"] = f\"{curr_dir}/models/decoders_20250413.pth\"\n",
    "\n",
    "#config[\"verbose\"] = 2\n",
    "#config[\"epochs_to_print\"] = config[\"num_epochs\"]\n",
    "\n",
    "#config['test_with_non_numerical_rep'] = True\n",
    "#config['train_model'] = False\n",
    "\n",
    "#config[\"testing_verbose\"] = 2\n",
    "config[\"multi_token_intervention\"]    = False\n",
    "config['static_encoding']             = False\n",
    "\n",
    "config[\"calculate_encoding_accuracy\"] = True\n",
    "#config[\"n_samples\"] = 1\n",
    "#config[\"verbose\"] = 2\n",
    "\n",
    "\n",
    "if log_wandb:\n",
    "    wandb.finish() # If there is an active current run, terminate it\n",
    "    wandb.init(\n",
    "        project = \"Symbolic LLM - Fine Tune Decoders\",\n",
    "        config  = config\n",
    "    )\n",
    "    wandb.init(\n",
    "        project = \"Symbolic LLM - Fine Tune Decoders\",\n",
    "        #name    = f\"Transformer encoder - 3 Digits\",\n",
    "        #name    = f\"Transformer encoder - 4 Digits\",\n",
    "        name    = f\"Linear encoder - 3 Digits - AB Test (Initialize Decoder False)\",\n",
    "        #name    = f\"Linear encoder - 4 Digits\",\n",
    "        config  = config\n",
    "    )\n",
    "\n",
    "    print(f\"STARTING NEW EXPERIMENT (run_id = {wandb.run.id})\\n\\n\")\n",
    "\n",
    "run_experiment(generator=generator, config=config)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c25a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
